{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bed64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import emoji\n",
    "import contractions\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "pd.options.display.max_colwidth=200\n",
    "pd.options.display.max_rows=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c160727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/fhkt65ls68nbd5xt0lj3sbww0000gn/T/ipykernel_59839/3005909632.py:2: DtypeWarning: Columns (0,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('/Users/rosni/Library/CloudStorage/OneDrive-NJIT/Dissertation/curbside/Data/Python files/Final_data.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "371272"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f735f01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'author id', 'created_at', 'geo', 'id', 'lang', 'like_count',\n",
       "       'quote_count', 'reply_count', 'retweet_count', 'source', 'tweet',\n",
       "       'Category', 'channel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821cbaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delivery    329103\n",
       "curbside     42162\n",
       "Name: channel, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['channel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "108a8ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Consumer         8379\n",
       "Advertisement    8092\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b28f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368414"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset='tweet')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0101bcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Consumer         8377\n",
       "Advertisement    8092\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d53a13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Category</th>\n",
       "      <th>channel</th>\n",
       "      <th>new_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>47199132.0</td>\n",
       "      <td>2015-01-21 19:23:29+00:00</td>\n",
       "      <td></td>\n",
       "      <td>5.580000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>@jhineline meanwhile up the street, grocery shoppers park at trader joe's using the convenienttent curbside.</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>curbside</td>\n",
       "      <td>@jhineline meanwhile up the street, grocery shoppers park at trader joe's using the convenienttent curbside.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>34079088.0</td>\n",
       "      <td>2015-01-13 15:29:39+00:00</td>\n",
       "      <td></td>\n",
       "      <td>5.550000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>.@walmart is in trial phase of online orderering + curbside pickup with grocery, truly a disruptor and convenienttent! #nrf15 #futureofretail</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>curbside</td>\n",
       "      <td>.@walmart is in trial phase of online orderering + curbside pickup with grocery, truly a disruptor and convenienttent! #nrf15 #futureofretail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>119482829.0</td>\n",
       "      <td>2015-01-09 23:01:37+00:00</td>\n",
       "      <td></td>\n",
       "      <td>5.540000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>thank you @harristeeter for providing a curbside grocery pick up. i love you. that's all.</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>curbside</td>\n",
       "      <td>thank you @harristeeter for providing a curbside grocery pick up. i love you. that's all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>718558608.0</td>\n",
       "      <td>2015-01-08 04:08:17+00:00</td>\n",
       "      <td></td>\n",
       "      <td>5.530000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>i can't get to the grocery store weekly to buy food &amp;amp; you want me to haul trash across town? support curbside #recycle</td>\n",
       "      <td>Advertisement</td>\n",
       "      <td>curbside</td>\n",
       "      <td>i can't get to the grocery store weekly to buy food &amp;amp; you want me to haul trash across town? support curbside #recycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>16347551.0</td>\n",
       "      <td>2015-02-24 19:47:44+00:00</td>\n",
       "      <td>01fbe706f872cb32</td>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>loving @peapod delivers  i was in &amp;amp; out of the grocery store with a week of food using their curbside pick up in 2 minutes. #gamechanging</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>curbside</td>\n",
       "      <td>loving @peapod delivers  i was in &amp;amp; out of the grocery store with a week of food using their curbside pick up in 2 minutes. #gamechanging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index    author id                 created_at               geo  \\\n",
       "0     5   47199132.0  2015-01-21 19:23:29+00:00                     \n",
       "1     8   34079088.0  2015-01-13 15:29:39+00:00                     \n",
       "2     9  119482829.0  2015-01-09 23:01:37+00:00                     \n",
       "3    10  718558608.0  2015-01-08 04:08:17+00:00                     \n",
       "4    14   16347551.0  2015-02-24 19:47:44+00:00  01fbe706f872cb32   \n",
       "\n",
       "             id lang  like_count  quote_count  reply_count  retweet_count  \\\n",
       "0  5.580000e+17   en         0.0          0.0          0.0            0.0   \n",
       "1  5.550000e+17   en         1.0          0.0          0.0            2.0   \n",
       "2  5.540000e+17   en         0.0          0.0          1.0            0.0   \n",
       "3  5.530000e+17   en         0.0          0.0          0.0            0.0   \n",
       "4  5.700000e+17   en         0.0          0.0          2.0            0.0   \n",
       "\n",
       "               source  \\\n",
       "0  Twitter for iPhone   \n",
       "1  Twitter for iPhone   \n",
       "2  Twitter for iPhone   \n",
       "3  Twitter for iPhone   \n",
       "4  Twitter for iPhone   \n",
       "\n",
       "                                                                                                                                           tweet  \\\n",
       "0                                   @jhineline meanwhile up the street, grocery shoppers park at trader joe's using the convenienttent curbside.   \n",
       "1  .@walmart is in trial phase of online orderering + curbside pickup with grocery, truly a disruptor and convenienttent! #nrf15 #futureofretail   \n",
       "2                                                      thank you @harristeeter for providing a curbside grocery pick up. i love you. that's all.   \n",
       "3                     i can't get to the grocery store weekly to buy food &amp; you want me to haul trash across town? support curbside #recycle   \n",
       "4  loving @peapod delivers  i was in &amp; out of the grocery store with a week of food using their curbside pick up in 2 minutes. #gamechanging   \n",
       "\n",
       "        Category   channel  \\\n",
       "0       Consumer  curbside   \n",
       "1       Consumer  curbside   \n",
       "2       Consumer  curbside   \n",
       "3  Advertisement  curbside   \n",
       "4       Consumer  curbside   \n",
       "\n",
       "                                                                                                                                       new_tweet  \n",
       "0                                   @jhineline meanwhile up the street, grocery shoppers park at trader joe's using the convenienttent curbside.  \n",
       "1  .@walmart is in trial phase of online orderering + curbside pickup with grocery, truly a disruptor and convenienttent! #nrf15 #futureofretail  \n",
       "2                                                      thank you @harristeeter for providing a curbside grocery pick up. i love you. that's all.  \n",
       "3                     i can't get to the grocery store weekly to buy food &amp; you want me to haul trash across town? support curbside #recycle  \n",
       "4  loving @peapod delivers  i was in &amp; out of the grocery store with a week of food using their curbside pick up in 2 minutes. #gamechanging  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "data['Year'] = data['created_at'].dt.year\n",
    "data['Month'] = data['created_at'].dt.month_name()\n",
    "data['Day'] = data['created_at'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c2c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 00:07:25+00:00\n",
      "2022-11-29 23:51:09+00:00\n"
     ]
    }
   ],
   "source": [
    "print(min(data['created_at']))\n",
    "print(max(data['created_at']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a91d9742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words:  9761584\n",
      "Average number of words per tweet:  26.5\n",
      "Minimum number of words per tweet:  2\n",
      "Maximum number of words per tweet:  108\n",
      "Maximum number of words per tweet:  108\n",
      "Median number of words per tweet:  23.0\n"
     ]
    }
   ],
   "source": [
    "count = data['tweet'].str.split().str.len()\n",
    "print(\"Total number of words: \", count.sum())\n",
    "print(\"Average number of words per tweet: \", round(count.mean(),2))\n",
    "print(\"Minimum number of words per tweet: \", round(count.min(),2))\n",
    "print(\"Maximum number of words per tweet: \", round(count.max(),2))\n",
    "print(\"Maximum number of words per tweet: \", round(count.max(),2))\n",
    "print(\"Median number of words per tweet: \", round(count.median(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a1221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30047"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data[data['created_at'] >= '2022-01-01']\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "182afcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51f6d667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'index', 'author id', 'created_at', 'geo', 'id', 'lang',\n",
       "       'like_count', 'quote_count', 'reply_count', 'retweet_count', 'source',\n",
       "       'tweet', 'Category', 'channel', 'new_tweet', 'Year', 'Month', 'Day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aef7ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('level_0', axis=1)\n",
    "data = data.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88f7fa41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author id', 'created_at', 'geo', 'id', 'lang', 'like_count',\n",
       "       'quote_count', 'reply_count', 'retweet_count', 'source', 'tweet',\n",
       "       'Category', 'channel', 'new_tweet', 'Year', 'Month', 'Day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664858f",
   "metadata": {},
   "source": [
    "### Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "199bb2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@cdcarter13 my wife stopped at the grocery store for curbside pickup where they put your orderer in your trunk with no contact. while she was there she saw tons of boomers going into the store with no protection at all.\n",
      "@padme_kenobi @kaylive @fakemorganhope @itsaronbitch @robwhite010 @davidfholt they also say itâ€™s probably safer to get food from a restaurant curb side versus grocery shopping right now. less exposure. they shouldnâ€™t close. also workers should watch hands after each curbside delivery.\n"
     ]
    }
   ],
   "source": [
    "def lower_case(txt):\n",
    "    return txt.lower()\n",
    "data['tweet'] = data['tweet'].apply(lambda x : lower_case(x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c82e19",
   "metadata": {},
   "source": [
    "### Get number of tweets with retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1178750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    368412\n",
       "True          1\n",
       "Name: tweet, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get number of tweets with retweet\n",
    "def retweet_count(text):\n",
    "    if 'cc@' in text:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df = data['tweet'].apply(lambda x: retweet_count(x))\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4adc7b7",
   "metadata": {},
   "source": [
    "### Get all html tags in tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feed1fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_html_tags(text):\n",
    "    tags = []\n",
    "    html = re.findall('<.*?>',text)\n",
    "    if html != None:\n",
    "        tags.extend(html)\n",
    "    return tags\n",
    "\n",
    "html_tags = data['tweet'].apply(lambda x: get_html_tags(x))\n",
    "html_tags = [i for i in html_tags if len(i)!=0]\n",
    "html_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3d349",
   "metadata": {},
   "source": [
    "### Get all markdown links in tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00f5cfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[start using drones](https://t.co/zs7mt3cr8o)']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_html_tags(text):\n",
    "    tags = []\n",
    "    html = re.findall(r'\\[.*?\\]\\(.*?\\)',text)\n",
    "    if html != None:\n",
    "        tags.extend(html)\n",
    "    return tags\n",
    "\n",
    "html_tags = data['tweet'].apply(lambda x: get_html_tags(x))\n",
    "html_tags = [i for i in html_tags if len(i)!=0]\n",
    "html_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af0edeb",
   "metadata": {},
   "source": [
    "### Get html elements in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d43dbf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&lt; = <\n"
     ]
    }
   ],
   "source": [
    "html_elements = []\n",
    "for i in range(len(data)):\n",
    "    regexp = \"&.+?;\" \n",
    "    html_elements.extend(re.findall(regexp,data['tweet'][i]))\n",
    "html_elements = set(html_elements)\n",
    "\n",
    "from html import unescape\n",
    "for i in html_elements:\n",
    "    print(i,\"=\", unescape(i))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621e6b6",
   "metadata": {},
   "source": [
    "### Replace html elements in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "766e7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('&amp;', '&', x))\n",
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('&gt;', '>', x))\n",
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('&lt;', '<', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7473a01",
   "metadata": {},
   "source": [
    "### Remove mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd6b6533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my wife stopped at the grocery store for curbside pickup where they put your orderer in your trunk with no contact. while she was there she saw tons of boomers going into the store with no protection at all.\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('@(\\w+)', '', x))\n",
    "print(data['tweet'][5003])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351fb58c",
   "metadata": {},
   "source": [
    "### Replace URL with 'hyperlink'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9817c0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " amazon has grocery delivery??? where the hell have i been???\n"
     ]
    }
   ],
   "source": [
    "print(data['tweet'][50015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22a5c722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my wife stopped at the grocery store for curbside pickup where they put your orderer in your trunk with no contact. while she was there she saw tons of boomers going into the store with no protection at all.\n",
      " amazon has grocery delivery??? where the hell have i been???\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub(r'http\\S+', 'hyperlink', x))\n",
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub(r'www\\.[^ ]+', 'hyperlink', x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][50015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55423193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_link(data):\n",
    "    words = ' '.join(data['tweet']).split()\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word == 'hyperlink':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_i(data):\n",
    "    words = ' '.join(data['tweet']).split()\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word == 'i':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_my(data):\n",
    "    words = ' '.join(data['tweet']).split()\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word == 'my':\n",
    "            count += 1\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbb9c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194361 167064 87123\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7794a",
   "metadata": {},
   "source": [
    "### Replace email ids with keyword email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd9bff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my wife stopped at the grocery store for curbside pickup where they put your orderer in your trunk with no contact. while she was there she saw tons of boomers going into the store with no protection at all.\n",
      "      they also say itâ€™s probably safer to get food from a restaurant curb side versus grocery shopping right now. less exposure. they shouldnâ€™t close. also workers should watch hands after each curbside delivery.\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})', r'email',x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "836e8cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194361 167064 87123\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde65c1a",
   "metadata": {},
   "source": [
    "### Check number of tweet with emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc8eaaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [02:18<00:00, 2664.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{False: 357119, True: 11294}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def check_emoji(text):\n",
    "    emo = emoji.UNICODE_EMOJI\n",
    "    emojis = {**emo['en'],**emo['es'],**emo['pt'],**emo['it'],**emo['fr'],**emo['de']}\n",
    "    allchars = [i for i in text]\n",
    "    emoji_list = [c for c in allchars if c in emojis]\n",
    "    words = text.split()\n",
    "    contain_emoji = False\n",
    "    for word in words:\n",
    "        if word in emoji_list:\n",
    "            contain_emoji = True\n",
    "        else:\n",
    "            contain_emoji = False\n",
    "    return contain_emoji\n",
    "\n",
    "tqdm.pandas()\n",
    "count_emoji = data['tweet'].progress_apply(lambda x: check_emoji(x))\n",
    "print(count_emoji.value_counts().to_dict())\n",
    "#print(f'The {count_emoji.value_counts()[1]/count_emoji.value_counts().sum()} of the tweets have emojis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e88eb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {'ðŸ˜©': 'tired', 'ðŸ¤¦': 'embarassment', 'ðŸ˜…': 'happy', 'ðŸ¤£': 'funny', 'ðŸ˜Ž': 'awesome','ðŸ˜‚': 'happy', 'ðŸ™Œ': 'joy', 'ðŸ‘€': 'curious', 'ðŸ‘Œ': 'approval', 'ðŸ¤¢': 'disgust', 'ðŸ™‚': 'happy', 'ðŸ˜­': 'unhappy', 'ðŸ’µ': 'dollar',  'â¤': 'love', 'ðŸ¤·': 'ignorance','ðŸ˜¬': 'embarassment', 'âœ”': 'approval', \n",
    "              'ðŸ˜ƒ': 'happy', 'ðŸŽ¯': 'success', 'ðŸ˜': 'love','ðŸ˜¤': 'frustration', 'ðŸ™': 'gratitude','ðŸ‘': 'applaud', 'ðŸ‘': 'approval', 'ðŸ¤”': 'thinking', 'ðŸ™ƒ': 'silliness', 'ðŸ’•': 'love', 'ðŸŽ‰': 'celebration','ðŸ¤©': 'fascinate', 'ðŸ˜–': 'frustration', 'ðŸ¤¬': 'angry', 'ðŸ¤—': 'happy', 'ðŸ’ƒ': 'happy', 'ðŸ¤¯': 'frustration', 'ðŸ˜‘': 'frustration','âœ¨': 'excitement', 'ðŸ’ž': 'love', 'ðŸ™„': 'disapproval', 'ðŸ˜‰': 'joke', 'ðŸ˜„': 'happy', 'ðŸ’œ': 'love', 'ðŸ˜¡': 'angry','ðŸ˜': 'happy', 'ðŸ˜±': 'fearful:', 'ðŸ˜’': 'unhappy', \n",
    "              'ðŸ˜‹': 'tasty', 'ðŸ˜': 'frustration', 'âœŒ': 'peace', 'ðŸ˜†': 'funny','ðŸ˜‡': 'blessing', 'ðŸ˜”': 'regretful', 'ðŸ’¯': 'agree','ðŸ™ˆ': 'disbelieving', 'ðŸ˜•': 'confused', 'ðŸ˜Š': 'happy','ðŸ˜«': ':tired', 'ðŸ’°': 'money','ðŸ¤ª': 'funny', 'ðŸ˜¿': 'sad', 'ðŸ˜£': 'frustration', 'ðŸ¥´': 'confused', 'ðŸ¤ž': 'luck','ðŸ™…': 'disapproval', 'ðŸ¤˜': 'love', 'ðŸ’©': 'angry','ðŸ˜³': 'embarrassment',  'ðŸ’›': 'love', 'â˜¹': 'sad','ðŸ˜œ': 'funny','ðŸ¥°': 'love', 'ðŸ™†': 'approval', 'ðŸ¤¤': 'longing', 'ðŸ˜': 'smug', \n",
    "              'ðŸ’š': 'love', 'ðŸ¥º': 'pleading', 'ðŸ¤¥': 'lying','ðŸ’–': 'love',  'ðŸ§': 'ponder', 'ðŸ˜¥': ':sad', 'ðŸ™‡': 'apology', 'ðŸ˜¢': 'sad','ðŸ¤•': 'hurt', 'â˜º': 'happy', 'ðŸ˜¶': 'silence', 'ðŸ˜§': 'sad', 'ðŸ’”': 'sad','â™¥': 'love','ðŸ¤“': 'nerd', 'ðŸ˜²': 'surprise', 'ðŸ§¡': 'love', 'ðŸš«': ':prohibited:','ðŸ˜°': ':anxious', 'ðŸ˜': 'funny', 'ðŸ‘Ž': 'disapproval','ðŸ˜€': 'happy', 'ðŸŒž': 'happy','ðŸ˜µ': 'disbelief','ðŸ’™': 'love', 'ðŸ˜¯': 'surprise', 'ðŸ™': 'angry','ðŸ˜ž': ':disappoint', 'ðŸ˜˜': 'love', 'ðŸ˜º': 'happy',\n",
    "              'ðŸ¤¨': 'confused', 'ðŸ˜ ': 'angry', 'ðŸ˜ª': ':sleepy','ðŸ˜¦': 'sad', 'ðŸ’—': 'love','ðŸ˜“': 'frustration', 'ðŸ˜¨': ':fearful', 'ðŸ˜Œ': ':relieved', 'â£': 'love',  'ðŸ¤¡': 'funny','ðŸ˜®': 'amazed', 'ðŸ¥³': 'happy','ðŸ˜ˆ': 'trouble',  'ðŸ¤®': 'disgust', 'ðŸ˜´': ':sleeping','ðŸ˜»': 'love', 'ðŸ¥¶': 'cold', 'ðŸ¤­': 'embarrassment,', 'ðŸ–•': 'angry', 'ðŸ’˜': 'love', 'ðŸ¤‘': 'money','ðŸ¥²': 'gratitude', 'ðŸ¤Ÿ': 'love', 'ðŸ’«': 'dizzy', 'ðŸ˜Ÿ': 'worry', 'ðŸ–¤': 'love','ðŸ’¸': 'money', 'ðŸ˜›': 'fun','ðŸ™': 'disapproval', \n",
    "              'ðŸ’Ÿ': 'love', 'ðŸ™€': 'tired', 'ðŸ¤’': 'ill', 'ðŸ˜™': 'love', 'ðŸ’': 'love', 'ðŸ‘¿': 'angry', 'ðŸ‘ª': 'family', 'ðŸ¤°': 'pregnant woman',  \n",
    "              'ðŸ˜¸': 'happy', 'ðŸ¤': 'speechless', 'ðŸ¤«': 'quiet','ðŸ¥±': 'tired', 'ðŸ’¢': 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75693e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02148bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read dictionary from json file\n",
    "with open(\"../../Supplementary data/emoji_dictionary.json\",\"r\") as f:\n",
    "    emoji_dict = json.load(f)\n",
    "len(emoji_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "112923a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walmart testing curbside grocery pickup in huntsville hyperlink\n",
      "proudly introducing \"shopping for survivors\" - a free grocery shopping and curbside pick-up service for cancer patients. thanks !\n"
     ]
    }
   ],
   "source": [
    "print(data['tweet'][8])\n",
    "print(data['tweet'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cde52863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [02:25<00:00, 2528.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walmart testing curbside grocery pickup in huntsville hyperlink\n",
      "proudly introducing \"shopping for survivors\" - a free grocery shopping and curbside pick-up service for cancer patients. thanks !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove emojis\n",
    "tqdm.pandas()\n",
    "def give_emoji_free_text(text, emoji_dict):\n",
    "    emo = emoji.UNICODE_EMOJI\n",
    "    emojis = {**emo['en'],**emo['es'],**emo['pt'],**emo['it'],**emo['fr'],**emo['de']}\n",
    "    all_lst = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        lst = []\n",
    "        allchars = [i for i in word]\n",
    "        for j in allchars:\n",
    "            if j in emojis:\n",
    "                if j in emoji_dict:\n",
    "                    lst.append(' ')\n",
    "                    lst.append(emoji_dict[j])\n",
    "            else:\n",
    "                lst.append(j)\n",
    "        all_lst.append(''.join(lst))\n",
    "    return ' '.join(all_lst)\n",
    "\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x: give_emoji_free_text(x,emoji_dict))\n",
    "print(data['tweet'][8])\n",
    "print(data['tweet'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1715b951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194525 167083 87128\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f9360",
   "metadata": {},
   "source": [
    "### Remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d56329fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped at the grocery store for curbside pickup where they put your orderer in your trunk with no contact. while she was there she saw tons of boomers going into the store with no protection at all.\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "data['tweet'] = data['tweet'].apply(lambda x: contractions.fix(x))\n",
    "print(data['tweet'][5003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f251d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194525 213515 87128\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d80c2c",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2df598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9705812\n",
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact. she she saw tons boomers going store protection all.\n",
      "proudly introducing \"shopping survivors\" - free grocery shopping curbside pick-up service cancer patients. thanks !\n",
      "6708406\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "words = ' '.join(data['tweet']).split()\n",
    "print(len(words))\n",
    "from nltk.corpus import stopwords\n",
    "stop = ['what','which','this','that',\"that'll\",'these','those','is','are','was','were','be','been','being','have',\n",
    "              'has','had','having','do','does','did','doing','a','an','the', 'and','but','if','or','because','as','until',\n",
    "              'while','of','at','by','for','with','about','against','between','into','through','during','before','after',\n",
    "              'above','below','to','from','up','down','in','out','on','off','over','under','again','further','then',\n",
    "              'once','here','there','when','where','why','how','all','any','both','each','few','more','most','other',\n",
    "              'some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just',\n",
    "              'don',\"don't\",'should',\"should've\",'now','d','ll','m','o','re','ve','y','ain','aren',\"aren't\",'couldn',\n",
    "              \"couldn't\", 'didn',\"didn't\",'doesn',\"doesn't\",'hadn',\"hadn't\",'hasn',\"hasn't\",'haven',\"haven't\",'isn',\"isn't\",\n",
    "              'ma','mightn',\"mightn't\",'mustn',\"mustn't\",'needn',\"needn't\",'shan',\"shan't\",'shouldn',\"shouldn't\",'wasn',\n",
    "              \"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn',\"wouldn't\"]\n",
    "data['tweet'] =  data['tweet'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][16])\n",
    "words = ' '.join(data['tweet']).split()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "752689f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194525 213515 87128\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1ec2b",
   "metadata": {},
   "source": [
    "### Remove next line characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a58b1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact. she she saw tons boomers going store protection all.\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now. less exposure. they close. also workers watch hands curbside delivery.\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('\\n', ' ', x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4245902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194525 213515 87128\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e62d3",
   "metadata": {},
   "source": [
    "### Remove accented characters to their ASCII values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1dc7456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact. she she saw tons boomers going store protection all.\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now. less exposure. they close. also workers watch hands curbside delivery.\n"
     ]
    }
   ],
   "source": [
    "def accented_to_ascii(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "data['tweet'] = data['tweet'].apply(lambda x: accented_to_ascii(x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f387526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194578 213532 87129\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fc53e",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "322500b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact  she she saw tons boomers going store protection all \n",
      "proudly introducing  shopping survivors    free grocery shopping curbside pick up service cancer patients  thanks  \n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('[!\"#%â€œ[â€¢â–ºâ€“&â€\\'()*+,-/:;<=>?@\\\\^_{|}~`]', ' ', x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83a97e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195617 218051 87708\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e50854",
   "metadata": {},
   "source": [
    "### Remove spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9bfd757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact she she saw tons boomers going store protection all \n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also workers watch hands curbside delivery \n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub(' +', ' ', x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3bddacdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195617 218051 87708\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824f15f",
   "metadata": {},
   "source": [
    "### Replacing currency symbol by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84d055ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ucwradio walmart expands its curbside grocery pickup service you s hyperlink technology\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub(r\"\\$(\\d+)\", r\"\\1 dollars\", x))\n",
    "print(data['tweet'][780])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "077c1054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195617 218051 87708\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54131b11",
   "metadata": {},
   "source": [
    "### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee1e65e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact she she saw tons boomers going store protection all\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also workers watch hands curbside delivery\n"
     ]
    }
   ],
   "source": [
    "import num2words\n",
    "def remove_num(txt):\n",
    "    return ' '.join(' ' if i.isdigit() else i for i in txt.split())\n",
    "\n",
    "data['tweet'] = data['tweet'].apply(lambda x : remove_num(x))  \n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac42914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195617 218051 87708\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a7b5ee",
   "metadata": {},
   "source": [
    "### Find words with multiple same consecutive letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6826baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:17<00:00, 21271.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809575 3794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "two_consecutive_letters = []\n",
    "three_consecutive_letters = []\n",
    "count_2 = 0\n",
    "count_3 = 0\n",
    "for i in tqdm(range(len(data))):\n",
    "    words = data['tweet'][i].split(' ')\n",
    "    for word in words:\n",
    "        if re.search(r'((\\w)\\2{2,})', word):\n",
    "            count_3 += 1\n",
    "            if word not in three_consecutive_letters:\n",
    "                three_consecutive_letters.append(word)\n",
    "        elif re.search(r'((\\w)\\2{1,})', word):\n",
    "            count_2 += 1\n",
    "            if word not in two_consecutive_letters:\n",
    "                two_consecutive_letters.append(word)\n",
    "print(count_2, count_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16511a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = to_remove = ['bbb', 'litttt', 'daaaaa', 'holllla', 'ggg', 'cccsc', 'oooo', 'ppp', '1000xs', 'ooooo', 'ffffff', 'haaa','brrr', 'econ1110', 'a444', 'mmmpanadas', 'toowaahhhh', 'mmmmmmmm', 'mmmagtweets', 'zzzzzz', 'mmmmm', 'ifttt', 'from1000km', 'bellletstalk', 'xxx', 'mmm', 'zzzz', 'vrai777', 'tracycopygrrrl', \n",
    "             'zzzzzzzzzzzzzzz', 'ooo', 'jefffrey', 'whole30fff', 'yyccc', 'mmmm', 'mmmmmm', 'hrmmm', 'fffuuuu', '1000mg', 'aaa', 'www', '7771x29', 'ffffffffffff', '2000s', 'taaac', 'brandongaillle', 'niggaaaaa','$qqq', 'heee', 'ummmmm', 'paullly', 'theyllgetcontactlessintheyear3000', 'hbuuu', 'sumkt444', 'nascarvvvrroooommmmmmm', 'kkkempsey', 'ttttttt', 'tttt', 'oooooo', 'lorderdddd', 'teamfccc', \n",
    "             'wwwwwwwwwww', '1000s', 'lll', 'ieeeorg', 'reeee', 'ltdjnrtnnnbggngvcfi', 'wwww', 'hhh', 'onnnnnnn','hheee', 'mmmmmmm', 'xxxx', '1000000000x', 'eee', 'ffbbbllaarrrrrrnnttts', 'wlll', 'pcccommunitymarkets',\n",
    "             'paccc', 'supportpaccc', 'amoooo', 'cj333', 'balllin', '000mph', 'reeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee','squeee', '000s', 'hhhh', 'rdockmatthews111', '5000sq', '1000x', 'aaaaaaa', 'booo19', 'ggggggggg','mysicklecelllife', 'faaaaa', 'maccleennnny', 'hmmmmm', 'aaaaaaaaaaaa', '000ppl', 'hmmmyess', 'aaaaaa','bestnyaaaa', 'aaaa', '000sq', '2000k', 'ccc', 'eeee', 'ooota', 'neee', 'waangooonline', 'hal2000bot',\n",
    "             'vcccares', '$glittergrl2000', 'nuuu', 'jaaahaaack', 'breeellery', 'aaaaaaaaaaa', 'bbbslc', 'hhhhhhhhh','3000th', 'sss', 'pleading$sbreezyyy', 'siaaa', 'aaaaaaaa', '15902201009000473m', '15904058179022263m', 'zzzzzzz', 'zzzzzzzzzzzzzzzzz', '000th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cfd3468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:08<00:00, 45062.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = 0\n",
    "for i in tqdm(range(len(data))):\n",
    "    count = 0\n",
    "    words = data['tweet'][i].split(' ')\n",
    "    for word in words:\n",
    "        if word in words_to_remove:\n",
    "            count += 1\n",
    "    total_count += count\n",
    "total_count            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71a19f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:07<00:00, 49712.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact she she saw tons boomers going store protection all\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also workers watch hands curbside delivery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_words(text, words_to_remove):\n",
    "    words = text.split(' ')\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in words_to_remove:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x : remove_words(x, words_to_remove))  \n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a538d550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1274"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_list = {'hummmm': 'hum','alll':'all','allll':'all','alllll':'all','buttt':'but', 'butttt':'but','shoppinggg':'shopping','lifeee': 'life', 'ehhhhhhhh': 'eh','hourrr': 'hour', 'weeeeeeeeeeeeeeeeeeeeeeeeee': 'we', 'buuuuuuuuut': 'but','ohhh': 'oh', 'sooo': 'so', 'superbowlllll': 'superbowl', 'damnnnn': 'damn', 'verrrry': 'very', 'oooh': 'oh', 'aahhhh': 'ah', 'fuckkkk': 'fuck','yooo': 'yo', 'effffffffffff': 'eff', \n",
    "                'okurrrrrrrr': 'ok', 'bizzzznesss': 'business', 'waaaaayyy': 'way', 'soooo': 'so','fffffffuuuuuuck': 'fuck', 'hiiii': 'hi', 'ssoooo': 'so', 'lmaooo': 'lmao', 'mannnnn': 'man', 'businesssupportingbusiness': 'business supporting business', 'eeeek': 'ek', 'alll': 'all', 'pssst': 'past', 'cccp': 'cp', 'annnnd': 'and','ummm': 'umm','hmmm': 'hmm', 'wonderfullly': 'wonderfully', 'smallbusinessstrong': 'small business strong','mayyyyyybe': 'may be', 'waaaaay': 'way', 'psssst': 'past','mooood': 'mood', 'usuallly': 'usually','ahhhh': 'aah', 'yasss': 'yes', 'lossses': 'losses', 'nevaaaa': 'never', 'stilllovewegmans': 'still love wegmans', 'mannn': 'man', 'awwww': 'aww', 'fukkkk': 'fuck', 'exciiited': 'excited', 'ahhh': 'aah', \n",
    "                'allll': 'all', 'outdooors': 'outdoors', 'uuugh': 'ugh', 'aaah': 'ahh', 'blesssss': 'bless', 'maaaybe': 'may be', 'dayssss':'days', 'bruhhh': 'bruh', 'itttttttt': 'it', 'waaaaaaaaaaaay': 'way', 'raaaaah': 'rah','eatwelllivewellku': 'eat well live well','yyyyes': 'yes', 'goooooooo': 'go', 'alllllllllll': 'all', 'aaand': 'and','ahhhhh': 'aah', 'hmmmm': 'hmm', 'fuuuuccckkkkkk': 'fuck', 'thoughtttt': 'thought', 'buttt': 'but', 'storessss': 'stores', 'goooo': 'go', 'ohhhh': 'oh', 'thennnn': 'then', 'whyyy': 'why', 'dumbbb': 'dumb', 'fuuuuuck': 'fuck', 'alllll': 'all','wooeee': 'woe', 'boooooo': 'boo', 'nahhhhh': 'nah', 'monthssss': 'months', 'shoooing': 'shooing', 'massshootings': 'mass shootings', 'amerikkka': 'amerika', 'hiiighly': 'highly', 'awww': 'aww', 'eeesh': 'esh', 'jooookeee': 'joke', 'realllllyyy': 'really',\n",
    "                'okokokokokkkk': 'ok','aaah':'aah','aaahh':'aah', 'ahhh':'aah','ahhhh':'aah','ahhhhh':'aah','byeee': 'bye', 'yeahhh': 'yeah', 'neeeda': 'need', 'shiiiiiiz': 'shit', 'buuuut': 'but', 'hateeeee': 'hate', 'otttt': 'ot', 'hnnng': 'hang', 'aaaaugh': 'ugh', 'partayyyyyy': 'party', 'heatedddd': 'heated', 'aayyy': 'aay', 'buuutttt': 'but', 'trashhhh': 'trash', 'waaaay': 'way', 'whaaat': 'what', 'uhhh': 'ugh','sweetoooo': 'sweet', 'fuuuuq': 'fuck', 'honeyyy': 'honey', 'aaaand': 'and', 'teeeellll': 'tell', 'coolinnnnnn': 'colin', 'businessnewzzz': 'business news', 'grrr': 'grr','hellllllo': 'hello', 'pleaseee': 'please', 'whaaaaa': 'wha', 'whhhyyyyy': 'why', 'ewwwwww': 'eww','grrrr': 'grr', 'lolll': 'lol', 'bihhhhhh': 'bih', 'yesssss': 'yes', 'arghhhh': 'argh', 'lovellloves': 'love','pfffft': 'pft', 'lessshoppingmorewriting': 'less shopping more writing', 'workswonderfullly': 'works wonderfully', 'grocerybusinessservice': 'grocery business service', 'litttttt': 'lit', 'juuuuust': 'just', 'couchhhhhh': 'couch', 'insteaddddd': 'instead', 'innnnteresting': 'interesting', 'waaaaah': 'wah', 'fuuuuuuck': 'fuck', 'gawwwwd': 'gawd', 'sooooo': 'so', 'daaang': 'dang', 'hellllllllllp': 'help', 'sawww': 'saw', 'booo': 'boo', 'yaaaasssss': 'yes', 'grrrrrr': 'grr', 'livveee': 'live', 'starvingggg': 'starving','worthlessshopper': 'worthless hopper', 'pleaseeee': 'please', 'terrrrible': 'terrible', 'wrrrrr': 'wrr', 'neeeeeeed': 'need','gurrrrrr': 'grr','outtttt': 'out', 'nowzzz': 'now', 'maaaan': 'man', 'hungryyyyy': 'hungry', 'reeeeeeally': 'really', 'wheee': 'whee', 'groooooans': 'groan', 'errmaaaagaaaahdddd': 'omg', 'booooooooooooo': 'boo', 'haaaaaaaaaaaaaaaaaaaaaaaaaaate': 'hate','frannnd': 'frand', 'hmmmmmm': 'hm', 'arrreeeeee': 'are', 'laaawd': 'loud', \n",
    "                'foodddddddddd': 'food','coffeee':'coffee', 'lesssocietymoreworders': 'less society more worders', 'unluckyyyyy': 'unlucky', 'naaahr': 'nah', 'sweeeeet': 'sweet', 'ahhhhhh': 'ahh', 'eeep': 'eep', 'baaaare': 'bare', 'geeky0001': 'geky01', 'loooooook': 'lok', 'dribbble': 'drible', 'm9mk8888m9':'m9mk8m9', 'yassss': 'yas', 'jeeez': 'jeez', 'miiighty': 'mighty', 'ummmm': 'umm', 'moooostly': 'mostly', 'whaaaaaa': 'wha', 'ooooppssss': 'oops', 'daaaa': 'daa', 'woooffffff': 'woof', 'wannna': 'wana', 'selllig': 'selig', 'convenienttentttttt': 'convenientent', 'speeedy': 'speedy', 'whaaaaat': 'what', 'yaaaas': 'yes', 'hnnnnnng': 'hang','beeeeeeeeeach': 'beach', 'strollling': 'strolling', 'dddribbble': 'drible','wiiggggg': 'wig',  'reeeeeal': 'real', 'dolll': 'doll', 'uppp': 'up', 'reallllllllly': 'really', 'loveee': 'love', 'girrrrl': 'girl', 'cmoooooon': 'come on', 'neeeeed': 'need', 'fuuuuuuuuuuuuuuuuuuuuuuuckkkk': 'fuck','businessscript': 'business script', 'ehhh': 'eh', 'cluuutch': 'clutch', 'kiiinda': 'kinda','reeeaaaaally': 'really', 'eeeugh': 'enough', 'welcummm': 'welcome', 'embarsssed': 'embarrassed', 'buuuuutttt': 'but','looovvve': 'love', 'weee': 'we', 'ooohhhh': 'oh', 'unveilingthebusinessstories': 'unveilingthebusinestories',  'guuuuhyperlink': 'hyperlink','riiising': 'rising', 'biittttccchhheeeessss': 'bitch', 'whewwwwwwwwww': 'whew', 'whaaatttt': 'what', 'stilll': 'still', 'huuuuge': 'huge', 'suuuper': 'super', 'alllllllll': 'all', 'mannnn': 'man', 'pffft': 'pft', 'reallly': 'really', 'whuuuuuuh': 'whuh', 'suckssss': 'sucks', 'awonderfullly': 'wonderfully', 'staaaaaarve': 'starve', 'cuuute': 'cute', 'ayyyyyyy': 'ay', 'yaaassssszzzzz': 'yes', 'goooees': 'goes','toooooo': 'to', 'bleeeeh': 'bleh', 'harrrrrd': 'hard', 'maaaaad': 'mad', 'lazyyyyyy': 'lazy', 'uuuuummm': 'umm','bweakfaaaaaaaast': 'bweakfast', 'againnnnnn': 'again', 'hrmmmm': 'hrm', 'hooopefully': 'hopefuly', 'psssttt': 'pst','swearrrrr': 'swear', 'weeeeeek': 'week', 'brahhhh': 'brah', 'checkkkk': 'check','everrrrr': 'ever', 'annnnnnnd': 'and', 'suuuucks': 'sucks', 'atttorney': 'attorney', 'peanutbuttter': 'peanut butter', 'ayyy': 'ay',  'wtffff': 'wtf', 'tttthhhhhiiissssss': 'this', 'hateee': 'hate', 'savedddd': 'saved','squeeeee': 'squee',  'oooouuuu': 'ou', 'naaaaa': 'na', 'weeee': 'we', 'yessss': 'yes', 'yeaaaa': 'yes', 'homeacccess': 'home access', 'timberrr':'timber', 'tahdayyy': 'today', 'yeees': 'yes', 'stayinnn': 'staying', 'aliveee': 'alive', 'heeeeeere': 'here', 'gr000oceries': 'groceries', 'taaaagetherrr': 'together', 'appp': 'app', 'sighhh': 'sigh', 'wooohoo': 'woohoo', 'winterrrr': 'winter', 'friiiendssss': 'friends', 'ooooof': 'of', 'yuuup': 'yes', 'everythinggg': 'everything', 'thankkk': 'thank', 'youuu': 'you', 'aaaannndddd': 'and', 'babaaaay': 'baby', 'ayeee': 'aye', 'babiesss': 'babies', 'yaaas': 'yes', 'coinnn': 'coin', 'nevvvaaaa': 'never','aaaack': 'ack', 'mmmlove': 'love', 'dayyyy': 'day', 'cribmusss': 'cribmus', 'employeeengagement': 'employee engagement', 'diseaseeee': 'disease', 'beyonddd': 'beyond', 'jeeeeesus': 'jesus', 'coffeeeeee': 'coffee', 'wwasss': 'was', 'alriiight': 'alright', 'tieeeeeeed': 'tied','hellooooo': 'hello', 'reeeaaalllll': 'real', 'whoooo': 'who', 'hulll': 'hull', 'whhyyyyy': 'why', 'oooooold': 'old','orrrr': 'or', 'weeeek': 'week', 'dayyyyy': 'day', 'ohhhyeaaahhh': 'oh yeah', 'helloooo': 'hello', 'yallll': 'you all','agghhhhhhhhhhhhhhhhhhhhhhhhhhh': 'argh', 'agghhhhhhhhhhhhhhhhhhhhhhhhhhhhh': 'argh', 'finallly': 'finally','adressses': 'addresses', 'reallllly': 'really', 'omggg': 'omg', 'yaaaaa': 'ya', 'laggg': 'lag', 'helllooooee': 'hello', 'hellllooooee': 'hello', 'hungryyy': 'hungry', 'apppayments': 'apayments', 'littt': 'lit', 'suuuuuuuuper': 'super', 'feeees': 'fees', 'bihhhh': 'bih', 'gawddd': 'gawd', 'allllll': 'all', 'deeeply': 'deeply', 'fullfilllment': 'fulfilment', 'pfff': 'pf', 'weirdddd': 'weird', 'nowwwwwww': 'now', 'heyyyy': 'hey','luuuuucky': 'lucky', 'haaand': 'hand', 'nowwwww': 'now',  'heeelp': 'help', 'shppper': 'shopper', 'mooost': 'most', 'fooods': 'foods', 'girlllll': 'girl','whatttt': 'what', 'factttt': 'fact', 'yaaaassssss': 'yes', 'worrry': 'worry', 'buuutt': 'but', 'andddd': 'and','groooound': 'ground', 'commmunication': 'communication', 'biggg': 'big', 'geeeezus': 'jeesus',              'woooo': 'woo', 'paaaranoid': 'paranoid', 'wuaaaaaaaaaaaa': 'wua', 'yerrrrrr': 'yer', 'craaaap': 'crap','weeeeeed': 'weed', 'daaaays': 'days', 'runnning': 'running', 'eeech': 'ech', 'everrr': 'ever', 'quinnnnnnn': 'quin','yummm': 'yum', 'goood': 'god',  'okaaaaay': 'okay', 'weeeeeeeee': 'we', 'chilll': 'chill', 'yayyy': 'yay', 'realllly': 'really', 'oooof': 'of', 'wayyyyy': 'way', 'booooo': 'boo', 'yyyyeesssssssss': 'yes', 'stafff': 'staff', 'ooooooookay': 'okay', 'suppposed': 'supposed','thanksssss': 'thanks', 'boyyyyy': 'boy', 'ugguhughughghhhh': 'ugh', 'reaalllyy': 'really', 'tyyy': 'ty','yesss': 'yes', 'weeeird': 'weird', 'wheeeeeeee': 'whee', 'aieee': 'aie', 'pissssssed': 'pissed', 'anddddd': 'and','errrr': 'err', 'freeeeeee': 'free', 'dayzzzz': 'days', 'killerrrr': 'killer', 'piiised': 'pissed', 'wooooop': 'woop', 'localbusinessservices': 'local business services', 'happyhmmm': 'happy','housssseeee': 'house', 'gaaaaah': 'gah', 'sbappp': 'sbap', 'scurrred': 'scurred', 'messsages': 'messages', 'whaaaat': 'what', 'soooon': 'soon', 'theee': 'the', 'villlage': 'village', 'woooow': 'wow','chiliiii': 'chili', 'uhhmmmm': 'uhm', 'itwonderfullly': 'it wonderfully','rulllessssss': 'rules', 'aaaannnnnd': 'and', 'ewwwhyperlink': 'hyperlink', 'sammeee': 'same', 'geeez': 'geez', 'wayyyyyyy': 'way', 'eatwelllivewell': 'eat well live well', 'tooo': 'too', 'nopeeee': 'nope', 'aswonderfullly': 'as wonderfully', 'aaaaaaaaallllll': 'all', 'resuppply': 'resupply', 'yeahhhhh': 'yeah', 'lesss': 'less', 'innnnn': 'in','shppping': 'shipping', 'yeaaaah': 'yeah', 'whyyyyy': 'why','ooops': 'oops', 'neoowww': 'neow', 'seeems': 'seems', 'cluuuuuuutch': 'clutch', 'hellllll': 'hell', 'yaaaaaa': 'ya', 'aaahh': 'aah', 'byyyyyeeeeee': 'bye', 'pleaseeeeeee': 'please', 'maaaaan': 'man','lollll': 'lol', 'caaaat': 'cat', 'foood': 'food', 'faaaaaave': 'favourite', 'knowwww': 'know', 'weeeks': 'weeks', 'yaaa': 'ya','reeeeaaaalllly': 'really', 'themmm': 'them', 'gooooooood': 'good','ssshhhh': 'shh', 'waaaawaaaaawaaaaa': 'wow', 'businessschools': 'business schools', 'locallly': 'localy', 'boooooy': 'boy', 'plzzzzz': 'plz', 'realllyyyy': 'really', 'lmdaoooo': 'lmdao', 'daaaaaam': 'damn', 'woohooo': 'woohoo', 'whaattt': 'what','truuuuu': 'true', 'whaaaaaaat': 'what', 'meeee': 'me', 'reeeeeallly': 'really', 'ffffuuuuccccckkkkkkkkkkk': 'fuck', 'winnnning': 'winning', 'pppl': 'ppl', 'siiiigh': 'sigh', 'thurrr': 'thur', 'hopefullly': 'hopefully', 'lettting': 'letting', 'reaaaaaaally': 'really', 'buisnesssupport': 'buisness support', 'bombbbb': 'bomb','wuuut': 'wut', 'fuuu': 'fuu', 'trafffic': 'traffic', 'butttt': 'but', 'coffeee': 'coffee', 'maaamm': 'mam', 'reeeaally': 'really', 'gooo': 'go', 'sammeeee': 'same', 'swoooooon': 'swon', 'todaaaaaaay': 'today', 'nccc': 'nc','uggg': 'ugh', 'hellllllp': 'help', 'grrrrrrr': 'grr', 'shoooot': 'shot', 'fffuzzy': 'fuzzy', 'gettting': 'getting','telll': 'tell', 'maaaaybe': 'may be', 'duuuuude': 'dude', 'dooooo': 'do', 'weyyy': 'wey', 'boycotttrumppressconferences': 'boycotrumpresconferences','nowww': 'now', 'hooraaaay': 'hooray',  'woohoooo': 'woohoo', 'hhhmmm': 'hmm', 'yummmmmm': 'yum', 'wellllllll': 'well', 'woahhh': 'whoa', 'buuuuusssssinesssss': 'business','muuuumm': 'mum', 'girlll': 'girl', 'uuuuugh': 'ugh', 'thiiis': 'this', 'wayyyy': 'way', 'ateeee': 'ate', 'ohhhhhh': 'oh', 'ahhhhhhhhhhh': 'aah','saaaaammmmeee': 'same', 'saaammmmeee': 'same', 'infinittyyyyyy': 'infinity','welppp': 'welp', 'hellooooooo': 'hello', 'tryinggg': 'trying', 'roaaaaarsome': 'roarsome', 'cooo': 'co', 'oookies': 'okay','ittttttt': 'it', 'willl': 'will', 'neededdddddd': 'needed', 'haaay': 'hay', 'daysss': 'days', 'hahahhhahah': 'haha','oooph': 'oph', 'youlllive': 'you will live', 'defffff': 'def', 'bitttttt': 'bit', 'ideaaaa': 'idea', 'waitttt': 'wait','dangggggggggg': 'dang', 'lmaooimfatboyandwillstarveooooooooooooooooooo': 'lmao i am fat boy and wil starve', 'aaaannnnnndddddd': 'and', 'cevicheee': 'ceviche', 'tostadassss': 'tostadas', 'boooooooox': 'box', 'yaaaay': 'yay', 'guyssss': 'guys', 'aaaassssss': 'as', 'allllllaaaat': 'alat', 'waaaaayyyyy': 'way', 'coooooookies': 'cookies', 'folksss': 'folks', 'meee': 'me', 'yuuuup': 'yup', 'daaark': 'dark', 'sorryyy': 'sorry', 'concepttttt': 'concept', 'booosted': 'boosted','threeeee': 'three', 'grrrrr': 'grr','starrrrrrrrrrving': 'starving',  'baybayyyy': 'baby', 'aaaaafffffff': 'aaf', 'maxxxx': 'max', 'waaaaaaaah': 'wah', 'arghhh': 'argh',  'flosssssss': 'floss',  'wayyy': 'way', 'burgesss': 'burges', 'wahhh': 'wah', 'maaam': 'mam', 'brrrr': 'brr', 'yippeeeee': 'yippee', 'loool': 'lol', 'ooohh': 'oh', 'loooooool': 'lol', 'donneeee': 'done', 'ewwww': 'eww', 'everrrrrr': 'ever', 'looot': 'lot', 'deliverrrry': 'delivery', 'isss': 'is', 'orrrrrr': 'or', 'duuuude': 'dude', 'ftwwwwww': 'ftw', 'hallllp': 'help', 'fuuuuck': 'fuck', 'yaaaaas': 'yes','businessscience': 'business science','myyy': 'my', 'successstory': 'sucess story', 'fffffffuck': 'fuck', 'thisssssss': 'this', 'ahhhhhhh': 'aah', 'yaaaaasss': 'yes', 'tthhhe': 'the', 'donttouchmeee': 'dont touch me',  'jeeeeeze': 'jeeze', 'suuuuch': 'such', 'twattttt': 'twat', 'tomorrrow': 'tomorrow', 'aaalllll': 'all','gooooaaaaaalll': 'goal', 'heeeeeeere': 'here', 'weeeeeee': 'we', 'gooooo': 'go', 'firrre': 'fire', 'chainssss': 'chains', 'whhhaaatttt': 'what', 'yaaasss': 'yes', 'notttt': 'not', 'haaaaaaaate': 'hate','hungeeeee': 'huge', 'waaaaa': 'wah', 'wheeeeee': 'whee', 'milllennials': 'millennial', 'aghhh': 'agh', 'errrrr': 'err', 'bestttt': 'best', 'buuuuuut': 'but', 'hmmmph': 'hmph', 'maaaaaaaan': 'man', 'buuuuut': 'but', 'guuuuurrrllll': 'girl', 'heyyy': 'hey', 'ummmmmmm': 'umm', \n",
    "                'yassssss': 'yes', 'fuckkk': 'fuck', 'arhhh': 'arh', 'hooo': 'ho', 'itttttt': 'it', 'boooo': 'boo', 'zzzzz': 'z', \n",
    "                'nahhhh': 'nah','guyssss':'guys','damnnnn':'damn', 'grrr':'grr','grrrr':'grr','grrrrrrrr': 'grr', 'mmmaybe': 'may be', 'deeeeets': 'details', 'waaaaaay': 'way', 'knowwwwww': 'know', 'hollaaaaaaa': 'hola', 'exxxtra': 'extra', 'neeeed': 'need', 'yaaaassss': 'yas', 'seamlessshopping': 'seamleshoping','plsss': 'please', 'hnnnnngggggg': 'hang', 'retweeet': 'retweet', 'retweeetplease': 'retweet please', 'tyyyyy': 'ty','yaaaaassss': 'yes', 'aahhh': 'aah','yupppp': 'yup', 'damnnnnnnn': 'damn','successstories': 'success stories', 'nnnreal': 'unreal', 'reggggular': 'regular', 'screeeam': 'scream', 'stahhpppp': 'stop', 'girrrrllllllllll': 'girl', 'mooooooooooon': 'moon', 'ouuuuuuuchhhh': 'ouch', 'yoooo': 'yo', 'lessstress': 'less stress', 'positivevibezzz': 'positive vibes', 'oookkkkayyy': 'okay', 'breeezyy': 'breezy', 'wisssshh': 'wish', 'foooood': 'food', 'oooooooookay': 'okay','thaaanks': 'thanks', 'loook': 'look', 'paaaarrrrrty': 'party','hollaaaa': 'hola', 'suuuuuper': 'super', 'hateeee': 'hate', 'gaaaaahhhhhh': 'gah','callling': 'calling','honestbeeeeee': 'honestbee', 'hoooonest': 'honest', 'neeeeeeeeeever': 'never', 'ricccch': 'rich', 'juuuuuuust': 'just', 'whaaattt': 'what', 'whyyyyyy': 'why','happyxxx': 'happy','ayyye': 'aye', 'hardddd': 'hard', 'pleeeeeeease': 'please','suuuuucks': 'sucks', 'shhhhh': 'shh', 'naaaaaaaaah': 'nah', 'heeey': 'hey', 'yooooo': 'yo','wooooooooorse': 'worse', 'ittttt': 'it', 'buuut': 'but', 'succkssss': 'sucks', 'slowwwwlyyyy': 'slowly','yaaassss': 'yes', 'annnnnd': 'and', 'herrreeee': 'here', 'yeaaaaaah': 'yeah', 'murderrrr': 'murder','yeaaaaaaaaaah': 'yeah', 'aaaany': 'any', 'uppppp': 'up', 'fuuun': 'fun', 'tadaaa': 'tada', \n",
    "                'hufffff': 'huff', 'juuust': 'just','finnne': 'fine','errr': 'err', 'odddeliverydrivercomplaints': 'odd delivery driver complaints', 'eeek': 'eek', 'girrrrrl': 'girl', 'hmmmmmmm': 'hmm', 'onlyyyyyyy': 'only', 'guysssssss': 'guys','lotttttt': 'lot', 'ohhhhh': 'oh', 'ruuuns': 'runs', 'weeeekend': 'weekend', 'shoppping': 'shopping', 'verrry': 'very', 'fooooooood': 'food', 'tooooooo': 'too', 'reeeeally': 'really','zoooo': 'zoo', 'uhhhh': 'ugh', 'sorrryyyy': 'sorry', 'ssshhh': 'shh', 'embarrassmentummm': 'embarassment', 'plssssssss': 'please', 'todayyyy': 'today', 'neverrrr': 'never', 'craaazy': 'crazy', 'annnd': 'and', 'boooomb': 'bomb','aliiive': 'alive', 'ecologicallly': 'ecologicaly', 'nooo': 'no', 'waaaait': 'wait', 'tooooo': 'too', 'ohhhhhhh': 'oh', 'whyyyy': 'why', 'pleeaaaassseeee': 'please', 'fuuuudge': 'fudge','shiddddd': 'shid', 'biiig': 'big', 'yooooooo': 'yo', 'coldddddddddd': 'cold', 'mighttttt': 'might','freeeee': 'free', 'pleaseeeeee': 'please', 'helll': 'hell','mmkkayyy': 'i am okay', 'reeeaaalll': 'real','bxtchhhhhhh': 'bitch', 'eeeeek': 'eek', 'ooooowweeeeee': 'owe','toooooooo': 'too', 'retweee': 'retweet', 'daaayyyysss': 'days', 'hmmmmmmmmmmm': 'hmm', 'heeeeere': 'here', 'fineeee': 'fine', 'naurrr': 'naur', 'liviiddddd': 'livid', 'looool': 'lol', 'stokedddd': 'stoked', 'pleeeease': 'please', 'arghhhhhh': 'argh', 'especiallly': 'especialy', 'supppper': 'super', 'ummmmmm': 'umm', 'seeee': 'see', 'wtfff': 'wtf',  'slowwwwwservice': 'slow service', 'suuuuuuper': 'super', 'whhhaaaaaaaa': 'whaa', 'soooers': 'soers','eeeeeeeeeekkkkkkkk': 'eek', 'don888': 'don8', 'extreeeemly': 'extremely','saaame': 'same', 'booooy': 'boy', 'ooof': 'of', 'bitchhh': 'bitch', 'babbieeeee': 'baby', 'refuseeee': 'refuse', 'jooooooooooke': 'joke', 'juskooo': 'jusko', 'vaxxxed': 'vaxed', 'loooow': 'low', 'slowwwww': 'slow', 'waaaayyyyy': 'way','businesssuccess': 'business success', 'naaaaaaa': 'na', 'goooooooooooooo': 'go', 'fuuuuuuuck': 'fuck', 'shoppinggg': 'shopping', 'bihhh': 'bih', 'pshhh': 'psh', 'saaaaaame': 'same', 'faaaaaaccckkk': 'fuck', 'employeeengagment': 'employee engagement', 'ooop': 'oops', 'sighhhhhhhhhhhhhhhhh': 'sigh', 'howwwww': 'how', 'ooooold': 'old', 'ghhhhggh': 'ghgh', 'shitttt': 'shit', 'tfff': 'tf', 'saaaaame': 'same', 'whaaaaatttt': 'what', 'yeeeeaah': 'yeah', 'seeet': 'set', 'yeee': 'ye', 'uuuugggg': 'ugh',  'pleaseeeee': 'please', 'tipzzzzz': 'tips', 'aarrrrrrrrghhhhhhh': 'argh', 'foreseennnnn': 'foreseen', 'yooouuu': 'you', 'ayyyyyy': 'ay', 'sucksss': 'sucks', 'screeeeeams': 'screams',  'mamoy': 'mamoy',  'whoooooof': 'whoof', 'chillle': 'chile', 'buttttt': 'but', 'actuallly': 'actually', 'wooot': 'woot', 'businessstandard': 'business standard', 'idcccc': 'idc', 'fulllllll': 'full', 'seeeeesh': 'sesh', 'handddd': 'hand', 'smhhh': 'smh', 'sodaaaa': 'soda', 'smallbusinesssupport': 'small busines support','nervessssssss': 'nerves', 'snowww': 'snow', 'ssssssssssoooo': 'so', 'biiit': 'bit', 'wooooooooooo': 'woo', 'swwwwing': 'swing', 'shittty': 'shity', 'yyyy': 'y', 'maaaann': 'man', 'tippppsss': 'tips', 'neeeeeeeeeeeds': 'needs', 'shoppinggggg': 'shopping', 'arghhhhh': 'argh', 'listennn': 'listen', 'neeeever': 'never', 'eeeep': 'eep', 'tooold': 'told', 'bahhh': 'bah', 'sneeeeeeezers': 'sneezers', 'lovexxx': 'love', 'yeeepppp': 'yep', 'aaamazzzing': 'amazing', 'fuuuuuuuuuuuuck': 'fuck','omfggg': 'omfg', 'monthsssss': 'months', 'maaaaaaaaay': 'may', 'arrrrghhh': 'argh', 'dyinggggg': 'dying', 'muuuuuuuuch': 'much', 'wheeew': 'whew','springgggg': 'spring', 'cmonnnnnn': 'cmon', 'thaaat': 'that', 'jeeeez': 'jeez', 'nawww': 'naw', 'shhhh': 'shh', 'waaaaahhh': 'wah', 'thatttt': 'that', 'clutchhhhhhhh': 'clutch', 'squeeee': 'squee', 'maaaaaan': 'man', 'eatttt': 'eat', 'woweee': 'wowe', 'plzzzz': 'plz', 'babyyyyy': 'baby', 'crucialllll': 'crucial', 'beekeeeper': 'beekeeper', 'beeee': 'be', 'morningggg': 'morning', 'stopppppo': 'stop', 'itsalllocal': 'its a local', 'hmrsss': 'hmrs','successsee': 'success','burrrr': 'burr', 'wheeee': 'whee', 'anyoneeee': 'anyone', 'annndd': 'and', 'businesss': 'business', \n",
    "                'incrennnible': 'incredible','lmaooo':'lmao', 'helloooo':'hello','gaaaah': 'gah', 'blowssssssss': 'blows', 'veeeeeggggg': 'veg', 'thiissss': 'this', 'eccc': 'ecc', 'seee': 'see','nahhhhhh': 'nah', 'caaaaasually': 'casually', 'thissss': 'this', 'wellllll': 'well', 'sayinnggg': 'saying', 'dinnerrrr': 'dinner', 'ughhh': 'ugh', 'coffeeeee': 'coffee', 'apppreciate': 'appreciate', 'fooddddddd': 'food', 'waaaa': 'waa', 'blacklivesmattters': 'black lives matters', 'pleaseeeeeeeee': 'please', 'haaaa': 'haa','bagsss': 'bags', 'aiyaaa': 'aiya', 'sixxxxxx': 'six', 'weeeeekssssss': 'weeks', 'aaaaaaaaaaaaaaahhhhhh': 'aah','suuure': 'sure', 'youuuu': 'you', 'freestufffriday': 'free stuff friday', 'bammmmmm': 'bam', 'ewww': 'eew', 'sisss': 'sis', 'yeesss': 'yes', 'sweeeeeeeet': 'sweet', 'girllll': 'girl', 'suuuuck': 'suck', 'wooooooah': 'woah', 'sayweee': 'say we', 'yoooooo': 'yo', 'hellllla': 'hell', 'eeegads': 'egad', 'lmfaaooo': 'lmfao', 'hhhhhg': 'hg', 'fooooooodddd': 'food', 'fooooood': 'food', 'tttil': 'till', 'sayyyy': 'say', 'booooooo': 'boo','faaaar': 'far', 'peopleeeeeeee': 'people', 'auuugh': 'ugh', 'fuuuck': 'fuck', 'sirrr': 'sir', 'plzzz': 'plz', 'yummmm': 'yum', 'sleep': 'sleep', 'alsooo': 'also', 'reallllllly': 'really', 'nahhhhhhting': 'nothing', 'wayyyyyyyy': 'way', 'wheeeeeeeeee': 'whee', 'niiice': 'nice', 'shoooo': 'shoo', 'yeaaah': 'yeah', 'bwahhhahaah': 'haha', 'hahaaaa': 'haha', 'ordererrr': 'orderer', 'behhh': 'beh', 'plsssss': 'pls', 'neeed': 'ned', 'pffffftt': 'pft', 'choooooosing': 'chosing', 'byeeeeeeee': 'bye', 'welllll': 'well', 'naaaame': 'name', 'strugggllleeeee': 'struggle', 'yeahhhh': 'yeah', 'waayyy': 'way', 'aaaw': 'aww', 'yummmmmmmmmmmm': 'yum', 'nahhh': 'nah', 'silaaaaaa': 'sila', 'aaaaahhhhh': 'aah', 'bayyyy': 'bay', \n",
    "                'saaammee': 'same','alll':'all', 'grrr':'grr','yummm':'yummy','yeees':'yes','yasss':'yes','yassss':'yes','whyyy':'why','whaaat':'what','lolll':'lol','weee':'we', 'weeee':'we','tooo':'too','tyyy':'ty','uuugh':'ugh', 'sooo':'so','suuuuper': 'super', 'whyyyyyyy': 'why', 'reaaaaaally': 'really', 'fuccc': 'fuck', 'neverrr': 'never', 'ffff': 'f', 'maaan': 'man', 'asssist': 'assist', 'uuuuu': 'u', 'foreverrrrrer': 'forever',  'aaaaaah': 'aah', 'hellooooooooooo': 'hello', 'mmmmmmmrrrrrrrrrrrr': 'mr',  'mmmrrrrrrrrrrooooooooooorrrm': 'mrorm', 'feee': 'fee', 'freee': 'fre', 'stressss': 'stress', 'yeeeeee': 'yee', 'deeeeerrrrrp': 'derp', 'wooohoooo': 'woohoo', 'mehhhh': 'meh', 'bahhhh': 'bah', 'chillinnnn': 'chilling', 'whattt': 'what', 'lmaooooo': 'lmao', 'daaaaamn': 'damn', 'lolllll': 'lol','psssssst': 'pst', 'carrrrrr': 'car', 'paaaain': 'pain', 'reeeeeee': 're', 'madeeeee': 'made', 'oooow': 'oaw', 'wooooo': 'woo', 'ahhhhhhhhhgg': 'ahg',  'blehhhhh': 'bleh', 'teeeeeny': 'tiny', 'carbzzzz': 'carbz', 'efffing': 'effing', 'kakraaa': 'kakra', 'hoooooo': 'ho', 'jeeeeez': 'jeez', 'hahaaa': 'haha', 'suuuuuuuuure': 'sure', 'hummm': 'hum', 'myoooosive': 'myosive', 'prrrrobably': 'probably', 'waiiit': 'wait','fuuuuuuucked': 'fucked', 'thanksgiviiing': 'thanksgiving', 'waaaant': 'want', 'girrrlll': 'girl', 'gaaaame': 'game', 'whewww': 'whew', 'awwwwkward': 'awkward', 'godddddd': 'god', 'whoooooo': 'who', 'dopppeee': 'dope', 'looolll': 'lol', 'ammmmiwrongggggg': 'am i wrong', 'weeek': 'week', 'yeeeeahhh': 'yeah', 'knowww': 'know', 'nottt': 'not', '4evaaaa': '4eva', 'wiiild': 'wild', 'wooo': 'woo', 'yummmmmmmm': 'yum','aaaall': 'all', 'herreeeee': 'here', 'seasonnnnnn': 'season', 'ummmmmmmm': 'umm', 'goootcha': 'gotcha', 'oooops': 'oops', 'yeeeees': 'yes', 'sheeeeeeeee': 'she', 'mmmfph': 'mfph', 'everrrr': 'ever', 'stufff': 'stuf', 'fkkkk': 'fuck', 'eliteeeeeeee': 'elite', 'maybeeee': 'maybe', 'brrrrr': 'brr', 'yyeesss': 'yes', 'waaaayyy': 'way', 'reaallly': 'really', 'lmaaao': 'lmao', 'eyyy': 'eyy', 'givingggg': 'giving', 'toooo': 'too', 'hiii': 'hi', 'yeeeeeeeees': 'yes', 'arrrgh': 'argh', 'fuuuccccckkk': 'fuck', 'covidvacccine': 'covid vaccine', 'mmmay': 'may', 'wellll': 'well', 'yeeeeesss': 'yes', 'laaaaa': 'la', 'sameeee': 'same', 'goooood': 'good', 'afternoonnnn': 'afternoon', 'fffb': 'fb', 'aaaccedent': 'accident', 'clutcchhhhh': 'clutch', 'whennnnnn': 'when', 'reeeeeeeally': 'really', 'rigghhhttttt': 'right', 'ahhhhhhhh': 'aah','walkkk': 'walk', 'phewwww': 'phew', 'overrr': 'over', 'fuuuuuuuuck': 'fuck', 'aaaah': 'aah', 'classsss': 'class', 'maaaaaybe': 'may be', 'daaaamn': 'damn', 'businessses': 'busineses', 'hooooo': 'hoo', 'whooo': 'who', 'onlineshoppping': 'online shopping','hiswonderfullly': 'his wonderfully', 'paymisseee': 'paymise', 'hoooo': 'ho', 'orrr': 'or', 'waaaaaaah': 'wah', 'aaahhhhhh': 'aah', 'caaalllllll': 'call', 'reeeally': 'really', 'ooookay': 'okay', 'juuuuuuuuuuust': 'just', 'earlyyyy': 'early', 'fuuuccckkk': 'fuck', 'caaaant': 'cannot', 'maaaannnnnn': 'man', 'uuuuuuuu': 'u', 'boooring': 'boring', 'downnnn': 'down', 'cuteee': 'cute', 'gaaaarooooossssssss': 'garos',  'buttttttttt': 'but', 'racccisssttt': 'racist', 'baaaaaaby': 'baby', 'orrrrrrr': 'or', 'reallyyy': 'really', 'aaaanndd': 'and', 'oohhhh': 'oh','anddd': 'and', 'happychildlessspinster': 'happy childless spinster', 'goooooo': 'go', 'baaaack': 'back','succckkks': 'sucks','whaaaaaaaat': 'what', 'thruuuuuuuuuuu': 'thru', 'whooot': 'whot', 'haaaaaaaaate': 'hate', 'nervousssss': 'nervous', 'heeeeeeeelpppppppppp': 'help', 'mannnnnn': 'man', 'oooooooook': 'ok', 'yussss': 'yes', 'bigggggg': 'big', 'igrrr': 'igr', 'botttom': 'bottom', 'muuurrr': 'mur', 'suuucks': 'sucks','juuuust': 'just', 'awwwhyperlink': 'hyperlink', 'speeeedy': 'speedy', 'uuuf': 'uff', 'crazzzzyyyy': 'crazy', 'aaahhhh': 'aah', 'businesssolutions': 'business solutions', 'teeesst': 'test', 'yeeeah': 'yeah', 'gimmme': 'give me', 'thewonderfullly': 'the wonderfully', 'ffffffuck': 'fuck', 'baybeeeee': 'baby', 'backkk': 'back', 'sweetttttt': 'sweet', 'myhhhh': 'myh', 'babyyyy': 'baby', 'todayyyyyy': 'today', 'laaaa': 'la', 'fulllockdown': 'full lockdown', 'goddddd': 'god', 'maaaannn': 'man', 'uuuugh': 'ugh', 'tierrrrrrrrr': 'tier', 'sleeeeeping': 'sleeping', 'sunnnybeemarket': 'sunybee market', 'eeevvveeerrr': 'ever', 'haaarddddddd': 'hard', 'saaaame': 'same', 'pffffizer': 'pfizer', 'mmmmicrobes': 'microbes', 'byeeee': 'bye', 'founddd': 'found', 'serviceeeee': 'service', 'wantttt': 'want', 'buyyyy': 'buy', 'meaaaaaaat': 'meat', 'mmmh': 'mmh', 'wahhhhh': 'wah', 'heeeere': 'here', 'thankssss': 'thanks', 'hummmer': 'humer', 'yeeeeeeeeeaaaaaahhhhhh': 'yeah', 'unhappyblesssss': 'unhappy bless', 'dooor': 'door', 'welll': 'well', 'focuuussssss': 'focus',  'hisss': 'his', 'babyyy': 'baby', 'whateverrrr': 'whatever', 'weeeeee': 'we', 'bzzzzzzz': 'bz', 'naaaa': 'na', 'cosssssss': 'coss','lifeeee': 'life', 'businesssolution': 'business solution', 'pleaseeeeeeee': 'please','yuuum': 'yum', 'buttttttttttt': 'but', 'pissedddd': 'pissed', 'geeesh': 'geesh', 'helloooooo': 'hello', 'fineeeee': 'fine','alllllll': 'all', 'againnnn': 'again', 'muuuuch': 'much', 'ewwwww': 'eww', 'buuuunch': 'bunch', 'freeaaaakk': 'freak', 'aaauugh': 'ugh', 'aaaaahh': 'aah', 'aaaaaaaahhh': 'aah', 'waaaayyyyyy': 'way','hereee': 'here', 'shhh': 'shh', 'gooood': 'good', 'todayyy': 'today', 'outttt': 'out', 'nervesssss': 'nerves', 'yuuuuup': 'yup', 'sheeee': 'she','buildiiiiiing': 'building', 'bankkkkk': 'bank', 'usssa': 'usa', 'weeeee': 'we', 'nohhh': 'noh','bessst': 'best', 'ughnnn': 'ugh', 'worlddddddddddd': 'world', 'yeeeaaaaaahhh': 'yeah', 'whewwww': 'whew', 'suuuuuuucks': 'sucks', 'uhhhm': 'uhm', 'wuuuuu': 'wu', 'aaaaayyyy': 'ayy','10000x': '10x', 'perioddddd': 'period', 'alsoooo': 'also', 'thiccc': 'thick', 'lezzgoooo': 'let go', 'ighttttt': 'ight', 'giiirl': 'girl', 'd7000': 'd70', 'xxxl': 'xl', 'q17775306': 'q175306','hhhhhhhhhh': 'h', '$mmmb': '$mb', 'dxxx': 'dx','000$': '0$','000pm': '0pm', '1119th': '19th', '1000k': '10k', 'appph': 'aph', '$misssadiemaeve': '$misadiemaeve','x10000': 'x10', '0xf8da8ecee025444ad8dcf065ae09cffcd55c6116': '0xf8da8ece0254ad8dcf065ae09cfcd5c616'\n",
    "                   }\n",
    "len(replace_list)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3837ada9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1274"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replace_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74f60951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:01<00:00, 225777.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3265"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = 0\n",
    "rep_words = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    count = 0\n",
    "    words = data['tweet'][i].split(' ')\n",
    "    for word in words:\n",
    "        if word in replace_list:\n",
    "            count += 1\n",
    "            rep_words.append(word)\n",
    "    total_count += count\n",
    "total_count   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90151178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1264/1274 [02:54<00:01,  7.53it/s]/var/folders/2k/fhkt65ls68nbd5xt0lj3sbww0000gn/T/ipykernel_59839/2053584832.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data.loc[:,'tweet'] = data.tweet.str.replace(i,replace_list[i])\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1274/1274 [02:55<00:00,  7.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(replace_list):\n",
    "    data.loc[:,'tweet'] = data.tweet.str.replace(i,replace_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dfec363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195618 218051 87709\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30c528",
   "metadata": {},
   "source": [
    "### Replace slangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d8341c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slangs = {}\n",
    "with open('../../Supplementary data/slangs.txt', 'r') as f:\n",
    "    for i in f:\n",
    "        for j in i.split(','):\n",
    "            j = j.replace('{','')\n",
    "            j = j.replace('}','')\n",
    "            j = j.replace('\\\"','')\n",
    "            a = j.split(':')[0].strip()\n",
    "            b = j.split(':')[1].strip()\n",
    "            slangs[a] = b\n",
    "len(slangs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa4df71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:01<00:00, 220657.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15992"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "slang = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    words = data['tweet'][i].split(' ')\n",
    "    for word in words:\n",
    "        if word in slangs:\n",
    "            count += 1\n",
    "            slang.append(word)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffc678a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact she she saw tons boomers going store protection all\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also workers watch hands curbside delivery\n"
     ]
    }
   ],
   "source": [
    "def replace_slangs(text):\n",
    "    return ' '.join([slangs.get(i.lower(),i.lower()) for i in text.split()])\n",
    "data['tweet'] = data['tweet'].apply(lambda x : replace_slangs(x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ca19795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195618 218209 87739\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813d541",
   "metadata": {},
   "source": [
    "### Find words representing coronavirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64acbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_corona(text):\n",
    "    replace_list = {'coronavirus':'corona','covid19':'corona','covidiot':'corona idiot','covid':'corona'}\n",
    "    words = text.split(' ')\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        for i in replace_list:\n",
    "            if i in word:\n",
    "                word = word.replace(i, replace_list[i])\n",
    "        new_words.append(word) \n",
    "    return ' '.join(new_words)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44139c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:01<00:00, 271052.16it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x : replace_corona(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "013cb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(data['tweet']).split()\n",
    "dic_corona = {}\n",
    "for word in words:\n",
    "    if 'corona' in word:\n",
    "        if word in dic_corona:\n",
    "            dic_corona[word] += 1\n",
    "        else:\n",
    "            dic_corona[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af73294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = dic_corona.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "80e486b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corona ['corona']\n",
      "coronas ['coronas']\n",
      "coronacanada ['corona', 'canada']\n",
      "corona2019 ['corona', '2019']\n",
      "nocorona ['no', 'corona']\n",
      "coronalockdown ['corona', 'lockdown']\n",
      "coronaactnow ['corona', 'act', 'now']\n",
      "coronacalendar ['corona', 'calendar']\n",
      "highriskcorona ['high', 'risk', 'corona']\n",
      "coronataskforce ['corona', 'taskforce']\n",
      "coronacrisis ['corona', 'crisis']\n",
      "coronaoutbreak ['corona', 'outbreak']\n",
      "coronatx ['corona', 'tx']\n",
      "coronapocalypse ['cor', 'on', 'apocalypse']\n",
      "coronaab ['corona', 'ab']\n",
      "coronadining ['corona', 'dining']\n",
      "coronachat ['corona', 'chat']\n",
      "coronaetiquette ['corona', 'etiquette']\n",
      "coronaquarentinedays ['corona', 'qu', 'arent', 'in', 'e', 'days']\n",
      "seattlecorona ['seattle', 'corona']\n",
      "coronalife ['corona', 'life']\n",
      "coronabahamas ['corona', 'bahamas']\n",
      "coronafacts ['corona', 'facts']\n",
      "coronaidiots ['corona', 'idiots']\n",
      "therescoronainsidethere ['there', 's', 'corona', 'inside', 'there']\n",
      "floridacorona ['florida', 'corona']\n",
      "coronacooties ['corona', 'cooties']\n",
      "businessduringcorona ['business', 'during', 'corona']\n",
      "coronadiaries ['corona', 'diaries']\n",
      "coronadatabased ['corona', 'data', 'based']\n",
      "floridacoronaepicenter ['florida', 'corona', 'epicenter']\n",
      "macorona ['macor', 'on', 'a']\n",
      "coronaresource ['corona', 'resource']\n",
      "coronaframeofmind ['corona', 'frame', 'of', 'mind']\n",
      "coronasafe ['corona', 'safe']\n",
      "coronabegona ['corona', 'be', 'gona']\n",
      "mycoronalife ['my', 'corona', 'life']\n",
      "welcometothecptcoronaedition ['welcome', 'to', 'the', 'c', 'pt', 'corona', 'edition']\n",
      "occorona ['oc', 'corona']\n",
      "coronaontario ['corona', 'ontario']\n",
      "coronaeniers ['corona', 'eni', 'ers']\n",
      "texascorona ['texas', 'corona']\n",
      "coronabc ['corona', 'bc']\n",
      "coronaid ['corona', 'id']\n",
      "coronans ['corona', 'ns']\n",
      "coronaisairborne ['corona', 'is', 'airborne']\n",
      "procorona ['pro', 'corona']\n",
      "coronatesting ['corona', 'testing']\n",
      "coronavariant ['corona', 'variant']\n",
      "flcorona ['fl', 'corona']\n",
      "longcorona ['long', 'corona']\n",
      "coronavt ['corona', 'vt']\n",
      "coronanb ['corona', 'nb']\n",
      "coronastories ['corona', 'stories']\n",
      "coronachallenge ['corona', 'challenge']\n",
      "stopthespreadcorona ['stop', 'the', 'spread', 'corona']\n",
      "coronaintexas ['corona', 'in', 'texas']\n",
      "californiacorona ['california', 'corona']\n",
      "coronacoping ['corona', 'coping']\n",
      "coronaisntover ['corona', 'isnt', 'over']\n",
      "coronapositive ['corona', 'positive']\n",
      "coronasnotover ['coronas', 'not', 'over']\n",
      "coronasafety ['corona', 'safety']\n",
      "coronasecure ['corona', 'secure']\n",
      "trumpcorona ['trump', 'corona']\n",
      "dgkscorona ['d', 'g', 'ks', 'corona']\n",
      "coronabreak ['corona', 'break']\n",
      "coronaprecautions ['corona', 'precautions']\n",
      "coronainsa ['corona', 'in', 's', 'a']\n",
      "coronalog ['corona', 'log']\n",
      "coronaizer ['corona', 'ize', 'r']\n",
      "coronakfla ['corona', 'kf', 'la']\n",
      "coronawise ['corona', 'wise']\n",
      "coronahammer ['corona', 'hammer']\n",
      "coronakim ['corona', 'kim']\n",
      "breakthroughcorona ['breakthrough', 'corona']\n",
      "coronachristmas ['corona', 'christmas']\n",
      "markedbycorona ['marked', 'by', 'corona']\n",
      "coronaisnotover ['corona', 'is', 'not', 'over']\n",
      "notgettingcorona ['not', 'getting', 'corona']\n",
      "corona84 ['corona', '84']\n",
      "coronadelmar ['corona', 'delmar']\n",
      "coronado ['coronado']\n",
      "wuhancoronaoutbreak ['wuhan', 'corona', 'outbreak']\n",
      "coronaroundup ['corona', 'roundup']\n",
      "coronaqa ['corona', 'qa']\n",
      "coronausa ['corona', 'usa']\n",
      "coronaupdate ['corona', 'update']\n",
      "iacorona ['i', 'a', 'corona']\n",
      "communityagainstcorona ['community', 'against', 'corona']\n",
      "coronaresponse ['corona', 'response']\n",
      "coronaph ['corona', 'ph']\n",
      "coronamutualaidsea ['corona', 'mutual', 'aid', 'sea']\n",
      "coronapr ['corona', 'pr']\n",
      "coronacalories ['corona', 'calories']\n",
      "coronacarbs ['corona', 'carb', 's']\n",
      "coronalk ['coronal', 'k']\n",
      "highhcoronacasenumbersreported ['high', 'h', 'corona', 'case', 'numbers', 'reported']\n",
      "coronaau ['corona', 'au']\n",
      "coronayyc ['corona', 'yy', 'c']\n",
      "coronaneighbourhoodsquad ['corona', 'neighbourhood', 'squad']\n",
      "coronasl ['coronas', 'l']\n",
      "coronawestchester ['corona', 'westchester']\n",
      "coronaug ['corona', 'ug']\n",
      "coronatimes ['corona', 'times']\n",
      "coronav ['corona', 'v']\n",
      "coronadisparenting ['corona', 'd', 'is', 'parenting']\n",
      "approvalcorona ['approval', 'corona']\n",
      "coronaalberta ['corona', 'alberta']\n",
      "nyccorona ['nyc', 'corona']\n",
      "coronavtj ['corona', 'vt', 'j']\n",
      "mycoronadiary ['my', 'corona', 'diary']\n",
      "probablycorona ['probably', 'corona']\n",
      "cicorona ['ci', 'corona']\n",
      "pandemiccorona ['pandemic', 'corona']\n",
      "coronapositives ['corona', 'positives']\n",
      "coronanewyourk ['corona', 'new', 'your', 'k']\n",
      "coronaquarantine ['corona', 'quarantine']\n",
      "coronanz ['corona', 'nz']\n",
      "coronask ['coronas', 'k']\n",
      "coronaproblems ['corona', 'problems']\n",
      "corona20 ['corona', '20']\n",
      "coronashutdown ['corona', 'shutdown']\n",
      "agricandcorona ['agri', 'c', 'and', 'corona']\n",
      "coronatwitter ['corona', 'twitter']\n",
      "coronaon ['corona', 'on']\n",
      "coronadeutschland ['corona', 'deutschland']\n",
      "corona2019de ['corona', '2019', 'de']\n",
      "lifeaftercorona ['life', 'after', 'corona']\n",
      "coronahumor ['corona', 'humor']\n",
      "coronaus ['corona', 'us']\n",
      "coronaviru ['corona', 'vi', 'ru']\n",
      "lifewithcorona ['life', 'with', 'corona']\n",
      "coronasuggestions ['corona', 'suggestions']\n",
      "coronaaus ['corona', 'a', 'us']\n",
      "coronafood ['corona', 'food']\n",
      "anticorona ['anti', 'corona']\n",
      "wecanbeatcorona ['we', 'can', 'beat', 'corona']\n",
      "coronaamazon ['corona', 'amazon']\n",
      "coronasa ['coronas', 'a']\n",
      "coronaseattle ['corona', 'seattle']\n",
      "raleighbeatscorona ['raleigh', 'beats', 'corona']\n",
      "coronaquestion ['corona', 'question']\n",
      "coronatips ['corona', 'tips']\n",
      "coronahysteria ['corona', 'hysteria']\n",
      "coronatexas ['corona', 'texas']\n",
      "inthetimesofcorona ['in', 'the', 'times', 'of', 'corona']\n",
      "coronacomics ['corona', 'comics']\n",
      "dietinginthetimeofcorona ['dieting', 'in', 'the', 'time', 'of', 'corona']\n",
      "starvinginthetimesofcorona ['starving', 'in', 'the', 'times', 'of', 'corona']\n",
      "comedyinthetimesofcorona ['comedy', 'in', 'the', 'times', 'of', 'corona']\n",
      "coronadiet ['corona', 'diet']\n",
      "beatcorona ['beat', 'corona']\n",
      "komeshacorona ['ko', 'mesha', 'corona']\n",
      "weshallovercomecorona ['we', 'shall', 'overcome', 'corona']\n",
      "coronaopportunitys ['corona', 'opportunity', 's']\n",
      "coronapersonals ['corona', 'personals']\n",
      "coronavir ['corona', 'vi', 'r']\n",
      "coronacooking ['corona', 'cooking']\n",
      "coronavi ['corona', 'vi']\n",
      "coronadreams ['corona', 'dreams']\n",
      "phlcoronafund ['ph', 'l', 'corona', 'fund']\n",
      "coronainequalitygap ['corona', 'inequality', 'gap']\n",
      "linkincorona ['linkin', 'corona']\n",
      "notcorona ['not', 'corona']\n",
      "corona] ['corona']\n",
      "coronacomfortfood ['corona', 'comfort', 'food']\n",
      "coronazim ['corona', 'zim']\n",
      "levityinthetimeofcorona ['levity', 'in', 'the', 'time', 'of', 'corona']\n",
      "coronanj ['corona', 'nj']\n",
      "coronajoys ['corona', 'joys']\n",
      "uniteagainstcorona ['unite', 'against', 'corona']\n",
      "coronadelivery ['corona', 'delivery']\n",
      "coronasupplies ['corona', 'supplies']\n",
      "lifeduringcorona ['life', 'during', 'corona']\n",
      "sgocorona ['s', 'go', 'corona']\n",
      "coronacomfortfoods ['corona', 'comfort', 'foods']\n",
      "coronacolorado ['corona', 'colorado']\n",
      "coronabaking ['corona', 'baking']\n",
      "gocoronacoronago ['go', 'corona', 'corona', 'go']\n",
      "lifeinthetimeofcorona ['life', 'in', 'the', 'time', 'of', 'corona']\n",
      "precorona ['pre', 'corona']\n",
      "coronathings ['corona', 'things']\n",
      "coronaclusterfuck ['corona', 'cluster', 'fuck']\n",
      "coronaessentials ['corona', 'essentials']\n",
      "stayhometogetcoronaout ['stay', 'home', 'to', 'get', 'corona', 'out']\n",
      "thatcoronalife ['that', 'corona', 'life']\n",
      "informationagainstcorona ['information', 'against', 'corona']\n",
      "coronawarriors ['corona', 'warriors']\n",
      "coronamusings ['corona', 'musings']\n",
      "coronasupport ['corona', 'support']\n",
      "coronanyc ['corona', 'nyc']\n",
      "heroesofcorona ['heroes', 'of', 'corona']\n",
      "postcorona ['post', 'corona']\n",
      "coronainsomnia ['corona', 'insomnia']\n",
      "positiveimpactofcorona ['positive', 'impact', 'of', 'corona']\n",
      "jobequitycorona ['job', 'equity', 'corona']\n",
      "coronari ['corona', 'ri']\n",
      "livinginacoronaworld ['living', 'in', 'a', 'corona', 'world']\n",
      "coronakind ['corona', 'kind']\n",
      "coronaworkers ['corona', 'workers']\n",
      "coronakindness ['corona', 'kindness']\n",
      "coronaheroes ['corona', 'heroes']\n",
      "irsfailurecorona ['irs', 'failure', 'corona']\n",
      "corona2020 ['corona', '2020']\n",
      "coronasuffolk ['corona', 'suffolk']\n",
      "coronamb ['corona', 'mb']\n",
      "unsungcoronaheroes ['unsung', 'corona', 'heroes']\n",
      "coronabonus ['corona', 'bonus']\n",
      "coronatrump ['corona', 'trump']\n",
      "coronapandemic ['corona', 'pandemic']\n",
      "ageofcoronav ['age', 'of', 'corona', 'v']\n",
      "ifcoronaneverhappened ['if', 'corona', 'never', 'happened']\n",
      "coronahelp ['corona', 'help']\n",
      "coronation ['coronation']\n",
      "coronama ['corona', 'ma']\n",
      "coronafun ['corona', 'fun']\n",
      "idahocorona ['idaho', 'corona']\n",
      "coronatime ['corona', 'time']\n",
      "coronaiaries ['corona', 'i', 'aries']\n",
      "coloradocorona ['colorado', 'corona']\n",
      "closingthecoronagap ['closing', 'the', 'corona', 'gap']\n",
      "coronareality ['corona', 'reality']\n",
      "coronaiocy ['corona', 'ioc', 'y']\n",
      "coronatest ['corona', 'test']\n",
      "deliveryduringcorona ['delivery', 'during', 'corona']\n",
      "donthecoronacon ['don', 'the', 'corona', 'con']\n",
      "coronawinter ['corona', 'winter']\n",
      "coronadiscoveries ['corona', 'discoveries']\n",
      "catchingcorona ['catching', 'corona']\n",
      "zerocorona ['zero', 'corona']\n",
      "coronaimpact ['corona', 'impact']\n",
      "coronasucks ['corona', 'sucks']\n",
      "coronazero ['corona', 'zero']\n",
      "coronachronicles ['corona', 'chronicles']\n",
      "allwhilebeingcoronasafe ['all', 'while', 'being', 'corona', 'safe']\n",
      "cpmgcoronasolutions ['c', 'pmg', 'corona', 'solutions']\n",
      "coronansw ['corona', 'nsw']\n",
      "corona21 ['corona', '21']\n",
      "coronarecovery ['corona', 'recovery']\n",
      "madcoronachat ['mad', 'corona', 'chat']\n",
      "madcoronakicking ['mad', 'corona', 'kicking']\n",
      "coronacrazy ['corona', 'crazy']\n",
      "coronaanswers ['corona', 'answers']\n",
      "coronaes ['coronae', 's']\n",
      "coronanfld ['corona', 'nfl', 'd']\n",
      "coronaalert ['corona', 'alert']\n",
      "coronarelief ['corona', 'relief']\n",
      "fuckcorona ['fuck', 'corona']\n",
      "coronafriendly ['corona', 'friendly']\n",
      "coronagoodreads ['corona', 'good', 'reads']\n",
      "coronaemergency2021 ['corona', 'emergency', '2021']\n",
      "postcoronalife ['post', 'corona', 'life']\n",
      "coronaroulette ['corona', 'roulette']\n",
      "safeduringcorona ['safe', 'during', 'corona']\n",
      "unite2fightcorona ['unite', '2', 'fight', 'corona']\n",
      "coronad ['corona', 'd']\n",
      "coronavic ['corona', 'vic']\n",
      "coronaact ['corona', 'act']\n",
      "coronaqld ['corona', 'qld']\n",
      "coronacoward ['corona', 'coward']\n",
      "pdxcorona ['pd', 'x', 'corona']\n",
      "sccorona ['sc', 'corona']\n",
      "coronaian ['corona', 'ian']\n",
      "decorona ['de', 'corona']\n",
      "coronaians ['corona', 'ian', 's']\n",
      "coronachristmasclub ['corona', 'christmas', 'club']\n",
      "noncorona ['non', 'corona']\n",
      "coronary ['coronary']\n",
      "coronachina ['corona', 'china']\n",
      "coronavius ['corona', 'vi', 'us']\n",
      "n13corona ['n', '13', 'corona']\n",
      "coronahaaregaindiajeetega ['corona', 'ha', 'are', 'ga', 'india', 'jeet', 'eg', 'a']\n",
      "indiafightscorona ['india', 'fights', 'corona']\n",
      "coronauk ['corona', 'uk']\n",
      "helpfightcorona ['help', 'fight', 'corona']\n",
      "coronachennai ['corona', 'chennai']\n",
      "coronavolunteer ['corona', 'volunteer']\n",
      "coronaandgroceries ['corona', 'and', 'groceries']\n",
      "coronanl ['corona', 'nl']\n",
      "coronainpakistan ['corona', 'in', 'pakistan']\n",
      "coronaoutbreakindia ['corona', 'outbreak', 'india']\n",
      "coronatoronto ['corona', 'toronto']\n",
      "coronainindia ['corona', 'in', 'india']\n",
      "coronazimbabwe ['corona', 'zimbabwe']\n",
      "goafightscorona ['go', 'a', 'fights', 'corona']\n",
      "coronawarrior ['corona', 'warrior']\n",
      "coronaindia ['corona', 'india']\n",
      "indorefightscorona ['indore', 'fights', 'corona']\n",
      "corona2019ireland ['corona', '2019', 'ireland']\n",
      "coronao ['corona', 'o']\n",
      "coronanigeria ['corona', 'nigeria']\n",
      "coronaout ['corona', 'out']\n",
      "coronatales ['corona', 'tales']\n",
      "stopcorona ['stop', 'corona']\n",
      "indiafightcorona ['india', 'fight', 'corona']\n",
      "kingstonrotarycoronaisolation ['kingston', 'rotary', 'corona', 'isolation']\n",
      "coronaharega ['corona', 'hare', 'ga']\n",
      "coronahelpers ['corona', 'helpers']\n",
      "gautengcorona ['gauteng', 'corona']\n",
      "coronadc ['corona', 'dc']\n",
      "bhilaifighttocorona ['bhil', 'a', 'i', 'fight', 'to', 'corona']\n",
      "combatcorona ['combat', 'corona']\n",
      "udaanfightscorona ['uda', 'an', 'fights', 'corona']\n",
      "modicoronaaddress ['modi', 'corona', 'address']\n",
      "punefightscorona ['pune', 'fights', 'corona']\n",
      "indiaawaitscoronaaid ['india', 'awaits', 'corona', 'aid']\n",
      "fightingcoronatogether ['fighting', 'corona', 'together']\n",
      "ahmedabadfightscorona ['ahmedabad', 'fights', 'corona']\n",
      "padrescoronatwitter ['padres', 'corona', 'twitter']\n",
      "coronanola ['corona', 'nola']\n",
      "coronaneworleans ['corona', 'new', 'orleans']\n",
      "camdencountycoronahotline ['camden', 'county', 'corona', 'hotline']\n",
      "coronarukona ['corona', 'ru', 'kona']\n",
      "stopthespreadofcorona ['stop', 'the', 'spread', 'of', 'corona']\n",
      "telanganacoronapublictaskforce ['te', 'langan', 'a', 'corona', 'public', 'taskforce']\n",
      "coronaroko ['corona', 'rok', 'o']\n",
      "telanganafightscorona ['te', 'langan', 'a', 'fights', 'corona']\n",
      "keralafightscorona ['kerala', 'fights', 'corona']\n",
      "odishafightscorona ['o', 'dish', 'a', 'fights', 'corona']\n",
      "survivingcorona ['surviving', 'corona']\n",
      "coronalockdownindia ['corona', 'lockdown', 'india']\n",
      "coronainmyarashtra ['corona', 'in', 'mya', 'rasht', 'ra']\n",
      "mycoronastory ['my', 'corona', 'story']\n",
      "defeatcorona ['defeat', 'corona']\n",
      "dronethecorona ['drone', 'the', 'corona']\n",
      "coronanorway ['corona', 'norway']\n",
      "coronaservices ['corona', 'services']\n",
      "coronalifeservices ['corona', 'life', 'services']\n",
      "coronahelper ['corona', 'helper']\n",
      "spreadlovenotcorona ['spread', 'love', 'not', 'corona']\n",
      "coronabilluk ['corona', 'bill', 'uk']\n",
      "coronakenya ['corona', 'kenya']\n",
      "coronaireland ['corona', 'ireland']\n",
      "coronafreepakistan ['corona', 'free', 'pakistan']\n",
      "coronaupdatesindia ['corona', 'updates', 'india']\n",
      "coronadallas ['corona', 'dallas']\n",
      "coronacrisisuk ['corona', 'crisis', 'uk']\n",
      "coronaaustralia ['corona', 'australia']\n",
      "corona19virus ['corona', '19', 'virus']\n",
      "corona2019au ['corona', '2019', 'au']\n",
      "coronawatch ['corona', 'watch']\n",
      "corona2019us ['corona', '2019', 'us']\n",
      "coronake ['corona', 'ke']\n",
      "duringcorona ['during', 'corona']\n",
      "coronasouthafrica ['corona', 'south', 'africa']\n",
      "coronawalkout ['corona', 'walkout']\n",
      "corona19 ['corona', '19']\n",
      "coronaeyeopener ['corona', 'eye', 'opener']\n",
      "coronahv ['corona', 'hv']\n",
      "coronajay2020 ['corona', 'jay', '2020']\n",
      "utahcorona ['utah', 'corona']\n",
      "coronatucson ['corona', 'tucson']\n",
      "waroncorona ['war', 'on', 'corona']\n",
      "coronaqatar ['corona', 'qatar']\n",
      "qatarcoronawatch ['qatar', 'corona', 'watch']\n",
      "coronasverige ['corona', 'sverige']\n",
      "coronaitaly ['corona', 'italy']\n",
      "buildforcorona ['build', 'for', 'corona']\n",
      "italycorona ['italy', 'corona']\n",
      "coronamx ['corona', 'mx']\n",
      "coronaneighbornetwork ['corona', 'neighbor', 'network']\n",
      "soscorona ['sos', 'corona']\n",
      "womencoronascot ['women', 'corona', 'scot']\n",
      "jamshedpurfightscorona ['jamshedpur', 'fights', 'corona']\n",
      "coronagroceryaid ['corona', 'grocery', 'aid']\n",
      "bokarofightscorona ['bo', 'karo', 'fights', 'corona']\n",
      "indoredefeatscorona ['indore', 'defeats', 'corona']\n",
      "ranchifightscorona ['ranchi', 'fights', 'corona']\n",
      "nobafightscorona ['no', 'ba', 'fights', 'corona']\n",
      "rutomurdersvscorona ['ru', 'to', 'murders', 'vs', 'corona']\n",
      "coronabalt ['corona', 'bal', 't']\n",
      "coronaservice ['corona', 'service']\n",
      "coronapei ['corona', 'pei']\n",
      "ukbidscorona ['uk', 'bids', 'corona']\n",
      "corona213 ['corona', '213']\n",
      "coronaai ['corona', 'a', 'i']\n",
      "coronasharyana ['coronas', 'haryana']\n",
      "medstudentcorona ['med', 'student', 'corona']\n",
      "helpdeskforcorona ['help', 'desk', 'for', 'corona']\n",
      "coronasainikas ['coronas', 'a', 'inika', 's']\n",
      "coronacare ['corona', 'care']\n",
      "hrcorona ['hr', 'corona']\n",
      "centrocomunitariocorona ['centro', 'com', 'unit', 'a', 'rio', 'corona']\n",
      "artversuscorona ['art', 'versus', 'corona']\n",
      "disabilityandcorona ['disability', 'and', 'corona']\n",
      "coronaqc ['corona', 'qc']\n",
      "coronamontreal ['corona', 'montreal']\n",
      "closedforcorona ['closed', 'for', 'corona']\n",
      "coronaisactuallycroatoan ['corona', 'is', 'actually', 'cro', 'a', 'to', 'an']\n",
      "corona2019india ['corona', '2019', 'india']\n",
      "coronafighters ['corona', 'fighters']\n",
      "coronameme ['corona', 'meme']\n",
      "coronamemes ['corona', 'memes']\n",
      "coronagrocery ['corona', 'grocery']\n",
      "coronaklang ['corona', 'klang']\n",
      "coronaparenting ['corona', 'parenting']\n",
      "beatingcoronatogether ['beating', 'corona', 'together']\n",
      "coronap ['corona', 'p']\n",
      "copingwithcorona ['coping', 'with', 'corona']\n",
      "fightcorona ['fight', 'corona']\n",
      "coronakeepsmeathome ['corona', 'keeps', 'meat', 'home']\n",
      "dronescombatcorona ['drones', 'combat', 'corona']\n",
      "canadacorona ['canada', 'corona']\n",
      "aapfcorona ['a', 'apf', 'corona']\n",
      "coronapakistan ['corona', 'pakistan']\n",
      "pakistanfightscorona ['pakistan', 'fights', 'corona']\n",
      "coronarelieffund ['corona', 'relief', 'fund']\n",
      "coronaheroesfund ['corona', 'heroes', 'fund']\n",
      "lancscorona ['lanc', 's', 'corona']\n",
      "coronaupdates ['corona', 'updates']\n",
      "coronacomedy ['corona', 'comedy']\n",
      "amdavadfightscorona ['am', 'dav', 'ad', 'fights', 'corona']\n",
      "coronakuwait ['corona', 'kuwait']\n",
      "netcoronaappeal ['net', 'corona', 'appeal']\n",
      "togetheragainstcorona ['together', 'against', 'corona']\n",
      "carenotcorona ['care', 'not', 'corona']\n",
      "coronanews ['corona', 'news']\n",
      "coronawin ['corona', 'win']\n",
      "coronaisrealke ['corona', 'is', 'real', 'ke']\n",
      "avoidcorona ['avoid', 'corona']\n",
      "coronaoutbrea ['corona', 'out', 'brea']\n",
      "coronaschool ['corona', 'school']\n",
      "give4corona ['give', '4', 'corona']\n",
      "coronay ['corona', 'y']\n",
      "strongagainstcorona ['strong', 'against', 'corona']\n",
      "fishsafightscorona ['fish', 's', 'a', 'fights', 'corona']\n",
      "coronaextremelyvulnerable ['corona', 'extremely', 'vulnerable']\n",
      "coronamutualaid ['corona', 'mutual', 'aid']\n",
      "coronamas ['corona', 'mas']\n",
      "coronasecondwave ['corona', 'second', 'wave']\n",
      "coronavaccine ['corona', 'vaccine']\n",
      "totallycorona ['totally', 'corona']\n",
      "coronaindiainfo ['corona', 'india', 'info']\n",
      "coronaresources ['corona', 'resources']\n",
      "coronasos ['coronas', 'os']\n",
      "coronaemergency ['corona', 'emergency']\n",
      "coronavolunteers ['corona', 'volunteers']\n",
      "coronahyderabad ['corona', 'hyderabad']\n",
      "coronavaccination ['corona', 'vaccination']\n",
      "icorona ['i', 'corona']\n",
      "coronafooddelivery ['corona', 'food', 'delivery']\n",
      "coronatnlockdown ['corona', 'tn', 'lockdown']\n",
      "coronasafenepal ['corona', 'safe', 'nepal']\n",
      "coronaindiahelp ['corona', 'india', 'help']\n",
      "chennaicoronahelp ['chennai', 'corona', 'help']\n",
      "chennaicoronasupport ['chennai', 'corona', 'support']\n",
      "coronasecondwaveinindia ['corona', 'second', 'wave', 'in', 'india']\n",
      "gujaratcoronasupport ['gujarat', 'corona', 'support']\n",
      "coronamalaysia ['corona', 'malaysia']\n",
      "coronashopping ['corona', 'shopping']\n",
      "coronadash ['corona', 'dash']\n",
      "coronaisreal ['corona', 'is', 'real']\n",
      "coronaprotectothers ['corona', 'protect', 'others']\n",
      "coronaplanb ['corona', 'plan', 'b']\n",
      "shoppinginthetimeofcorona ['shopping', 'in', 'the', 'time', 'of', 'corona']\n",
      "killcoronaideas ['kill', 'corona', 'ideas']\n",
      "coronasupportmysuru ['corona', 'support', 'my', 'suru']\n",
      "coronamamas ['corona', 'mamas']\n",
      "arcorona ['ar', 'corona']\n"
     ]
    }
   ],
   "source": [
    "import wordninja\n",
    "for word in new_words:\n",
    "    print(word, wordninja.split(word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4fc7b0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195618 218209 87739\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc75bc",
   "metadata": {},
   "source": [
    "### Divide connected words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "27faa658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:03<00:00, 116644.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15018"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of connected words\n",
    "connect_words =[]\n",
    "for i in tqdm(range(len(data))):\n",
    "    words = data['tweet'][i].split(' ')\n",
    "    for word in words:\n",
    "        if len(word) >12 and word not in connect_words:\n",
    "            connect_words.append(word)\n",
    "len(connect_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "86dda557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja\n",
    "def split_words(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if len(word) > 12:\n",
    "            new_words.extend(wordninja.split(word))\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2dd0b011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:03<00:00, 110322.54it/s]\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].progress_apply(lambda x:split_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9a3008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195868 218561 87929\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca1399",
   "metadata": {},
   "source": [
    "### Remove words with single letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab22cf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277156"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find number of words with only one letter\n",
    "tweets = np.array(data['tweet'])\n",
    "count = 0\n",
    "word_list = []\n",
    "for tweet in tweets:\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if len(word) <= 1:\n",
    "            word_list.append(word)\n",
    "            count += 1\n",
    "    \n",
    "    \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4704eaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:01<00:00, 192666.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact she she saw tons boomers going store protection all\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also workers watch hands curbside delivery\n"
     ]
    }
   ],
   "source": [
    "def remove_single_words(text):\n",
    "    single_words = ['o', 'd', 'r', 'k', 'e', 'y', 'n', 'b', 'c', 'h', 'a', 's', 'g', 'u', 'p', 'w', 'z', 'm', 'x', 'f',\n",
    "                'j', 'v', 'l', 't', 'q']\n",
    "    words = text.split()\n",
    "    lst = []\n",
    "    for word in words:\n",
    "        if word not in single_words:\n",
    "            lst.append(word)\n",
    "    return ' '.join(lst)\n",
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x : remove_single_words(x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5355ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195868 218561 87929\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ac67d",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "455824ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f45569bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [03:10<00:00, 1935.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stop grocery store curbside pickup they put your orderer your trunk contact she she saw ton boomer go store protection all\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also worker watch hand curbside delivery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "tqdm.pandas()\n",
    "data[\"tweet\"] = data[\"tweet\"].progress_apply(lambda text: lemmatize_words(text))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6160c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195869 218569 87929\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3454aaec",
   "metadata": {},
   "source": [
    "### Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7e7f8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8eb81ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding words with wrong spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "18998d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(data['tweet']).split()\n",
    "misspelled_words = spell.unknown(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cf5e6d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55552"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspelled_words = list(set(misspelled_words))\n",
    "len(misspelled_words)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bde29f9",
   "metadata": {},
   "source": [
    "spell.correction('hyperlink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d9572a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55552/55552 [1:04:12<00:00, 14.42it/s]\n"
     ]
    }
   ],
   "source": [
    "corrected_words = {}\n",
    "for i in tqdm(misspelled_words):\n",
    "    corrected = spell.correction(i)\n",
    "    if corrected != None and i != corrected:\n",
    "        corrected_words[i] = corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dfd079f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35574"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1edbb556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:01<00:00, 361480.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def correct_spellings(text, corrected):\n",
    "    words = text.split(' ')\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in corrected:\n",
    "            new_words.append(corrected[word])\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "            \n",
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x:correct_spellings(x, corrected_words))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "864c61d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195869 220145 89060\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a0554",
   "metadata": {},
   "source": [
    "### Remove non-english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f2fceed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "80efa11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6610274/6610274 [00:20<00:00, 322165.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34028"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find non-english words\n",
    "non_english = []\n",
    "eng_words = set(nltk.corpus.words.words())\n",
    "words = ' '.join(data['tweet']).split()\n",
    "for i in tqdm(words):\n",
    "    if i.lower() not in eng_words and i.lower() not in non_english:\n",
    "        non_english.append(i)\n",
    "len(non_english)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0f178e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "eng_words = set(nltk.corpus.words.words())\n",
    "keep = ['timelsots','timeslot','convenience','efficient','dissapoint','convenient','inconvenience','substitute',\n",
    "       'recycling', 'recycle', 'pandemic','omnichannel', 'covid', 'curbside','hyperlink','biodegradable', 'hyperlink']\n",
    "\n",
    "def remove_nonenglish_words(sent,eng_words,keep):\n",
    "    words = [w for w in nltk.wordpunct_tokenize(sent) if w.lower() in eng_words or w.lower() in keep]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6b4c4ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:02<00:00, 155129.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stop grocery store curbside pickup they put your ordered your trunk contact she she saw ton boomer go store protection all\n",
      "they also say it probably get food restaurant curb side versus grocery shopping right now less exposure they close also worker watch hand curbside delivery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x:remove_nonenglish_words(x,eng_words,keep))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be396ee",
   "metadata": {},
   "source": [
    "### Find words related to Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e6b5a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_curb(text):\n",
    "    text = text.replace('curb side', 'curbside')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f8a392d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:00<00:00, 1488164.99it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x: replace_curb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1eb86389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_covid(text):\n",
    "    text = text.replace('corona', 'covid')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1a68648b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368413/368413 [00:00<00:00, 1496361.95it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x: replace_covid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3e7c78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(data['tweet']).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4fa15b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368413"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cda1e7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Consumer         8377\n",
       "Advertisement    8092\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f7ccbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Cleaned_data_for_classification.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc160e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
