{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bed64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import emoji\n",
    "import contractions\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "pd.options.display.max_colwidth=200\n",
    "pd.options.display.max_rows=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c160727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/fhkt65ls68nbd5xt0lj3sbww0000gn/T/ipykernel_59839/3005909632.py:2: DtypeWarning: Columns (0,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('/Users/rosni/Library/CloudStorage/OneDrive-NJIT/Dissertation/curbside/Data/Python files/Final_data.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "371272"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f735f01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'author id', 'created_at', 'geo', 'id', 'lang', 'like_count',\n",
       "       'quote_count', 'reply_count', 'retweet_count', 'source', 'tweet',\n",
       "       'Category', 'channel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821cbaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delivery    329103\n",
       "curbside     42162\n",
       "Name: channel, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['channel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "108a8ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Consumer         8379\n",
       "Advertisement    8092\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b28f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368414"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset='tweet')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0101bcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Consumer         8377\n",
       "Advertisement    8092\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d53a13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Category</th>\n",
       "      <th>channel</th>\n",
       "      <th>new_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>47199132.0</td>\n",
       "      <td>2015-01-21 19:23:29+00:00</td>\n",
       "      <td></td>\n",
       "      <td>5.580000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>@jhineline meanwhile up the street, grocery shoppers park at trader joe's using the convenienttent curbside.</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>curbside</td>\n",
       "      <td>@jhineline meanwhile up the street, grocery shoppers park at trader joe's using the convenienttent curbside.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>34079088.0</td>\n",
       "      <td>2015-01-13 15:29:39+00:00</td>\n",
       "      <td></td>\n",
       "      <td>5.550000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>.@walmart is in trial phase of online orderering + curbside pickup with grocery, truly a disruptor and convenienttent! #nrf15 #futureofretail</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>curbside</td>\n",
       "      <td>.@walmart is in trial phase of online orderering + curbside pickup with grocery, truly a disruptor and convenienttent! #nrf15 #futureofretail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>119482829.0</td>\n",
       "      <td>2015-01-09 23:01:37+00:00</td>\n",
       "      <td></td>\n",
       "      <td>5.540000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>thank you @harristeeter for providing a curbside grocery pick up. i love you. that's all.</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>curbside</td>\n",
       "      <td>thank you @harristeeter for providing a curbside grocery pick up. i love you. that's all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>718558608.0</td>\n",
       "      <td>2015-01-08 04:08:17+00:00</td>\n",
       "      <td></td>\n",
       "      <td>5.530000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>i can't get to the grocery store weekly to buy food &amp;amp; you want me to haul trash across town? support curbside #recycle</td>\n",
       "      <td>Advertisement</td>\n",
       "      <td>curbside</td>\n",
       "      <td>i can't get to the grocery store weekly to buy food &amp;amp; you want me to haul trash across town? support curbside #recycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>16347551.0</td>\n",
       "      <td>2015-02-24 19:47:44+00:00</td>\n",
       "      <td>01fbe706f872cb32</td>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>loving @peapod delivers  i was in &amp;amp; out of the grocery store with a week of food using their curbside pick up in 2 minutes. #gamechanging</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>curbside</td>\n",
       "      <td>loving @peapod delivers  i was in &amp;amp; out of the grocery store with a week of food using their curbside pick up in 2 minutes. #gamechanging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index    author id                 created_at               geo  \\\n",
       "0     5   47199132.0  2015-01-21 19:23:29+00:00                     \n",
       "1     8   34079088.0  2015-01-13 15:29:39+00:00                     \n",
       "2     9  119482829.0  2015-01-09 23:01:37+00:00                     \n",
       "3    10  718558608.0  2015-01-08 04:08:17+00:00                     \n",
       "4    14   16347551.0  2015-02-24 19:47:44+00:00  01fbe706f872cb32   \n",
       "\n",
       "             id lang  like_count  quote_count  reply_count  retweet_count  \\\n",
       "0  5.580000e+17   en         0.0          0.0          0.0            0.0   \n",
       "1  5.550000e+17   en         1.0          0.0          0.0            2.0   \n",
       "2  5.540000e+17   en         0.0          0.0          1.0            0.0   \n",
       "3  5.530000e+17   en         0.0          0.0          0.0            0.0   \n",
       "4  5.700000e+17   en         0.0          0.0          2.0            0.0   \n",
       "\n",
       "               source  \\\n",
       "0  Twitter for iPhone   \n",
       "1  Twitter for iPhone   \n",
       "2  Twitter for iPhone   \n",
       "3  Twitter for iPhone   \n",
       "4  Twitter for iPhone   \n",
       "\n",
       "                                                                                                                                           tweet  \\\n",
       "0                                   @jhineline meanwhile up the street, grocery shoppers park at trader joe's using the convenienttent curbside.   \n",
       "1  .@walmart is in trial phase of online orderering + curbside pickup with grocery, truly a disruptor and convenienttent! #nrf15 #futureofretail   \n",
       "2                                                      thank you @harristeeter for providing a curbside grocery pick up. i love you. that's all.   \n",
       "3                     i can't get to the grocery store weekly to buy food &amp; you want me to haul trash across town? support curbside #recycle   \n",
       "4  loving @peapod delivers  i was in &amp; out of the grocery store with a week of food using their curbside pick up in 2 minutes. #gamechanging   \n",
       "\n",
       "        Category   channel  \\\n",
       "0       Consumer  curbside   \n",
       "1       Consumer  curbside   \n",
       "2       Consumer  curbside   \n",
       "3  Advertisement  curbside   \n",
       "4       Consumer  curbside   \n",
       "\n",
       "                                                                                                                                       new_tweet  \n",
       "0                                   @jhineline meanwhile up the street, grocery shoppers park at trader joe's using the convenienttent curbside.  \n",
       "1  .@walmart is in trial phase of online orderering + curbside pickup with grocery, truly a disruptor and convenienttent! #nrf15 #futureofretail  \n",
       "2                                                      thank you @harristeeter for providing a curbside grocery pick up. i love you. that's all.  \n",
       "3                     i can't get to the grocery store weekly to buy food &amp; you want me to haul trash across town? support curbside #recycle  \n",
       "4  loving @peapod delivers  i was in &amp; out of the grocery store with a week of food using their curbside pick up in 2 minutes. #gamechanging  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "data['Year'] = data['created_at'].dt.year\n",
    "data['Month'] = data['created_at'].dt.month_name()\n",
    "data['Day'] = data['created_at'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c2c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 00:07:25+00:00\n",
      "2022-11-29 23:51:09+00:00\n"
     ]
    }
   ],
   "source": [
    "print(min(data['created_at']))\n",
    "print(max(data['created_at']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a91d9742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words:  9761584\n",
      "Average number of words per tweet:  26.5\n",
      "Minimum number of words per tweet:  2\n",
      "Maximum number of words per tweet:  108\n",
      "Maximum number of words per tweet:  108\n",
      "Median number of words per tweet:  23.0\n"
     ]
    }
   ],
   "source": [
    "count = data['tweet'].str.split().str.len()\n",
    "print(\"Total number of words: \", count.sum())\n",
    "print(\"Average number of words per tweet: \", round(count.mean(),2))\n",
    "print(\"Minimum number of words per tweet: \", round(count.min(),2))\n",
    "print(\"Maximum number of words per tweet: \", round(count.max(),2))\n",
    "print(\"Maximum number of words per tweet: \", round(count.max(),2))\n",
    "print(\"Median number of words per tweet: \", round(count.median(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a1221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30047"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data[data['created_at'] >= '2022-01-01']\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "182afcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51f6d667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'index', 'author id', 'created_at', 'geo', 'id', 'lang',\n",
       "       'like_count', 'quote_count', 'reply_count', 'retweet_count', 'source',\n",
       "       'tweet', 'Category', 'channel', 'new_tweet', 'Year', 'Month', 'Day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aef7ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('level_0', axis=1)\n",
    "data = data.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88f7fa41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author id', 'created_at', 'geo', 'id', 'lang', 'like_count',\n",
       "       'quote_count', 'reply_count', 'retweet_count', 'source', 'tweet',\n",
       "       'Category', 'channel', 'new_tweet', 'Year', 'Month', 'Day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664858f",
   "metadata": {},
   "source": [
    "### Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "199bb2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@cdcarter13 my wife stopped at the grocery store for curbside pickup where they put your orderer in your trunk with no contact. while she was there she saw tons of boomers going into the store with no protection at all.\n",
      "@padme_kenobi @kaylive @fakemorganhope @itsaronbitch @robwhite010 @davidfholt they also say it’s probably safer to get food from a restaurant curb side versus grocery shopping right now. less exposure. they shouldn’t close. also workers should watch hands after each curbside delivery.\n"
     ]
    }
   ],
   "source": [
    "def lower_case(txt):\n",
    "    return txt.lower()\n",
    "data['tweet'] = data['tweet'].apply(lambda x : lower_case(x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c82e19",
   "metadata": {},
   "source": [
    "### Get number of tweets with retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1178750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    368412\n",
       "True          1\n",
       "Name: tweet, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get number of tweets with retweet\n",
    "def retweet_count(text):\n",
    "    if 'cc@' in text:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df = data['tweet'].apply(lambda x: retweet_count(x))\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4adc7b7",
   "metadata": {},
   "source": [
    "### Get all html tags in tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feed1fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_html_tags(text):\n",
    "    tags = []\n",
    "    html = re.findall('<.*?>',text)\n",
    "    if html != None:\n",
    "        tags.extend(html)\n",
    "    return tags\n",
    "\n",
    "html_tags = data['tweet'].apply(lambda x: get_html_tags(x))\n",
    "html_tags = [i for i in html_tags if len(i)!=0]\n",
    "html_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3d349",
   "metadata": {},
   "source": [
    "### Get all markdown links in tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00f5cfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[start using drones](https://t.co/zs7mt3cr8o)']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_html_tags(text):\n",
    "    tags = []\n",
    "    html = re.findall(r'\\[.*?\\]\\(.*?\\)',text)\n",
    "    if html != None:\n",
    "        tags.extend(html)\n",
    "    return tags\n",
    "\n",
    "html_tags = data['tweet'].apply(lambda x: get_html_tags(x))\n",
    "html_tags = [i for i in html_tags if len(i)!=0]\n",
    "html_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af0edeb",
   "metadata": {},
   "source": [
    "### Get html elements in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d43dbf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&lt; = <\n"
     ]
    }
   ],
   "source": [
    "html_elements = []\n",
    "for i in range(len(data)):\n",
    "    regexp = \"&.+?;\" \n",
    "    html_elements.extend(re.findall(regexp,data['tweet'][i]))\n",
    "html_elements = set(html_elements)\n",
    "\n",
    "from html import unescape\n",
    "for i in html_elements:\n",
    "    print(i,\"=\", unescape(i))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621e6b6",
   "metadata": {},
   "source": [
    "### Replace html elements in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "766e7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('&amp;', '&', x))\n",
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('&gt;', '>', x))\n",
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('&lt;', '<', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7473a01",
   "metadata": {},
   "source": [
    "### Remove mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd6b6533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my wife stopped at the grocery store for curbside pickup where they put your orderer in your trunk with no contact. while she was there she saw tons of boomers going into the store with no protection at all.\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('@(\\w+)', '', x))\n",
    "print(data['tweet'][5003])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351fb58c",
   "metadata": {},
   "source": [
    "### Replace URL with 'hyperlink'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9817c0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " amazon has grocery delivery??? where the hell have i been???\n"
     ]
    }
   ],
   "source": [
    "print(data['tweet'][50015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22a5c722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my wife stopped at the grocery store for curbside pickup where they put your orderer in your trunk with no contact. while she was there she saw tons of boomers going into the store with no protection at all.\n",
      " amazon has grocery delivery??? where the hell have i been???\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub(r'http\\S+', 'hyperlink', x))\n",
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub(r'www\\.[^ ]+', 'hyperlink', x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][50015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55423193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_link(data):\n",
    "    words = ' '.join(data['tweet']).split()\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word == 'hyperlink':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_i(data):\n",
    "    words = ' '.join(data['tweet']).split()\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word == 'i':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_my(data):\n",
    "    words = ' '.join(data['tweet']).split()\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word == 'my':\n",
    "            count += 1\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbb9c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194361 167064 87123\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7794a",
   "metadata": {},
   "source": [
    "### Replace email ids with keyword email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd9bff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my wife stopped at the grocery store for curbside pickup where they put your orderer in your trunk with no contact. while she was there she saw tons of boomers going into the store with no protection at all.\n",
      "      they also say it’s probably safer to get food from a restaurant curb side versus grocery shopping right now. less exposure. they shouldn’t close. also workers should watch hands after each curbside delivery.\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})', r'email',x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "836e8cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194361 167064 87123\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde65c1a",
   "metadata": {},
   "source": [
    "### Check number of tweet with emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc8eaaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 368413/368413 [02:18<00:00, 2664.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{False: 357119, True: 11294}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def check_emoji(text):\n",
    "    emo = emoji.UNICODE_EMOJI\n",
    "    emojis = {**emo['en'],**emo['es'],**emo['pt'],**emo['it'],**emo['fr'],**emo['de']}\n",
    "    allchars = [i for i in text]\n",
    "    emoji_list = [c for c in allchars if c in emojis]\n",
    "    words = text.split()\n",
    "    contain_emoji = False\n",
    "    for word in words:\n",
    "        if word in emoji_list:\n",
    "            contain_emoji = True\n",
    "        else:\n",
    "            contain_emoji = False\n",
    "    return contain_emoji\n",
    "\n",
    "tqdm.pandas()\n",
    "count_emoji = data['tweet'].progress_apply(lambda x: check_emoji(x))\n",
    "print(count_emoji.value_counts().to_dict())\n",
    "#print(f'The {count_emoji.value_counts()[1]/count_emoji.value_counts().sum()} of the tweets have emojis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e88eb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {'😩': 'tired', '🤦': 'embarassment', '😅': 'happy', '🤣': 'funny', '😎': 'awesome','😂': 'happy', '🙌': 'joy', '👀': 'curious', '👌': 'approval', '🤢': 'disgust', '🙂': 'happy', '😭': 'unhappy', '💵': 'dollar',  '❤': 'love', '🤷': 'ignorance','😬': 'embarassment', '✔': 'approval', \n",
    "              '😃': 'happy', '🎯': 'success', '😍': 'love','😤': 'frustration', '🙏': 'gratitude','👏': 'applaud', '👍': 'approval', '🤔': 'thinking', '🙃': 'silliness', '💕': 'love', '🎉': 'celebration','🤩': 'fascinate', '😖': 'frustration', '🤬': 'angry', '🤗': 'happy', '💃': 'happy', '🤯': 'frustration', '😑': 'frustration','✨': 'excitement', '💞': 'love', '🙄': 'disapproval', '😉': 'joke', '😄': 'happy', '💜': 'love', '😡': 'angry','😁': 'happy', '😱': 'fearful:', '😒': 'unhappy', \n",
    "              '😋': 'tasty', '😐': 'frustration', '✌': 'peace', '😆': 'funny','😇': 'blessing', '😔': 'regretful', '💯': 'agree','🙈': 'disbelieving', '😕': 'confused', '😊': 'happy','😫': ':tired', '💰': 'money','🤪': 'funny', '😿': 'sad', '😣': 'frustration', '🥴': 'confused', '🤞': 'luck','🙅': 'disapproval', '🤘': 'love', '💩': 'angry','😳': 'embarrassment',  '💛': 'love', '☹': 'sad','😜': 'funny','🥰': 'love', '🙆': 'approval', '🤤': 'longing', '😏': 'smug', \n",
    "              '💚': 'love', '🥺': 'pleading', '🤥': 'lying','💖': 'love',  '🧐': 'ponder', '😥': ':sad', '🙇': 'apology', '😢': 'sad','🤕': 'hurt', '☺': 'happy', '😶': 'silence', '😧': 'sad', '💔': 'sad','♥': 'love','🤓': 'nerd', '😲': 'surprise', '🧡': 'love', '🚫': ':prohibited:','😰': ':anxious', '😝': 'funny', '👎': 'disapproval','😀': 'happy', '🌞': 'happy','😵': 'disbelief','💙': 'love', '😯': 'surprise', '🙁': 'angry','😞': ':disappoint', '😘': 'love', '😺': 'happy',\n",
    "              '🤨': 'confused', '😠': 'angry', '😪': ':sleepy','😦': 'sad', '💗': 'love','😓': 'frustration', '😨': ':fearful', '😌': ':relieved', '❣': 'love',  '🤡': 'funny','😮': 'amazed', '🥳': 'happy','😈': 'trouble',  '🤮': 'disgust', '😴': ':sleeping','😻': 'love', '🥶': 'cold', '🤭': 'embarrassment,', '🖕': 'angry', '💘': 'love', '🤑': 'money','🥲': 'gratitude', '🤟': 'love', '💫': 'dizzy', '😟': 'worry', '🖤': 'love','💸': 'money', '😛': 'fun','🙍': 'disapproval', \n",
    "              '💟': 'love', '🙀': 'tired', '🤒': 'ill', '😙': 'love', '💝': 'love', '👿': 'angry', '👪': 'family', '🤰': 'pregnant woman',  \n",
    "              '😸': 'happy', '🤐': 'speechless', '🤫': 'quiet','🥱': 'tired', '💢': 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75693e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02148bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read dictionary from json file\n",
    "with open(\"../../Supplementary data/emoji_dictionary.json\",\"r\") as f:\n",
    "    emoji_dict = json.load(f)\n",
    "len(emoji_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "112923a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walmart testing curbside grocery pickup in huntsville hyperlink\n",
      "proudly introducing \"shopping for survivors\" - a free grocery shopping and curbside pick-up service for cancer patients. thanks !\n"
     ]
    }
   ],
   "source": [
    "print(data['tweet'][8])\n",
    "print(data['tweet'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cde52863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 368413/368413 [02:25<00:00, 2528.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walmart testing curbside grocery pickup in huntsville hyperlink\n",
      "proudly introducing \"shopping for survivors\" - a free grocery shopping and curbside pick-up service for cancer patients. thanks !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove emojis\n",
    "tqdm.pandas()\n",
    "def give_emoji_free_text(text, emoji_dict):\n",
    "    emo = emoji.UNICODE_EMOJI\n",
    "    emojis = {**emo['en'],**emo['es'],**emo['pt'],**emo['it'],**emo['fr'],**emo['de']}\n",
    "    all_lst = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        lst = []\n",
    "        allchars = [i for i in word]\n",
    "        for j in allchars:\n",
    "            if j in emojis:\n",
    "                if j in emoji_dict:\n",
    "                    lst.append(' ')\n",
    "                    lst.append(emoji_dict[j])\n",
    "            else:\n",
    "                lst.append(j)\n",
    "        all_lst.append(''.join(lst))\n",
    "    return ' '.join(all_lst)\n",
    "\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x: give_emoji_free_text(x,emoji_dict))\n",
    "print(data['tweet'][8])\n",
    "print(data['tweet'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1715b951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194525 167083 87128\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f9360",
   "metadata": {},
   "source": [
    "### Remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d56329fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped at the grocery store for curbside pickup where they put your orderer in your trunk with no contact. while she was there she saw tons of boomers going into the store with no protection at all.\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "data['tweet'] = data['tweet'].apply(lambda x: contractions.fix(x))\n",
    "print(data['tweet'][5003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f251d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194525 213515 87128\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d80c2c",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2df598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9705812\n",
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact. she she saw tons boomers going store protection all.\n",
      "proudly introducing \"shopping survivors\" - free grocery shopping curbside pick-up service cancer patients. thanks !\n",
      "6708406\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "words = ' '.join(data['tweet']).split()\n",
    "print(len(words))\n",
    "from nltk.corpus import stopwords\n",
    "stop = ['what','which','this','that',\"that'll\",'these','those','is','are','was','were','be','been','being','have',\n",
    "              'has','had','having','do','does','did','doing','a','an','the', 'and','but','if','or','because','as','until',\n",
    "              'while','of','at','by','for','with','about','against','between','into','through','during','before','after',\n",
    "              'above','below','to','from','up','down','in','out','on','off','over','under','again','further','then',\n",
    "              'once','here','there','when','where','why','how','all','any','both','each','few','more','most','other',\n",
    "              'some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just',\n",
    "              'don',\"don't\",'should',\"should've\",'now','d','ll','m','o','re','ve','y','ain','aren',\"aren't\",'couldn',\n",
    "              \"couldn't\", 'didn',\"didn't\",'doesn',\"doesn't\",'hadn',\"hadn't\",'hasn',\"hasn't\",'haven',\"haven't\",'isn',\"isn't\",\n",
    "              'ma','mightn',\"mightn't\",'mustn',\"mustn't\",'needn',\"needn't\",'shan',\"shan't\",'shouldn',\"shouldn't\",'wasn',\n",
    "              \"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn',\"wouldn't\"]\n",
    "data['tweet'] =  data['tweet'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][16])\n",
    "words = ' '.join(data['tweet']).split()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "752689f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194525 213515 87128\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1ec2b",
   "metadata": {},
   "source": [
    "### Remove next line characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a58b1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact. she she saw tons boomers going store protection all.\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now. less exposure. they close. also workers watch hands curbside delivery.\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('\\n', ' ', x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4245902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194525 213515 87128\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e62d3",
   "metadata": {},
   "source": [
    "### Remove accented characters to their ASCII values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1dc7456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact. she she saw tons boomers going store protection all.\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now. less exposure. they close. also workers watch hands curbside delivery.\n"
     ]
    }
   ],
   "source": [
    "def accented_to_ascii(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "data['tweet'] = data['tweet'].apply(lambda x: accented_to_ascii(x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f387526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194578 213532 87129\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fc53e",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "322500b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact  she she saw tons boomers going store protection all \n",
      "proudly introducing  shopping survivors    free grocery shopping curbside pick up service cancer patients  thanks  \n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub('[!\"#%“[•►–&”\\'()*+,-/:;<=>?@\\\\^_{|}~`]', ' ', x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83a97e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195617 218051 87708\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e50854",
   "metadata": {},
   "source": [
    "### Remove spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9bfd757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact she she saw tons boomers going store protection all \n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also workers watch hands curbside delivery \n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub(' +', ' ', x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3bddacdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195617 218051 87708\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824f15f",
   "metadata": {},
   "source": [
    "### Replacing currency symbol by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84d055ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ucwradio walmart expands its curbside grocery pickup service you s hyperlink technology\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda x : re.sub(r\"\\$(\\d+)\", r\"\\1 dollars\", x))\n",
    "print(data['tweet'][780])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "077c1054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195617 218051 87708\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54131b11",
   "metadata": {},
   "source": [
    "### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee1e65e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact she she saw tons boomers going store protection all\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also workers watch hands curbside delivery\n"
     ]
    }
   ],
   "source": [
    "import num2words\n",
    "def remove_num(txt):\n",
    "    return ' '.join(' ' if i.isdigit() else i for i in txt.split())\n",
    "\n",
    "data['tweet'] = data['tweet'].apply(lambda x : remove_num(x))  \n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac42914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195617 218051 87708\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a7b5ee",
   "metadata": {},
   "source": [
    "### Find words with multiple same consecutive letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6826baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 368413/368413 [00:17<00:00, 21271.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809575 3794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "two_consecutive_letters = []\n",
    "three_consecutive_letters = []\n",
    "count_2 = 0\n",
    "count_3 = 0\n",
    "for i in tqdm(range(len(data))):\n",
    "    words = data['tweet'][i].split(' ')\n",
    "    for word in words:\n",
    "        if re.search(r'((\\w)\\2{2,})', word):\n",
    "            count_3 += 1\n",
    "            if word not in three_consecutive_letters:\n",
    "                three_consecutive_letters.append(word)\n",
    "        elif re.search(r'((\\w)\\2{1,})', word):\n",
    "            count_2 += 1\n",
    "            if word not in two_consecutive_letters:\n",
    "                two_consecutive_letters.append(word)\n",
    "print(count_2, count_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16511a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = to_remove = ['bbb', 'litttt', 'daaaaa', 'holllla', 'ggg', 'cccsc', 'oooo', 'ppp', '1000xs', 'ooooo', 'ffffff', 'haaa','brrr', 'econ1110', 'a444', 'mmmpanadas', 'toowaahhhh', 'mmmmmmmm', 'mmmagtweets', 'zzzzzz', 'mmmmm', 'ifttt', 'from1000km', 'bellletstalk', 'xxx', 'mmm', 'zzzz', 'vrai777', 'tracycopygrrrl', \n",
    "             'zzzzzzzzzzzzzzz', 'ooo', 'jefffrey', 'whole30fff', 'yyccc', 'mmmm', 'mmmmmm', 'hrmmm', 'fffuuuu', '1000mg', 'aaa', 'www', '7771x29', 'ffffffffffff', '2000s', 'taaac', 'brandongaillle', 'niggaaaaa','$qqq', 'heee', 'ummmmm', 'paullly', 'theyllgetcontactlessintheyear3000', 'hbuuu', 'sumkt444', 'nascarvvvrroooommmmmmm', 'kkkempsey', 'ttttttt', 'tttt', 'oooooo', 'lorderdddd', 'teamfccc', \n",
    "             'wwwwwwwwwww', '1000s', 'lll', 'ieeeorg', 'reeee', 'ltdjnrtnnnbggngvcfi', 'wwww', 'hhh', 'onnnnnnn','hheee', 'mmmmmmm', 'xxxx', '1000000000x', 'eee', 'ffbbbllaarrrrrrnnttts', 'wlll', 'pcccommunitymarkets',\n",
    "             'paccc', 'supportpaccc', 'amoooo', 'cj333', 'balllin', '000mph', 'reeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee','squeee', '000s', 'hhhh', 'rdockmatthews111', '5000sq', '1000x', 'aaaaaaa', 'booo19', 'ggggggggg','mysicklecelllife', 'faaaaa', 'maccleennnny', 'hmmmmm', 'aaaaaaaaaaaa', '000ppl', 'hmmmyess', 'aaaaaa','bestnyaaaa', 'aaaa', '000sq', '2000k', 'ccc', 'eeee', 'ooota', 'neee', 'waangooonline', 'hal2000bot',\n",
    "             'vcccares', '$glittergrl2000', 'nuuu', 'jaaahaaack', 'breeellery', 'aaaaaaaaaaa', 'bbbslc', 'hhhhhhhhh','3000th', 'sss', 'pleading$sbreezyyy', 'siaaa', 'aaaaaaaa', '15902201009000473m', '15904058179022263m', 'zzzzzzz', 'zzzzzzzzzzzzzzzzz', '000th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cfd3468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 368413/368413 [00:08<00:00, 45062.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = 0\n",
    "for i in tqdm(range(len(data))):\n",
    "    count = 0\n",
    "    words = data['tweet'][i].split(' ')\n",
    "    for word in words:\n",
    "        if word in words_to_remove:\n",
    "            count += 1\n",
    "    total_count += count\n",
    "total_count            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71a19f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 368413/368413 [00:07<00:00, 49712.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact she she saw tons boomers going store protection all\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also workers watch hands curbside delivery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_words(text, words_to_remove):\n",
    "    words = text.split(' ')\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in words_to_remove:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x : remove_words(x, words_to_remove))  \n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a538d550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1274"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_list = {'hummmm': 'hum','alll':'all','allll':'all','alllll':'all','buttt':'but', 'butttt':'but','shoppinggg':'shopping','lifeee': 'life', 'ehhhhhhhh': 'eh','hourrr': 'hour', 'weeeeeeeeeeeeeeeeeeeeeeeeee': 'we', 'buuuuuuuuut': 'but','ohhh': 'oh', 'sooo': 'so', 'superbowlllll': 'superbowl', 'damnnnn': 'damn', 'verrrry': 'very', 'oooh': 'oh', 'aahhhh': 'ah', 'fuckkkk': 'fuck','yooo': 'yo', 'effffffffffff': 'eff', \n",
    "                'okurrrrrrrr': 'ok', 'bizzzznesss': 'business', 'waaaaayyy': 'way', 'soooo': 'so','fffffffuuuuuuck': 'fuck', 'hiiii': 'hi', 'ssoooo': 'so', 'lmaooo': 'lmao', 'mannnnn': 'man', 'businesssupportingbusiness': 'business supporting business', 'eeeek': 'ek', 'alll': 'all', 'pssst': 'past', 'cccp': 'cp', 'annnnd': 'and','ummm': 'umm','hmmm': 'hmm', 'wonderfullly': 'wonderfully', 'smallbusinessstrong': 'small business strong','mayyyyyybe': 'may be', 'waaaaay': 'way', 'psssst': 'past','mooood': 'mood', 'usuallly': 'usually','ahhhh': 'aah', 'yasss': 'yes', 'lossses': 'losses', 'nevaaaa': 'never', 'stilllovewegmans': 'still love wegmans', 'mannn': 'man', 'awwww': 'aww', 'fukkkk': 'fuck', 'exciiited': 'excited', 'ahhh': 'aah', \n",
    "                'allll': 'all', 'outdooors': 'outdoors', 'uuugh': 'ugh', 'aaah': 'ahh', 'blesssss': 'bless', 'maaaybe': 'may be', 'dayssss':'days', 'bruhhh': 'bruh', 'itttttttt': 'it', 'waaaaaaaaaaaay': 'way', 'raaaaah': 'rah','eatwelllivewellku': 'eat well live well','yyyyes': 'yes', 'goooooooo': 'go', 'alllllllllll': 'all', 'aaand': 'and','ahhhhh': 'aah', 'hmmmm': 'hmm', 'fuuuuccckkkkkk': 'fuck', 'thoughtttt': 'thought', 'buttt': 'but', 'storessss': 'stores', 'goooo': 'go', 'ohhhh': 'oh', 'thennnn': 'then', 'whyyy': 'why', 'dumbbb': 'dumb', 'fuuuuuck': 'fuck', 'alllll': 'all','wooeee': 'woe', 'boooooo': 'boo', 'nahhhhh': 'nah', 'monthssss': 'months', 'shoooing': 'shooing', 'massshootings': 'mass shootings', 'amerikkka': 'amerika', 'hiiighly': 'highly', 'awww': 'aww', 'eeesh': 'esh', 'jooookeee': 'joke', 'realllllyyy': 'really',\n",
    "                'okokokokokkkk': 'ok','aaah':'aah','aaahh':'aah', 'ahhh':'aah','ahhhh':'aah','ahhhhh':'aah','byeee': 'bye', 'yeahhh': 'yeah', 'neeeda': 'need', 'shiiiiiiz': 'shit', 'buuuut': 'but', 'hateeeee': 'hate', 'otttt': 'ot', 'hnnng': 'hang', 'aaaaugh': 'ugh', 'partayyyyyy': 'party', 'heatedddd': 'heated', 'aayyy': 'aay', 'buuutttt': 'but', 'trashhhh': 'trash', 'waaaay': 'way', 'whaaat': 'what', 'uhhh': 'ugh','sweetoooo': 'sweet', 'fuuuuq': 'fuck', 'honeyyy': 'honey', 'aaaand': 'and', 'teeeellll': 'tell', 'coolinnnnnn': 'colin', 'businessnewzzz': 'business news', 'grrr': 'grr','hellllllo': 'hello', 'pleaseee': 'please', 'whaaaaa': 'wha', 'whhhyyyyy': 'why', 'ewwwwww': 'eww','grrrr': 'grr', 'lolll': 'lol', 'bihhhhhh': 'bih', 'yesssss': 'yes', 'arghhhh': 'argh', 'lovellloves': 'love','pfffft': 'pft', 'lessshoppingmorewriting': 'less shopping more writing', 'workswonderfullly': 'works wonderfully', 'grocerybusinessservice': 'grocery business service', 'litttttt': 'lit', 'juuuuust': 'just', 'couchhhhhh': 'couch', 'insteaddddd': 'instead', 'innnnteresting': 'interesting', 'waaaaah': 'wah', 'fuuuuuuck': 'fuck', 'gawwwwd': 'gawd', 'sooooo': 'so', 'daaang': 'dang', 'hellllllllllp': 'help', 'sawww': 'saw', 'booo': 'boo', 'yaaaasssss': 'yes', 'grrrrrr': 'grr', 'livveee': 'live', 'starvingggg': 'starving','worthlessshopper': 'worthless hopper', 'pleaseeee': 'please', 'terrrrible': 'terrible', 'wrrrrr': 'wrr', 'neeeeeeed': 'need','gurrrrrr': 'grr','outtttt': 'out', 'nowzzz': 'now', 'maaaan': 'man', 'hungryyyyy': 'hungry', 'reeeeeeally': 'really', 'wheee': 'whee', 'groooooans': 'groan', 'errmaaaagaaaahdddd': 'omg', 'booooooooooooo': 'boo', 'haaaaaaaaaaaaaaaaaaaaaaaaaaate': 'hate','frannnd': 'frand', 'hmmmmmm': 'hm', 'arrreeeeee': 'are', 'laaawd': 'loud', \n",
    "                'foodddddddddd': 'food','coffeee':'coffee', 'lesssocietymoreworders': 'less society more worders', 'unluckyyyyy': 'unlucky', 'naaahr': 'nah', 'sweeeeet': 'sweet', 'ahhhhhh': 'ahh', 'eeep': 'eep', 'baaaare': 'bare', 'geeky0001': 'geky01', 'loooooook': 'lok', 'dribbble': 'drible', 'm9mk8888m9':'m9mk8m9', 'yassss': 'yas', 'jeeez': 'jeez', 'miiighty': 'mighty', 'ummmm': 'umm', 'moooostly': 'mostly', 'whaaaaaa': 'wha', 'ooooppssss': 'oops', 'daaaa': 'daa', 'woooffffff': 'woof', 'wannna': 'wana', 'selllig': 'selig', 'convenienttentttttt': 'convenientent', 'speeedy': 'speedy', 'whaaaaat': 'what', 'yaaaas': 'yes', 'hnnnnnng': 'hang','beeeeeeeeeach': 'beach', 'strollling': 'strolling', 'dddribbble': 'drible','wiiggggg': 'wig',  'reeeeeal': 'real', 'dolll': 'doll', 'uppp': 'up', 'reallllllllly': 'really', 'loveee': 'love', 'girrrrl': 'girl', 'cmoooooon': 'come on', 'neeeeed': 'need', 'fuuuuuuuuuuuuuuuuuuuuuuuckkkk': 'fuck','businessscript': 'business script', 'ehhh': 'eh', 'cluuutch': 'clutch', 'kiiinda': 'kinda','reeeaaaaally': 'really', 'eeeugh': 'enough', 'welcummm': 'welcome', 'embarsssed': 'embarrassed', 'buuuuutttt': 'but','looovvve': 'love', 'weee': 'we', 'ooohhhh': 'oh', 'unveilingthebusinessstories': 'unveilingthebusinestories',  'guuuuhyperlink': 'hyperlink','riiising': 'rising', 'biittttccchhheeeessss': 'bitch', 'whewwwwwwwwww': 'whew', 'whaaatttt': 'what', 'stilll': 'still', 'huuuuge': 'huge', 'suuuper': 'super', 'alllllllll': 'all', 'mannnn': 'man', 'pffft': 'pft', 'reallly': 'really', 'whuuuuuuh': 'whuh', 'suckssss': 'sucks', 'awonderfullly': 'wonderfully', 'staaaaaarve': 'starve', 'cuuute': 'cute', 'ayyyyyyy': 'ay', 'yaaassssszzzzz': 'yes', 'goooees': 'goes','toooooo': 'to', 'bleeeeh': 'bleh', 'harrrrrd': 'hard', 'maaaaad': 'mad', 'lazyyyyyy': 'lazy', 'uuuuummm': 'umm','bweakfaaaaaaaast': 'bweakfast', 'againnnnnn': 'again', 'hrmmmm': 'hrm', 'hooopefully': 'hopefuly', 'psssttt': 'pst','swearrrrr': 'swear', 'weeeeeek': 'week', 'brahhhh': 'brah', 'checkkkk': 'check','everrrrr': 'ever', 'annnnnnnd': 'and', 'suuuucks': 'sucks', 'atttorney': 'attorney', 'peanutbuttter': 'peanut butter', 'ayyy': 'ay',  'wtffff': 'wtf', 'tttthhhhhiiissssss': 'this', 'hateee': 'hate', 'savedddd': 'saved','squeeeee': 'squee',  'oooouuuu': 'ou', 'naaaaa': 'na', 'weeee': 'we', 'yessss': 'yes', 'yeaaaa': 'yes', 'homeacccess': 'home access', 'timberrr':'timber', 'tahdayyy': 'today', 'yeees': 'yes', 'stayinnn': 'staying', 'aliveee': 'alive', 'heeeeeere': 'here', 'gr000oceries': 'groceries', 'taaaagetherrr': 'together', 'appp': 'app', 'sighhh': 'sigh', 'wooohoo': 'woohoo', 'winterrrr': 'winter', 'friiiendssss': 'friends', 'ooooof': 'of', 'yuuup': 'yes', 'everythinggg': 'everything', 'thankkk': 'thank', 'youuu': 'you', 'aaaannndddd': 'and', 'babaaaay': 'baby', 'ayeee': 'aye', 'babiesss': 'babies', 'yaaas': 'yes', 'coinnn': 'coin', 'nevvvaaaa': 'never','aaaack': 'ack', 'mmmlove': 'love', 'dayyyy': 'day', 'cribmusss': 'cribmus', 'employeeengagement': 'employee engagement', 'diseaseeee': 'disease', 'beyonddd': 'beyond', 'jeeeeesus': 'jesus', 'coffeeeeee': 'coffee', 'wwasss': 'was', 'alriiight': 'alright', 'tieeeeeeed': 'tied','hellooooo': 'hello', 'reeeaaalllll': 'real', 'whoooo': 'who', 'hulll': 'hull', 'whhyyyyy': 'why', 'oooooold': 'old','orrrr': 'or', 'weeeek': 'week', 'dayyyyy': 'day', 'ohhhyeaaahhh': 'oh yeah', 'helloooo': 'hello', 'yallll': 'you all','agghhhhhhhhhhhhhhhhhhhhhhhhhhh': 'argh', 'agghhhhhhhhhhhhhhhhhhhhhhhhhhhhh': 'argh', 'finallly': 'finally','adressses': 'addresses', 'reallllly': 'really', 'omggg': 'omg', 'yaaaaa': 'ya', 'laggg': 'lag', 'helllooooee': 'hello', 'hellllooooee': 'hello', 'hungryyy': 'hungry', 'apppayments': 'apayments', 'littt': 'lit', 'suuuuuuuuper': 'super', 'feeees': 'fees', 'bihhhh': 'bih', 'gawddd': 'gawd', 'allllll': 'all', 'deeeply': 'deeply', 'fullfilllment': 'fulfilment', 'pfff': 'pf', 'weirdddd': 'weird', 'nowwwwwww': 'now', 'heyyyy': 'hey','luuuuucky': 'lucky', 'haaand': 'hand', 'nowwwww': 'now',  'heeelp': 'help', 'shppper': 'shopper', 'mooost': 'most', 'fooods': 'foods', 'girlllll': 'girl','whatttt': 'what', 'factttt': 'fact', 'yaaaassssss': 'yes', 'worrry': 'worry', 'buuutt': 'but', 'andddd': 'and','groooound': 'ground', 'commmunication': 'communication', 'biggg': 'big', 'geeeezus': 'jeesus',              'woooo': 'woo', 'paaaranoid': 'paranoid', 'wuaaaaaaaaaaaa': 'wua', 'yerrrrrr': 'yer', 'craaaap': 'crap','weeeeeed': 'weed', 'daaaays': 'days', 'runnning': 'running', 'eeech': 'ech', 'everrr': 'ever', 'quinnnnnnn': 'quin','yummm': 'yum', 'goood': 'god',  'okaaaaay': 'okay', 'weeeeeeeee': 'we', 'chilll': 'chill', 'yayyy': 'yay', 'realllly': 'really', 'oooof': 'of', 'wayyyyy': 'way', 'booooo': 'boo', 'yyyyeesssssssss': 'yes', 'stafff': 'staff', 'ooooooookay': 'okay', 'suppposed': 'supposed','thanksssss': 'thanks', 'boyyyyy': 'boy', 'ugguhughughghhhh': 'ugh', 'reaalllyy': 'really', 'tyyy': 'ty','yesss': 'yes', 'weeeird': 'weird', 'wheeeeeeee': 'whee', 'aieee': 'aie', 'pissssssed': 'pissed', 'anddddd': 'and','errrr': 'err', 'freeeeeee': 'free', 'dayzzzz': 'days', 'killerrrr': 'killer', 'piiised': 'pissed', 'wooooop': 'woop', 'localbusinessservices': 'local business services', 'happyhmmm': 'happy','housssseeee': 'house', 'gaaaaah': 'gah', 'sbappp': 'sbap', 'scurrred': 'scurred', 'messsages': 'messages', 'whaaaat': 'what', 'soooon': 'soon', 'theee': 'the', 'villlage': 'village', 'woooow': 'wow','chiliiii': 'chili', 'uhhmmmm': 'uhm', 'itwonderfullly': 'it wonderfully','rulllessssss': 'rules', 'aaaannnnnd': 'and', 'ewwwhyperlink': 'hyperlink', 'sammeee': 'same', 'geeez': 'geez', 'wayyyyyyy': 'way', 'eatwelllivewell': 'eat well live well', 'tooo': 'too', 'nopeeee': 'nope', 'aswonderfullly': 'as wonderfully', 'aaaaaaaaallllll': 'all', 'resuppply': 'resupply', 'yeahhhhh': 'yeah', 'lesss': 'less', 'innnnn': 'in','shppping': 'shipping', 'yeaaaah': 'yeah', 'whyyyyy': 'why','ooops': 'oops', 'neoowww': 'neow', 'seeems': 'seems', 'cluuuuuuutch': 'clutch', 'hellllll': 'hell', 'yaaaaaa': 'ya', 'aaahh': 'aah', 'byyyyyeeeeee': 'bye', 'pleaseeeeeee': 'please', 'maaaaan': 'man','lollll': 'lol', 'caaaat': 'cat', 'foood': 'food', 'faaaaaave': 'favourite', 'knowwww': 'know', 'weeeks': 'weeks', 'yaaa': 'ya','reeeeaaaalllly': 'really', 'themmm': 'them', 'gooooooood': 'good','ssshhhh': 'shh', 'waaaawaaaaawaaaaa': 'wow', 'businessschools': 'business schools', 'locallly': 'localy', 'boooooy': 'boy', 'plzzzzz': 'plz', 'realllyyyy': 'really', 'lmdaoooo': 'lmdao', 'daaaaaam': 'damn', 'woohooo': 'woohoo', 'whaattt': 'what','truuuuu': 'true', 'whaaaaaaat': 'what', 'meeee': 'me', 'reeeeeallly': 'really', 'ffffuuuuccccckkkkkkkkkkk': 'fuck', 'winnnning': 'winning', 'pppl': 'ppl', 'siiiigh': 'sigh', 'thurrr': 'thur', 'hopefullly': 'hopefully', 'lettting': 'letting', 'reaaaaaaally': 'really', 'buisnesssupport': 'buisness support', 'bombbbb': 'bomb','wuuut': 'wut', 'fuuu': 'fuu', 'trafffic': 'traffic', 'butttt': 'but', 'coffeee': 'coffee', 'maaamm': 'mam', 'reeeaally': 'really', 'gooo': 'go', 'sammeeee': 'same', 'swoooooon': 'swon', 'todaaaaaaay': 'today', 'nccc': 'nc','uggg': 'ugh', 'hellllllp': 'help', 'grrrrrrr': 'grr', 'shoooot': 'shot', 'fffuzzy': 'fuzzy', 'gettting': 'getting','telll': 'tell', 'maaaaybe': 'may be', 'duuuuude': 'dude', 'dooooo': 'do', 'weyyy': 'wey', 'boycotttrumppressconferences': 'boycotrumpresconferences','nowww': 'now', 'hooraaaay': 'hooray',  'woohoooo': 'woohoo', 'hhhmmm': 'hmm', 'yummmmmm': 'yum', 'wellllllll': 'well', 'woahhh': 'whoa', 'buuuuusssssinesssss': 'business','muuuumm': 'mum', 'girlll': 'girl', 'uuuuugh': 'ugh', 'thiiis': 'this', 'wayyyy': 'way', 'ateeee': 'ate', 'ohhhhhh': 'oh', 'ahhhhhhhhhhh': 'aah','saaaaammmmeee': 'same', 'saaammmmeee': 'same', 'infinittyyyyyy': 'infinity','welppp': 'welp', 'hellooooooo': 'hello', 'tryinggg': 'trying', 'roaaaaarsome': 'roarsome', 'cooo': 'co', 'oookies': 'okay','ittttttt': 'it', 'willl': 'will', 'neededdddddd': 'needed', 'haaay': 'hay', 'daysss': 'days', 'hahahhhahah': 'haha','oooph': 'oph', 'youlllive': 'you will live', 'defffff': 'def', 'bitttttt': 'bit', 'ideaaaa': 'idea', 'waitttt': 'wait','dangggggggggg': 'dang', 'lmaooimfatboyandwillstarveooooooooooooooooooo': 'lmao i am fat boy and wil starve', 'aaaannnnnndddddd': 'and', 'cevicheee': 'ceviche', 'tostadassss': 'tostadas', 'boooooooox': 'box', 'yaaaay': 'yay', 'guyssss': 'guys', 'aaaassssss': 'as', 'allllllaaaat': 'alat', 'waaaaayyyyy': 'way', 'coooooookies': 'cookies', 'folksss': 'folks', 'meee': 'me', 'yuuuup': 'yup', 'daaark': 'dark', 'sorryyy': 'sorry', 'concepttttt': 'concept', 'booosted': 'boosted','threeeee': 'three', 'grrrrr': 'grr','starrrrrrrrrrving': 'starving',  'baybayyyy': 'baby', 'aaaaafffffff': 'aaf', 'maxxxx': 'max', 'waaaaaaaah': 'wah', 'arghhh': 'argh',  'flosssssss': 'floss',  'wayyy': 'way', 'burgesss': 'burges', 'wahhh': 'wah', 'maaam': 'mam', 'brrrr': 'brr', 'yippeeeee': 'yippee', 'loool': 'lol', 'ooohh': 'oh', 'loooooool': 'lol', 'donneeee': 'done', 'ewwww': 'eww', 'everrrrrr': 'ever', 'looot': 'lot', 'deliverrrry': 'delivery', 'isss': 'is', 'orrrrrr': 'or', 'duuuude': 'dude', 'ftwwwwww': 'ftw', 'hallllp': 'help', 'fuuuuck': 'fuck', 'yaaaaas': 'yes','businessscience': 'business science','myyy': 'my', 'successstory': 'sucess story', 'fffffffuck': 'fuck', 'thisssssss': 'this', 'ahhhhhhh': 'aah', 'yaaaaasss': 'yes', 'tthhhe': 'the', 'donttouchmeee': 'dont touch me',  'jeeeeeze': 'jeeze', 'suuuuch': 'such', 'twattttt': 'twat', 'tomorrrow': 'tomorrow', 'aaalllll': 'all','gooooaaaaaalll': 'goal', 'heeeeeeere': 'here', 'weeeeeee': 'we', 'gooooo': 'go', 'firrre': 'fire', 'chainssss': 'chains', 'whhhaaatttt': 'what', 'yaaasss': 'yes', 'notttt': 'not', 'haaaaaaaate': 'hate','hungeeeee': 'huge', 'waaaaa': 'wah', 'wheeeeee': 'whee', 'milllennials': 'millennial', 'aghhh': 'agh', 'errrrr': 'err', 'bestttt': 'best', 'buuuuuut': 'but', 'hmmmph': 'hmph', 'maaaaaaaan': 'man', 'buuuuut': 'but', 'guuuuurrrllll': 'girl', 'heyyy': 'hey', 'ummmmmmm': 'umm', \n",
    "                'yassssss': 'yes', 'fuckkk': 'fuck', 'arhhh': 'arh', 'hooo': 'ho', 'itttttt': 'it', 'boooo': 'boo', 'zzzzz': 'z', \n",
    "                'nahhhh': 'nah','guyssss':'guys','damnnnn':'damn', 'grrr':'grr','grrrr':'grr','grrrrrrrr': 'grr', 'mmmaybe': 'may be', 'deeeeets': 'details', 'waaaaaay': 'way', 'knowwwwww': 'know', 'hollaaaaaaa': 'hola', 'exxxtra': 'extra', 'neeeed': 'need', 'yaaaassss': 'yas', 'seamlessshopping': 'seamleshoping','plsss': 'please', 'hnnnnngggggg': 'hang', 'retweeet': 'retweet', 'retweeetplease': 'retweet please', 'tyyyyy': 'ty','yaaaaassss': 'yes', 'aahhh': 'aah','yupppp': 'yup', 'damnnnnnnn': 'damn','successstories': 'success stories', 'nnnreal': 'unreal', 'reggggular': 'regular', 'screeeam': 'scream', 'stahhpppp': 'stop', 'girrrrllllllllll': 'girl', 'mooooooooooon': 'moon', 'ouuuuuuuchhhh': 'ouch', 'yoooo': 'yo', 'lessstress': 'less stress', 'positivevibezzz': 'positive vibes', 'oookkkkayyy': 'okay', 'breeezyy': 'breezy', 'wisssshh': 'wish', 'foooood': 'food', 'oooooooookay': 'okay','thaaanks': 'thanks', 'loook': 'look', 'paaaarrrrrty': 'party','hollaaaa': 'hola', 'suuuuuper': 'super', 'hateeee': 'hate', 'gaaaaahhhhhh': 'gah','callling': 'calling','honestbeeeeee': 'honestbee', 'hoooonest': 'honest', 'neeeeeeeeeever': 'never', 'ricccch': 'rich', 'juuuuuuust': 'just', 'whaaattt': 'what', 'whyyyyyy': 'why','happyxxx': 'happy','ayyye': 'aye', 'hardddd': 'hard', 'pleeeeeeease': 'please','suuuuucks': 'sucks', 'shhhhh': 'shh', 'naaaaaaaaah': 'nah', 'heeey': 'hey', 'yooooo': 'yo','wooooooooorse': 'worse', 'ittttt': 'it', 'buuut': 'but', 'succkssss': 'sucks', 'slowwwwlyyyy': 'slowly','yaaassss': 'yes', 'annnnnd': 'and', 'herrreeee': 'here', 'yeaaaaaah': 'yeah', 'murderrrr': 'murder','yeaaaaaaaaaah': 'yeah', 'aaaany': 'any', 'uppppp': 'up', 'fuuun': 'fun', 'tadaaa': 'tada', \n",
    "                'hufffff': 'huff', 'juuust': 'just','finnne': 'fine','errr': 'err', 'odddeliverydrivercomplaints': 'odd delivery driver complaints', 'eeek': 'eek', 'girrrrrl': 'girl', 'hmmmmmmm': 'hmm', 'onlyyyyyyy': 'only', 'guysssssss': 'guys','lotttttt': 'lot', 'ohhhhh': 'oh', 'ruuuns': 'runs', 'weeeekend': 'weekend', 'shoppping': 'shopping', 'verrry': 'very', 'fooooooood': 'food', 'tooooooo': 'too', 'reeeeally': 'really','zoooo': 'zoo', 'uhhhh': 'ugh', 'sorrryyyy': 'sorry', 'ssshhh': 'shh', 'embarrassmentummm': 'embarassment', 'plssssssss': 'please', 'todayyyy': 'today', 'neverrrr': 'never', 'craaazy': 'crazy', 'annnd': 'and', 'boooomb': 'bomb','aliiive': 'alive', 'ecologicallly': 'ecologicaly', 'nooo': 'no', 'waaaait': 'wait', 'tooooo': 'too', 'ohhhhhhh': 'oh', 'whyyyy': 'why', 'pleeaaaassseeee': 'please', 'fuuuudge': 'fudge','shiddddd': 'shid', 'biiig': 'big', 'yooooooo': 'yo', 'coldddddddddd': 'cold', 'mighttttt': 'might','freeeee': 'free', 'pleaseeeeee': 'please', 'helll': 'hell','mmkkayyy': 'i am okay', 'reeeaaalll': 'real','bxtchhhhhhh': 'bitch', 'eeeeek': 'eek', 'ooooowweeeeee': 'owe','toooooooo': 'too', 'retweee': 'retweet', 'daaayyyysss': 'days', 'hmmmmmmmmmmm': 'hmm', 'heeeeere': 'here', 'fineeee': 'fine', 'naurrr': 'naur', 'liviiddddd': 'livid', 'looool': 'lol', 'stokedddd': 'stoked', 'pleeeease': 'please', 'arghhhhhh': 'argh', 'especiallly': 'especialy', 'supppper': 'super', 'ummmmmm': 'umm', 'seeee': 'see', 'wtfff': 'wtf',  'slowwwwwservice': 'slow service', 'suuuuuuper': 'super', 'whhhaaaaaaaa': 'whaa', 'soooers': 'soers','eeeeeeeeeekkkkkkkk': 'eek', 'don888': 'don8', 'extreeeemly': 'extremely','saaame': 'same', 'booooy': 'boy', 'ooof': 'of', 'bitchhh': 'bitch', 'babbieeeee': 'baby', 'refuseeee': 'refuse', 'jooooooooooke': 'joke', 'juskooo': 'jusko', 'vaxxxed': 'vaxed', 'loooow': 'low', 'slowwwww': 'slow', 'waaaayyyyy': 'way','businesssuccess': 'business success', 'naaaaaaa': 'na', 'goooooooooooooo': 'go', 'fuuuuuuuck': 'fuck', 'shoppinggg': 'shopping', 'bihhh': 'bih', 'pshhh': 'psh', 'saaaaaame': 'same', 'faaaaaaccckkk': 'fuck', 'employeeengagment': 'employee engagement', 'ooop': 'oops', 'sighhhhhhhhhhhhhhhhh': 'sigh', 'howwwww': 'how', 'ooooold': 'old', 'ghhhhggh': 'ghgh', 'shitttt': 'shit', 'tfff': 'tf', 'saaaaame': 'same', 'whaaaaatttt': 'what', 'yeeeeaah': 'yeah', 'seeet': 'set', 'yeee': 'ye', 'uuuugggg': 'ugh',  'pleaseeeee': 'please', 'tipzzzzz': 'tips', 'aarrrrrrrrghhhhhhh': 'argh', 'foreseennnnn': 'foreseen', 'yooouuu': 'you', 'ayyyyyy': 'ay', 'sucksss': 'sucks', 'screeeeeams': 'screams',  'mamoy': 'mamoy',  'whoooooof': 'whoof', 'chillle': 'chile', 'buttttt': 'but', 'actuallly': 'actually', 'wooot': 'woot', 'businessstandard': 'business standard', 'idcccc': 'idc', 'fulllllll': 'full', 'seeeeesh': 'sesh', 'handddd': 'hand', 'smhhh': 'smh', 'sodaaaa': 'soda', 'smallbusinesssupport': 'small busines support','nervessssssss': 'nerves', 'snowww': 'snow', 'ssssssssssoooo': 'so', 'biiit': 'bit', 'wooooooooooo': 'woo', 'swwwwing': 'swing', 'shittty': 'shity', 'yyyy': 'y', 'maaaann': 'man', 'tippppsss': 'tips', 'neeeeeeeeeeeds': 'needs', 'shoppinggggg': 'shopping', 'arghhhhh': 'argh', 'listennn': 'listen', 'neeeever': 'never', 'eeeep': 'eep', 'tooold': 'told', 'bahhh': 'bah', 'sneeeeeeezers': 'sneezers', 'lovexxx': 'love', 'yeeepppp': 'yep', 'aaamazzzing': 'amazing', 'fuuuuuuuuuuuuck': 'fuck','omfggg': 'omfg', 'monthsssss': 'months', 'maaaaaaaaay': 'may', 'arrrrghhh': 'argh', 'dyinggggg': 'dying', 'muuuuuuuuch': 'much', 'wheeew': 'whew','springgggg': 'spring', 'cmonnnnnn': 'cmon', 'thaaat': 'that', 'jeeeez': 'jeez', 'nawww': 'naw', 'shhhh': 'shh', 'waaaaahhh': 'wah', 'thatttt': 'that', 'clutchhhhhhhh': 'clutch', 'squeeee': 'squee', 'maaaaaan': 'man', 'eatttt': 'eat', 'woweee': 'wowe', 'plzzzz': 'plz', 'babyyyyy': 'baby', 'crucialllll': 'crucial', 'beekeeeper': 'beekeeper', 'beeee': 'be', 'morningggg': 'morning', 'stopppppo': 'stop', 'itsalllocal': 'its a local', 'hmrsss': 'hmrs','successsee': 'success','burrrr': 'burr', 'wheeee': 'whee', 'anyoneeee': 'anyone', 'annndd': 'and', 'businesss': 'business', \n",
    "                'incrennnible': 'incredible','lmaooo':'lmao', 'helloooo':'hello','gaaaah': 'gah', 'blowssssssss': 'blows', 'veeeeeggggg': 'veg', 'thiissss': 'this', 'eccc': 'ecc', 'seee': 'see','nahhhhhh': 'nah', 'caaaaasually': 'casually', 'thissss': 'this', 'wellllll': 'well', 'sayinnggg': 'saying', 'dinnerrrr': 'dinner', 'ughhh': 'ugh', 'coffeeeee': 'coffee', 'apppreciate': 'appreciate', 'fooddddddd': 'food', 'waaaa': 'waa', 'blacklivesmattters': 'black lives matters', 'pleaseeeeeeeee': 'please', 'haaaa': 'haa','bagsss': 'bags', 'aiyaaa': 'aiya', 'sixxxxxx': 'six', 'weeeeekssssss': 'weeks', 'aaaaaaaaaaaaaaahhhhhh': 'aah','suuure': 'sure', 'youuuu': 'you', 'freestufffriday': 'free stuff friday', 'bammmmmm': 'bam', 'ewww': 'eew', 'sisss': 'sis', 'yeesss': 'yes', 'sweeeeeeeet': 'sweet', 'girllll': 'girl', 'suuuuck': 'suck', 'wooooooah': 'woah', 'sayweee': 'say we', 'yoooooo': 'yo', 'hellllla': 'hell', 'eeegads': 'egad', 'lmfaaooo': 'lmfao', 'hhhhhg': 'hg', 'fooooooodddd': 'food', 'fooooood': 'food', 'tttil': 'till', 'sayyyy': 'say', 'booooooo': 'boo','faaaar': 'far', 'peopleeeeeeee': 'people', 'auuugh': 'ugh', 'fuuuck': 'fuck', 'sirrr': 'sir', 'plzzz': 'plz', 'yummmm': 'yum', 'sleep': 'sleep', 'alsooo': 'also', 'reallllllly': 'really', 'nahhhhhhting': 'nothing', 'wayyyyyyyy': 'way', 'wheeeeeeeeee': 'whee', 'niiice': 'nice', 'shoooo': 'shoo', 'yeaaah': 'yeah', 'bwahhhahaah': 'haha', 'hahaaaa': 'haha', 'ordererrr': 'orderer', 'behhh': 'beh', 'plsssss': 'pls', 'neeed': 'ned', 'pffffftt': 'pft', 'choooooosing': 'chosing', 'byeeeeeeee': 'bye', 'welllll': 'well', 'naaaame': 'name', 'strugggllleeeee': 'struggle', 'yeahhhh': 'yeah', 'waayyy': 'way', 'aaaw': 'aww', 'yummmmmmmmmmmm': 'yum', 'nahhh': 'nah', 'silaaaaaa': 'sila', 'aaaaahhhhh': 'aah', 'bayyyy': 'bay', \n",
    "                'saaammee': 'same','alll':'all', 'grrr':'grr','yummm':'yummy','yeees':'yes','yasss':'yes','yassss':'yes','whyyy':'why','whaaat':'what','lolll':'lol','weee':'we', 'weeee':'we','tooo':'too','tyyy':'ty','uuugh':'ugh', 'sooo':'so','suuuuper': 'super', 'whyyyyyyy': 'why', 'reaaaaaally': 'really', 'fuccc': 'fuck', 'neverrr': 'never', 'ffff': 'f', 'maaan': 'man', 'asssist': 'assist', 'uuuuu': 'u', 'foreverrrrrer': 'forever',  'aaaaaah': 'aah', 'hellooooooooooo': 'hello', 'mmmmmmmrrrrrrrrrrrr': 'mr',  'mmmrrrrrrrrrrooooooooooorrrm': 'mrorm', 'feee': 'fee', 'freee': 'fre', 'stressss': 'stress', 'yeeeeee': 'yee', 'deeeeerrrrrp': 'derp', 'wooohoooo': 'woohoo', 'mehhhh': 'meh', 'bahhhh': 'bah', 'chillinnnn': 'chilling', 'whattt': 'what', 'lmaooooo': 'lmao', 'daaaaamn': 'damn', 'lolllll': 'lol','psssssst': 'pst', 'carrrrrr': 'car', 'paaaain': 'pain', 'reeeeeee': 're', 'madeeeee': 'made', 'oooow': 'oaw', 'wooooo': 'woo', 'ahhhhhhhhhgg': 'ahg',  'blehhhhh': 'bleh', 'teeeeeny': 'tiny', 'carbzzzz': 'carbz', 'efffing': 'effing', 'kakraaa': 'kakra', 'hoooooo': 'ho', 'jeeeeez': 'jeez', 'hahaaa': 'haha', 'suuuuuuuuure': 'sure', 'hummm': 'hum', 'myoooosive': 'myosive', 'prrrrobably': 'probably', 'waiiit': 'wait','fuuuuuuucked': 'fucked', 'thanksgiviiing': 'thanksgiving', 'waaaant': 'want', 'girrrlll': 'girl', 'gaaaame': 'game', 'whewww': 'whew', 'awwwwkward': 'awkward', 'godddddd': 'god', 'whoooooo': 'who', 'dopppeee': 'dope', 'looolll': 'lol', 'ammmmiwrongggggg': 'am i wrong', 'weeek': 'week', 'yeeeeahhh': 'yeah', 'knowww': 'know', 'nottt': 'not', '4evaaaa': '4eva', 'wiiild': 'wild', 'wooo': 'woo', 'yummmmmmmm': 'yum','aaaall': 'all', 'herreeeee': 'here', 'seasonnnnnn': 'season', 'ummmmmmmm': 'umm', 'goootcha': 'gotcha', 'oooops': 'oops', 'yeeeees': 'yes', 'sheeeeeeeee': 'she', 'mmmfph': 'mfph', 'everrrr': 'ever', 'stufff': 'stuf', 'fkkkk': 'fuck', 'eliteeeeeeee': 'elite', 'maybeeee': 'maybe', 'brrrrr': 'brr', 'yyeesss': 'yes', 'waaaayyy': 'way', 'reaallly': 'really', 'lmaaao': 'lmao', 'eyyy': 'eyy', 'givingggg': 'giving', 'toooo': 'too', 'hiii': 'hi', 'yeeeeeeeees': 'yes', 'arrrgh': 'argh', 'fuuuccccckkk': 'fuck', 'covidvacccine': 'covid vaccine', 'mmmay': 'may', 'wellll': 'well', 'yeeeeesss': 'yes', 'laaaaa': 'la', 'sameeee': 'same', 'goooood': 'good', 'afternoonnnn': 'afternoon', 'fffb': 'fb', 'aaaccedent': 'accident', 'clutcchhhhh': 'clutch', 'whennnnnn': 'when', 'reeeeeeeally': 'really', 'rigghhhttttt': 'right', 'ahhhhhhhh': 'aah','walkkk': 'walk', 'phewwww': 'phew', 'overrr': 'over', 'fuuuuuuuuck': 'fuck', 'aaaah': 'aah', 'classsss': 'class', 'maaaaaybe': 'may be', 'daaaamn': 'damn', 'businessses': 'busineses', 'hooooo': 'hoo', 'whooo': 'who', 'onlineshoppping': 'online shopping','hiswonderfullly': 'his wonderfully', 'paymisseee': 'paymise', 'hoooo': 'ho', 'orrr': 'or', 'waaaaaaah': 'wah', 'aaahhhhhh': 'aah', 'caaalllllll': 'call', 'reeeally': 'really', 'ooookay': 'okay', 'juuuuuuuuuuust': 'just', 'earlyyyy': 'early', 'fuuuccckkk': 'fuck', 'caaaant': 'cannot', 'maaaannnnnn': 'man', 'uuuuuuuu': 'u', 'boooring': 'boring', 'downnnn': 'down', 'cuteee': 'cute', 'gaaaarooooossssssss': 'garos',  'buttttttttt': 'but', 'racccisssttt': 'racist', 'baaaaaaby': 'baby', 'orrrrrrr': 'or', 'reallyyy': 'really', 'aaaanndd': 'and', 'oohhhh': 'oh','anddd': 'and', 'happychildlessspinster': 'happy childless spinster', 'goooooo': 'go', 'baaaack': 'back','succckkks': 'sucks','whaaaaaaaat': 'what', 'thruuuuuuuuuuu': 'thru', 'whooot': 'whot', 'haaaaaaaaate': 'hate', 'nervousssss': 'nervous', 'heeeeeeeelpppppppppp': 'help', 'mannnnnn': 'man', 'oooooooook': 'ok', 'yussss': 'yes', 'bigggggg': 'big', 'igrrr': 'igr', 'botttom': 'bottom', 'muuurrr': 'mur', 'suuucks': 'sucks','juuuust': 'just', 'awwwhyperlink': 'hyperlink', 'speeeedy': 'speedy', 'uuuf': 'uff', 'crazzzzyyyy': 'crazy', 'aaahhhh': 'aah', 'businesssolutions': 'business solutions', 'teeesst': 'test', 'yeeeah': 'yeah', 'gimmme': 'give me', 'thewonderfullly': 'the wonderfully', 'ffffffuck': 'fuck', 'baybeeeee': 'baby', 'backkk': 'back', 'sweetttttt': 'sweet', 'myhhhh': 'myh', 'babyyyy': 'baby', 'todayyyyyy': 'today', 'laaaa': 'la', 'fulllockdown': 'full lockdown', 'goddddd': 'god', 'maaaannn': 'man', 'uuuugh': 'ugh', 'tierrrrrrrrr': 'tier', 'sleeeeeping': 'sleeping', 'sunnnybeemarket': 'sunybee market', 'eeevvveeerrr': 'ever', 'haaarddddddd': 'hard', 'saaaame': 'same', 'pffffizer': 'pfizer', 'mmmmicrobes': 'microbes', 'byeeee': 'bye', 'founddd': 'found', 'serviceeeee': 'service', 'wantttt': 'want', 'buyyyy': 'buy', 'meaaaaaaat': 'meat', 'mmmh': 'mmh', 'wahhhhh': 'wah', 'heeeere': 'here', 'thankssss': 'thanks', 'hummmer': 'humer', 'yeeeeeeeeeaaaaaahhhhhh': 'yeah', 'unhappyblesssss': 'unhappy bless', 'dooor': 'door', 'welll': 'well', 'focuuussssss': 'focus',  'hisss': 'his', 'babyyy': 'baby', 'whateverrrr': 'whatever', 'weeeeee': 'we', 'bzzzzzzz': 'bz', 'naaaa': 'na', 'cosssssss': 'coss','lifeeee': 'life', 'businesssolution': 'business solution', 'pleaseeeeeeee': 'please','yuuum': 'yum', 'buttttttttttt': 'but', 'pissedddd': 'pissed', 'geeesh': 'geesh', 'helloooooo': 'hello', 'fineeeee': 'fine','alllllll': 'all', 'againnnn': 'again', 'muuuuch': 'much', 'ewwwww': 'eww', 'buuuunch': 'bunch', 'freeaaaakk': 'freak', 'aaauugh': 'ugh', 'aaaaahh': 'aah', 'aaaaaaaahhh': 'aah', 'waaaayyyyyy': 'way','hereee': 'here', 'shhh': 'shh', 'gooood': 'good', 'todayyy': 'today', 'outttt': 'out', 'nervesssss': 'nerves', 'yuuuuup': 'yup', 'sheeee': 'she','buildiiiiiing': 'building', 'bankkkkk': 'bank', 'usssa': 'usa', 'weeeee': 'we', 'nohhh': 'noh','bessst': 'best', 'ughnnn': 'ugh', 'worlddddddddddd': 'world', 'yeeeaaaaaahhh': 'yeah', 'whewwww': 'whew', 'suuuuuuucks': 'sucks', 'uhhhm': 'uhm', 'wuuuuu': 'wu', 'aaaaayyyy': 'ayy','10000x': '10x', 'perioddddd': 'period', 'alsoooo': 'also', 'thiccc': 'thick', 'lezzgoooo': 'let go', 'ighttttt': 'ight', 'giiirl': 'girl', 'd7000': 'd70', 'xxxl': 'xl', 'q17775306': 'q175306','hhhhhhhhhh': 'h', '$mmmb': '$mb', 'dxxx': 'dx','000$': '0$','000pm': '0pm', '1119th': '19th', '1000k': '10k', 'appph': 'aph', '$misssadiemaeve': '$misadiemaeve','x10000': 'x10', '0xf8da8ecee025444ad8dcf065ae09cffcd55c6116': '0xf8da8ece0254ad8dcf065ae09cfcd5c616'\n",
    "                   }\n",
    "len(replace_list)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3837ada9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1274"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replace_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74f60951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 368413/368413 [00:01<00:00, 225777.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3265"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = 0\n",
    "rep_words = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    count = 0\n",
    "    words = data['tweet'][i].split(' ')\n",
    "    for word in words:\n",
    "        if word in replace_list:\n",
    "            count += 1\n",
    "            rep_words.append(word)\n",
    "    total_count += count\n",
    "total_count   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90151178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|██████████████████████████████████████▋| 1264/1274 [02:54<00:01,  7.53it/s]/var/folders/2k/fhkt65ls68nbd5xt0lj3sbww0000gn/T/ipykernel_59839/2053584832.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data.loc[:,'tweet'] = data.tweet.str.replace(i,replace_list[i])\n",
      "100%|███████████████████████████████████████| 1274/1274 [02:55<00:00,  7.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(replace_list):\n",
    "    data.loc[:,'tweet'] = data.tweet.str.replace(i,replace_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dfec363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195618 218051 87709\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30c528",
   "metadata": {},
   "source": [
    "### Replace slangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d8341c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slangs = {}\n",
    "with open('../../Supplementary data/slangs.txt', 'r') as f:\n",
    "    for i in f:\n",
    "        for j in i.split(','):\n",
    "            j = j.replace('{','')\n",
    "            j = j.replace('}','')\n",
    "            j = j.replace('\\\"','')\n",
    "            a = j.split(':')[0].strip()\n",
    "            b = j.split(':')[1].strip()\n",
    "            slangs[a] = b\n",
    "len(slangs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa4df71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 368413/368413 [00:01<00:00, 220657.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15992"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "slang = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    words = data['tweet'][i].split(' ')\n",
    "    for word in words:\n",
    "        if word in slangs:\n",
    "            count += 1\n",
    "            slang.append(word)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffc678a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact she she saw tons boomers going store protection all\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also workers watch hands curbside delivery\n"
     ]
    }
   ],
   "source": [
    "def replace_slangs(text):\n",
    "    return ' '.join([slangs.get(i.lower(),i.lower()) for i in text.split()])\n",
    "data['tweet'] = data['tweet'].apply(lambda x : replace_slangs(x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ca19795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195618 218209 87739\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813d541",
   "metadata": {},
   "source": [
    "### Find words representing coronavirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64acbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_corona(text):\n",
    "    replace_list = {'coronavirus':'corona','covid19':'corona','covidiot':'corona idiot','covid':'corona'}\n",
    "    words = text.split(' ')\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        for i in replace_list:\n",
    "            if i in word:\n",
    "                word = word.replace(i, replace_list[i])\n",
    "        new_words.append(word) \n",
    "    return ' '.join(new_words)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44139c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 368413/368413 [00:01<00:00, 271052.16it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x : replace_corona(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "013cb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(data['tweet']).split()\n",
    "dic_corona = {}\n",
    "for word in words:\n",
    "    if 'corona' in word:\n",
    "        if word in dic_corona:\n",
    "            dic_corona[word] += 1\n",
    "        else:\n",
    "            dic_corona[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af73294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = dic_corona.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "80e486b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corona ['corona']\n",
      "coronas ['coronas']\n",
      "coronacanada ['corona', 'canada']\n",
      "corona2019 ['corona', '2019']\n",
      "nocorona ['no', 'corona']\n",
      "coronalockdown ['corona', 'lockdown']\n",
      "coronaactnow ['corona', 'act', 'now']\n",
      "coronacalendar ['corona', 'calendar']\n",
      "highriskcorona ['high', 'risk', 'corona']\n",
      "coronataskforce ['corona', 'taskforce']\n",
      "coronacrisis ['corona', 'crisis']\n",
      "coronaoutbreak ['corona', 'outbreak']\n",
      "coronatx ['corona', 'tx']\n",
      "coronapocalypse ['cor', 'on', 'apocalypse']\n",
      "coronaab ['corona', 'ab']\n",
      "coronadining ['corona', 'dining']\n",
      "coronachat ['corona', 'chat']\n",
      "coronaetiquette ['corona', 'etiquette']\n",
      "coronaquarentinedays ['corona', 'qu', 'arent', 'in', 'e', 'days']\n",
      "seattlecorona ['seattle', 'corona']\n",
      "coronalife ['corona', 'life']\n",
      "coronabahamas ['corona', 'bahamas']\n",
      "coronafacts ['corona', 'facts']\n",
      "coronaidiots ['corona', 'idiots']\n",
      "therescoronainsidethere ['there', 's', 'corona', 'inside', 'there']\n",
      "floridacorona ['florida', 'corona']\n",
      "coronacooties ['corona', 'cooties']\n",
      "businessduringcorona ['business', 'during', 'corona']\n",
      "coronadiaries ['corona', 'diaries']\n",
      "coronadatabased ['corona', 'data', 'based']\n",
      "floridacoronaepicenter ['florida', 'corona', 'epicenter']\n",
      "macorona ['macor', 'on', 'a']\n",
      "coronaresource ['corona', 'resource']\n",
      "coronaframeofmind ['corona', 'frame', 'of', 'mind']\n",
      "coronasafe ['corona', 'safe']\n",
      "coronabegona ['corona', 'be', 'gona']\n",
      "mycoronalife ['my', 'corona', 'life']\n",
      "welcometothecptcoronaedition ['welcome', 'to', 'the', 'c', 'pt', 'corona', 'edition']\n",
      "occorona ['oc', 'corona']\n",
      "coronaontario ['corona', 'ontario']\n",
      "coronaeniers ['corona', 'eni', 'ers']\n",
      "texascorona ['texas', 'corona']\n",
      "coronabc ['corona', 'bc']\n",
      "coronaid ['corona', 'id']\n",
      "coronans ['corona', 'ns']\n",
      "coronaisairborne ['corona', 'is', 'airborne']\n",
      "procorona ['pro', 'corona']\n",
      "coronatesting ['corona', 'testing']\n",
      "coronavariant ['corona', 'variant']\n",
      "flcorona ['fl', 'corona']\n",
      "longcorona ['long', 'corona']\n",
      "coronavt ['corona', 'vt']\n",
      "coronanb ['corona', 'nb']\n",
      "coronastories ['corona', 'stories']\n",
      "coronachallenge ['corona', 'challenge']\n",
      "stopthespreadcorona ['stop', 'the', 'spread', 'corona']\n",
      "coronaintexas ['corona', 'in', 'texas']\n",
      "californiacorona ['california', 'corona']\n",
      "coronacoping ['corona', 'coping']\n",
      "coronaisntover ['corona', 'isnt', 'over']\n",
      "coronapositive ['corona', 'positive']\n",
      "coronasnotover ['coronas', 'not', 'over']\n",
      "coronasafety ['corona', 'safety']\n",
      "coronasecure ['corona', 'secure']\n",
      "trumpcorona ['trump', 'corona']\n",
      "dgkscorona ['d', 'g', 'ks', 'corona']\n",
      "coronabreak ['corona', 'break']\n",
      "coronaprecautions ['corona', 'precautions']\n",
      "coronainsa ['corona', 'in', 's', 'a']\n",
      "coronalog ['corona', 'log']\n",
      "coronaizer ['corona', 'ize', 'r']\n",
      "coronakfla ['corona', 'kf', 'la']\n",
      "coronawise ['corona', 'wise']\n",
      "coronahammer ['corona', 'hammer']\n",
      "coronakim ['corona', 'kim']\n",
      "breakthroughcorona ['breakthrough', 'corona']\n",
      "coronachristmas ['corona', 'christmas']\n",
      "markedbycorona ['marked', 'by', 'corona']\n",
      "coronaisnotover ['corona', 'is', 'not', 'over']\n",
      "notgettingcorona ['not', 'getting', 'corona']\n",
      "corona84 ['corona', '84']\n",
      "coronadelmar ['corona', 'delmar']\n",
      "coronado ['coronado']\n",
      "wuhancoronaoutbreak ['wuhan', 'corona', 'outbreak']\n",
      "coronaroundup ['corona', 'roundup']\n",
      "coronaqa ['corona', 'qa']\n",
      "coronausa ['corona', 'usa']\n",
      "coronaupdate ['corona', 'update']\n",
      "iacorona ['i', 'a', 'corona']\n",
      "communityagainstcorona ['community', 'against', 'corona']\n",
      "coronaresponse ['corona', 'response']\n",
      "coronaph ['corona', 'ph']\n",
      "coronamutualaidsea ['corona', 'mutual', 'aid', 'sea']\n",
      "coronapr ['corona', 'pr']\n",
      "coronacalories ['corona', 'calories']\n",
      "coronacarbs ['corona', 'carb', 's']\n",
      "coronalk ['coronal', 'k']\n",
      "highhcoronacasenumbersreported ['high', 'h', 'corona', 'case', 'numbers', 'reported']\n",
      "coronaau ['corona', 'au']\n",
      "coronayyc ['corona', 'yy', 'c']\n",
      "coronaneighbourhoodsquad ['corona', 'neighbourhood', 'squad']\n",
      "coronasl ['coronas', 'l']\n",
      "coronawestchester ['corona', 'westchester']\n",
      "coronaug ['corona', 'ug']\n",
      "coronatimes ['corona', 'times']\n",
      "coronav ['corona', 'v']\n",
      "coronadisparenting ['corona', 'd', 'is', 'parenting']\n",
      "approvalcorona ['approval', 'corona']\n",
      "coronaalberta ['corona', 'alberta']\n",
      "nyccorona ['nyc', 'corona']\n",
      "coronavtj ['corona', 'vt', 'j']\n",
      "mycoronadiary ['my', 'corona', 'diary']\n",
      "probablycorona ['probably', 'corona']\n",
      "cicorona ['ci', 'corona']\n",
      "pandemiccorona ['pandemic', 'corona']\n",
      "coronapositives ['corona', 'positives']\n",
      "coronanewyourk ['corona', 'new', 'your', 'k']\n",
      "coronaquarantine ['corona', 'quarantine']\n",
      "coronanz ['corona', 'nz']\n",
      "coronask ['coronas', 'k']\n",
      "coronaproblems ['corona', 'problems']\n",
      "corona20 ['corona', '20']\n",
      "coronashutdown ['corona', 'shutdown']\n",
      "agricandcorona ['agri', 'c', 'and', 'corona']\n",
      "coronatwitter ['corona', 'twitter']\n",
      "coronaon ['corona', 'on']\n",
      "coronadeutschland ['corona', 'deutschland']\n",
      "corona2019de ['corona', '2019', 'de']\n",
      "lifeaftercorona ['life', 'after', 'corona']\n",
      "coronahumor ['corona', 'humor']\n",
      "coronaus ['corona', 'us']\n",
      "coronaviru ['corona', 'vi', 'ru']\n",
      "lifewithcorona ['life', 'with', 'corona']\n",
      "coronasuggestions ['corona', 'suggestions']\n",
      "coronaaus ['corona', 'a', 'us']\n",
      "coronafood ['corona', 'food']\n",
      "anticorona ['anti', 'corona']\n",
      "wecanbeatcorona ['we', 'can', 'beat', 'corona']\n",
      "coronaamazon ['corona', 'amazon']\n",
      "coronasa ['coronas', 'a']\n",
      "coronaseattle ['corona', 'seattle']\n",
      "raleighbeatscorona ['raleigh', 'beats', 'corona']\n",
      "coronaquestion ['corona', 'question']\n",
      "coronatips ['corona', 'tips']\n",
      "coronahysteria ['corona', 'hysteria']\n",
      "coronatexas ['corona', 'texas']\n",
      "inthetimesofcorona ['in', 'the', 'times', 'of', 'corona']\n",
      "coronacomics ['corona', 'comics']\n",
      "dietinginthetimeofcorona ['dieting', 'in', 'the', 'time', 'of', 'corona']\n",
      "starvinginthetimesofcorona ['starving', 'in', 'the', 'times', 'of', 'corona']\n",
      "comedyinthetimesofcorona ['comedy', 'in', 'the', 'times', 'of', 'corona']\n",
      "coronadiet ['corona', 'diet']\n",
      "beatcorona ['beat', 'corona']\n",
      "komeshacorona ['ko', 'mesha', 'corona']\n",
      "weshallovercomecorona ['we', 'shall', 'overcome', 'corona']\n",
      "coronaopportunitys ['corona', 'opportunity', 's']\n",
      "coronapersonals ['corona', 'personals']\n",
      "coronavir ['corona', 'vi', 'r']\n",
      "coronacooking ['corona', 'cooking']\n",
      "coronavi ['corona', 'vi']\n",
      "coronadreams ['corona', 'dreams']\n",
      "phlcoronafund ['ph', 'l', 'corona', 'fund']\n",
      "coronainequalitygap ['corona', 'inequality', 'gap']\n",
      "linkincorona ['linkin', 'corona']\n",
      "notcorona ['not', 'corona']\n",
      "corona] ['corona']\n",
      "coronacomfortfood ['corona', 'comfort', 'food']\n",
      "coronazim ['corona', 'zim']\n",
      "levityinthetimeofcorona ['levity', 'in', 'the', 'time', 'of', 'corona']\n",
      "coronanj ['corona', 'nj']\n",
      "coronajoys ['corona', 'joys']\n",
      "uniteagainstcorona ['unite', 'against', 'corona']\n",
      "coronadelivery ['corona', 'delivery']\n",
      "coronasupplies ['corona', 'supplies']\n",
      "lifeduringcorona ['life', 'during', 'corona']\n",
      "sgocorona ['s', 'go', 'corona']\n",
      "coronacomfortfoods ['corona', 'comfort', 'foods']\n",
      "coronacolorado ['corona', 'colorado']\n",
      "coronabaking ['corona', 'baking']\n",
      "gocoronacoronago ['go', 'corona', 'corona', 'go']\n",
      "lifeinthetimeofcorona ['life', 'in', 'the', 'time', 'of', 'corona']\n",
      "precorona ['pre', 'corona']\n",
      "coronathings ['corona', 'things']\n",
      "coronaclusterfuck ['corona', 'cluster', 'fuck']\n",
      "coronaessentials ['corona', 'essentials']\n",
      "stayhometogetcoronaout ['stay', 'home', 'to', 'get', 'corona', 'out']\n",
      "thatcoronalife ['that', 'corona', 'life']\n",
      "informationagainstcorona ['information', 'against', 'corona']\n",
      "coronawarriors ['corona', 'warriors']\n",
      "coronamusings ['corona', 'musings']\n",
      "coronasupport ['corona', 'support']\n",
      "coronanyc ['corona', 'nyc']\n",
      "heroesofcorona ['heroes', 'of', 'corona']\n",
      "postcorona ['post', 'corona']\n",
      "coronainsomnia ['corona', 'insomnia']\n",
      "positiveimpactofcorona ['positive', 'impact', 'of', 'corona']\n",
      "jobequitycorona ['job', 'equity', 'corona']\n",
      "coronari ['corona', 'ri']\n",
      "livinginacoronaworld ['living', 'in', 'a', 'corona', 'world']\n",
      "coronakind ['corona', 'kind']\n",
      "coronaworkers ['corona', 'workers']\n",
      "coronakindness ['corona', 'kindness']\n",
      "coronaheroes ['corona', 'heroes']\n",
      "irsfailurecorona ['irs', 'failure', 'corona']\n",
      "corona2020 ['corona', '2020']\n",
      "coronasuffolk ['corona', 'suffolk']\n",
      "coronamb ['corona', 'mb']\n",
      "unsungcoronaheroes ['unsung', 'corona', 'heroes']\n",
      "coronabonus ['corona', 'bonus']\n",
      "coronatrump ['corona', 'trump']\n",
      "coronapandemic ['corona', 'pandemic']\n",
      "ageofcoronav ['age', 'of', 'corona', 'v']\n",
      "ifcoronaneverhappened ['if', 'corona', 'never', 'happened']\n",
      "coronahelp ['corona', 'help']\n",
      "coronation ['coronation']\n",
      "coronama ['corona', 'ma']\n",
      "coronafun ['corona', 'fun']\n",
      "idahocorona ['idaho', 'corona']\n",
      "coronatime ['corona', 'time']\n",
      "coronaiaries ['corona', 'i', 'aries']\n",
      "coloradocorona ['colorado', 'corona']\n",
      "closingthecoronagap ['closing', 'the', 'corona', 'gap']\n",
      "coronareality ['corona', 'reality']\n",
      "coronaiocy ['corona', 'ioc', 'y']\n",
      "coronatest ['corona', 'test']\n",
      "deliveryduringcorona ['delivery', 'during', 'corona']\n",
      "donthecoronacon ['don', 'the', 'corona', 'con']\n",
      "coronawinter ['corona', 'winter']\n",
      "coronadiscoveries ['corona', 'discoveries']\n",
      "catchingcorona ['catching', 'corona']\n",
      "zerocorona ['zero', 'corona']\n",
      "coronaimpact ['corona', 'impact']\n",
      "coronasucks ['corona', 'sucks']\n",
      "coronazero ['corona', 'zero']\n",
      "coronachronicles ['corona', 'chronicles']\n",
      "allwhilebeingcoronasafe ['all', 'while', 'being', 'corona', 'safe']\n",
      "cpmgcoronasolutions ['c', 'pmg', 'corona', 'solutions']\n",
      "coronansw ['corona', 'nsw']\n",
      "corona21 ['corona', '21']\n",
      "coronarecovery ['corona', 'recovery']\n",
      "madcoronachat ['mad', 'corona', 'chat']\n",
      "madcoronakicking ['mad', 'corona', 'kicking']\n",
      "coronacrazy ['corona', 'crazy']\n",
      "coronaanswers ['corona', 'answers']\n",
      "coronaes ['coronae', 's']\n",
      "coronanfld ['corona', 'nfl', 'd']\n",
      "coronaalert ['corona', 'alert']\n",
      "coronarelief ['corona', 'relief']\n",
      "fuckcorona ['fuck', 'corona']\n",
      "coronafriendly ['corona', 'friendly']\n",
      "coronagoodreads ['corona', 'good', 'reads']\n",
      "coronaemergency2021 ['corona', 'emergency', '2021']\n",
      "postcoronalife ['post', 'corona', 'life']\n",
      "coronaroulette ['corona', 'roulette']\n",
      "safeduringcorona ['safe', 'during', 'corona']\n",
      "unite2fightcorona ['unite', '2', 'fight', 'corona']\n",
      "coronad ['corona', 'd']\n",
      "coronavic ['corona', 'vic']\n",
      "coronaact ['corona', 'act']\n",
      "coronaqld ['corona', 'qld']\n",
      "coronacoward ['corona', 'coward']\n",
      "pdxcorona ['pd', 'x', 'corona']\n",
      "sccorona ['sc', 'corona']\n",
      "coronaian ['corona', 'ian']\n",
      "decorona ['de', 'corona']\n",
      "coronaians ['corona', 'ian', 's']\n",
      "coronachristmasclub ['corona', 'christmas', 'club']\n",
      "noncorona ['non', 'corona']\n",
      "coronary ['coronary']\n",
      "coronachina ['corona', 'china']\n",
      "coronavius ['corona', 'vi', 'us']\n",
      "n13corona ['n', '13', 'corona']\n",
      "coronahaaregaindiajeetega ['corona', 'ha', 'are', 'ga', 'india', 'jeet', 'eg', 'a']\n",
      "indiafightscorona ['india', 'fights', 'corona']\n",
      "coronauk ['corona', 'uk']\n",
      "helpfightcorona ['help', 'fight', 'corona']\n",
      "coronachennai ['corona', 'chennai']\n",
      "coronavolunteer ['corona', 'volunteer']\n",
      "coronaandgroceries ['corona', 'and', 'groceries']\n",
      "coronanl ['corona', 'nl']\n",
      "coronainpakistan ['corona', 'in', 'pakistan']\n",
      "coronaoutbreakindia ['corona', 'outbreak', 'india']\n",
      "coronatoronto ['corona', 'toronto']\n",
      "coronainindia ['corona', 'in', 'india']\n",
      "coronazimbabwe ['corona', 'zimbabwe']\n",
      "goafightscorona ['go', 'a', 'fights', 'corona']\n",
      "coronawarrior ['corona', 'warrior']\n",
      "coronaindia ['corona', 'india']\n",
      "indorefightscorona ['indore', 'fights', 'corona']\n",
      "corona2019ireland ['corona', '2019', 'ireland']\n",
      "coronao ['corona', 'o']\n",
      "coronanigeria ['corona', 'nigeria']\n",
      "coronaout ['corona', 'out']\n",
      "coronatales ['corona', 'tales']\n",
      "stopcorona ['stop', 'corona']\n",
      "indiafightcorona ['india', 'fight', 'corona']\n",
      "kingstonrotarycoronaisolation ['kingston', 'rotary', 'corona', 'isolation']\n",
      "coronaharega ['corona', 'hare', 'ga']\n",
      "coronahelpers ['corona', 'helpers']\n",
      "gautengcorona ['gauteng', 'corona']\n",
      "coronadc ['corona', 'dc']\n",
      "bhilaifighttocorona ['bhil', 'a', 'i', 'fight', 'to', 'corona']\n",
      "combatcorona ['combat', 'corona']\n",
      "udaanfightscorona ['uda', 'an', 'fights', 'corona']\n",
      "modicoronaaddress ['modi', 'corona', 'address']\n",
      "punefightscorona ['pune', 'fights', 'corona']\n",
      "indiaawaitscoronaaid ['india', 'awaits', 'corona', 'aid']\n",
      "fightingcoronatogether ['fighting', 'corona', 'together']\n",
      "ahmedabadfightscorona ['ahmedabad', 'fights', 'corona']\n",
      "padrescoronatwitter ['padres', 'corona', 'twitter']\n",
      "coronanola ['corona', 'nola']\n",
      "coronaneworleans ['corona', 'new', 'orleans']\n",
      "camdencountycoronahotline ['camden', 'county', 'corona', 'hotline']\n",
      "coronarukona ['corona', 'ru', 'kona']\n",
      "stopthespreadofcorona ['stop', 'the', 'spread', 'of', 'corona']\n",
      "telanganacoronapublictaskforce ['te', 'langan', 'a', 'corona', 'public', 'taskforce']\n",
      "coronaroko ['corona', 'rok', 'o']\n",
      "telanganafightscorona ['te', 'langan', 'a', 'fights', 'corona']\n",
      "keralafightscorona ['kerala', 'fights', 'corona']\n",
      "odishafightscorona ['o', 'dish', 'a', 'fights', 'corona']\n",
      "survivingcorona ['surviving', 'corona']\n",
      "coronalockdownindia ['corona', 'lockdown', 'india']\n",
      "coronainmyarashtra ['corona', 'in', 'mya', 'rasht', 'ra']\n",
      "mycoronastory ['my', 'corona', 'story']\n",
      "defeatcorona ['defeat', 'corona']\n",
      "dronethecorona ['drone', 'the', 'corona']\n",
      "coronanorway ['corona', 'norway']\n",
      "coronaservices ['corona', 'services']\n",
      "coronalifeservices ['corona', 'life', 'services']\n",
      "coronahelper ['corona', 'helper']\n",
      "spreadlovenotcorona ['spread', 'love', 'not', 'corona']\n",
      "coronabilluk ['corona', 'bill', 'uk']\n",
      "coronakenya ['corona', 'kenya']\n",
      "coronaireland ['corona', 'ireland']\n",
      "coronafreepakistan ['corona', 'free', 'pakistan']\n",
      "coronaupdatesindia ['corona', 'updates', 'india']\n",
      "coronadallas ['corona', 'dallas']\n",
      "coronacrisisuk ['corona', 'crisis', 'uk']\n",
      "coronaaustralia ['corona', 'australia']\n",
      "corona19virus ['corona', '19', 'virus']\n",
      "corona2019au ['corona', '2019', 'au']\n",
      "coronawatch ['corona', 'watch']\n",
      "corona2019us ['corona', '2019', 'us']\n",
      "coronake ['corona', 'ke']\n",
      "duringcorona ['during', 'corona']\n",
      "coronasouthafrica ['corona', 'south', 'africa']\n",
      "coronawalkout ['corona', 'walkout']\n",
      "corona19 ['corona', '19']\n",
      "coronaeyeopener ['corona', 'eye', 'opener']\n",
      "coronahv ['corona', 'hv']\n",
      "coronajay2020 ['corona', 'jay', '2020']\n",
      "utahcorona ['utah', 'corona']\n",
      "coronatucson ['corona', 'tucson']\n",
      "waroncorona ['war', 'on', 'corona']\n",
      "coronaqatar ['corona', 'qatar']\n",
      "qatarcoronawatch ['qatar', 'corona', 'watch']\n",
      "coronasverige ['corona', 'sverige']\n",
      "coronaitaly ['corona', 'italy']\n",
      "buildforcorona ['build', 'for', 'corona']\n",
      "italycorona ['italy', 'corona']\n",
      "coronamx ['corona', 'mx']\n",
      "coronaneighbornetwork ['corona', 'neighbor', 'network']\n",
      "soscorona ['sos', 'corona']\n",
      "womencoronascot ['women', 'corona', 'scot']\n",
      "jamshedpurfightscorona ['jamshedpur', 'fights', 'corona']\n",
      "coronagroceryaid ['corona', 'grocery', 'aid']\n",
      "bokarofightscorona ['bo', 'karo', 'fights', 'corona']\n",
      "indoredefeatscorona ['indore', 'defeats', 'corona']\n",
      "ranchifightscorona ['ranchi', 'fights', 'corona']\n",
      "nobafightscorona ['no', 'ba', 'fights', 'corona']\n",
      "rutomurdersvscorona ['ru', 'to', 'murders', 'vs', 'corona']\n",
      "coronabalt ['corona', 'bal', 't']\n",
      "coronaservice ['corona', 'service']\n",
      "coronapei ['corona', 'pei']\n",
      "ukbidscorona ['uk', 'bids', 'corona']\n",
      "corona213 ['corona', '213']\n",
      "coronaai ['corona', 'a', 'i']\n",
      "coronasharyana ['coronas', 'haryana']\n",
      "medstudentcorona ['med', 'student', 'corona']\n",
      "helpdeskforcorona ['help', 'desk', 'for', 'corona']\n",
      "coronasainikas ['coronas', 'a', 'inika', 's']\n",
      "coronacare ['corona', 'care']\n",
      "hrcorona ['hr', 'corona']\n",
      "centrocomunitariocorona ['centro', 'com', 'unit', 'a', 'rio', 'corona']\n",
      "artversuscorona ['art', 'versus', 'corona']\n",
      "disabilityandcorona ['disability', 'and', 'corona']\n",
      "coronaqc ['corona', 'qc']\n",
      "coronamontreal ['corona', 'montreal']\n",
      "closedforcorona ['closed', 'for', 'corona']\n",
      "coronaisactuallycroatoan ['corona', 'is', 'actually', 'cro', 'a', 'to', 'an']\n",
      "corona2019india ['corona', '2019', 'india']\n",
      "coronafighters ['corona', 'fighters']\n",
      "coronameme ['corona', 'meme']\n",
      "coronamemes ['corona', 'memes']\n",
      "coronagrocery ['corona', 'grocery']\n",
      "coronaklang ['corona', 'klang']\n",
      "coronaparenting ['corona', 'parenting']\n",
      "beatingcoronatogether ['beating', 'corona', 'together']\n",
      "coronap ['corona', 'p']\n",
      "copingwithcorona ['coping', 'with', 'corona']\n",
      "fightcorona ['fight', 'corona']\n",
      "coronakeepsmeathome ['corona', 'keeps', 'meat', 'home']\n",
      "dronescombatcorona ['drones', 'combat', 'corona']\n",
      "canadacorona ['canada', 'corona']\n",
      "aapfcorona ['a', 'apf', 'corona']\n",
      "coronapakistan ['corona', 'pakistan']\n",
      "pakistanfightscorona ['pakistan', 'fights', 'corona']\n",
      "coronarelieffund ['corona', 'relief', 'fund']\n",
      "coronaheroesfund ['corona', 'heroes', 'fund']\n",
      "lancscorona ['lanc', 's', 'corona']\n",
      "coronaupdates ['corona', 'updates']\n",
      "coronacomedy ['corona', 'comedy']\n",
      "amdavadfightscorona ['am', 'dav', 'ad', 'fights', 'corona']\n",
      "coronakuwait ['corona', 'kuwait']\n",
      "netcoronaappeal ['net', 'corona', 'appeal']\n",
      "togetheragainstcorona ['together', 'against', 'corona']\n",
      "carenotcorona ['care', 'not', 'corona']\n",
      "coronanews ['corona', 'news']\n",
      "coronawin ['corona', 'win']\n",
      "coronaisrealke ['corona', 'is', 'real', 'ke']\n",
      "avoidcorona ['avoid', 'corona']\n",
      "coronaoutbrea ['corona', 'out', 'brea']\n",
      "coronaschool ['corona', 'school']\n",
      "give4corona ['give', '4', 'corona']\n",
      "coronay ['corona', 'y']\n",
      "strongagainstcorona ['strong', 'against', 'corona']\n",
      "fishsafightscorona ['fish', 's', 'a', 'fights', 'corona']\n",
      "coronaextremelyvulnerable ['corona', 'extremely', 'vulnerable']\n",
      "coronamutualaid ['corona', 'mutual', 'aid']\n",
      "coronamas ['corona', 'mas']\n",
      "coronasecondwave ['corona', 'second', 'wave']\n",
      "coronavaccine ['corona', 'vaccine']\n",
      "totallycorona ['totally', 'corona']\n",
      "coronaindiainfo ['corona', 'india', 'info']\n",
      "coronaresources ['corona', 'resources']\n",
      "coronasos ['coronas', 'os']\n",
      "coronaemergency ['corona', 'emergency']\n",
      "coronavolunteers ['corona', 'volunteers']\n",
      "coronahyderabad ['corona', 'hyderabad']\n",
      "coronavaccination ['corona', 'vaccination']\n",
      "icorona ['i', 'corona']\n",
      "coronafooddelivery ['corona', 'food', 'delivery']\n",
      "coronatnlockdown ['corona', 'tn', 'lockdown']\n",
      "coronasafenepal ['corona', 'safe', 'nepal']\n",
      "coronaindiahelp ['corona', 'india', 'help']\n",
      "chennaicoronahelp ['chennai', 'corona', 'help']\n",
      "chennaicoronasupport ['chennai', 'corona', 'support']\n",
      "coronasecondwaveinindia ['corona', 'second', 'wave', 'in', 'india']\n",
      "gujaratcoronasupport ['gujarat', 'corona', 'support']\n",
      "coronamalaysia ['corona', 'malaysia']\n",
      "coronashopping ['corona', 'shopping']\n",
      "coronadash ['corona', 'dash']\n",
      "coronaisreal ['corona', 'is', 'real']\n",
      "coronaprotectothers ['corona', 'protect', 'others']\n",
      "coronaplanb ['corona', 'plan', 'b']\n",
      "shoppinginthetimeofcorona ['shopping', 'in', 'the', 'time', 'of', 'corona']\n",
      "killcoronaideas ['kill', 'corona', 'ideas']\n",
      "coronasupportmysuru ['corona', 'support', 'my', 'suru']\n",
      "coronamamas ['corona', 'mamas']\n",
      "arcorona ['ar', 'corona']\n"
     ]
    }
   ],
   "source": [
    "import wordninja\n",
    "for word in new_words:\n",
    "    print(word, wordninja.split(word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4fc7b0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195618 218209 87739\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc75bc",
   "metadata": {},
   "source": [
    "### Divide connected words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "27faa658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 368413/368413 [00:03<00:00, 116644.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15018"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of connected words\n",
    "connect_words =[]\n",
    "for i in tqdm(range(len(data))):\n",
    "    words = data['tweet'][i].split(' ')\n",
    "    for word in words:\n",
    "        if len(word) >12 and word not in connect_words:\n",
    "            connect_words.append(word)\n",
    "len(connect_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "86dda557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja\n",
    "def split_words(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if len(word) > 12:\n",
    "            new_words.extend(wordninja.split(word))\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2dd0b011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 368413/368413 [00:03<00:00, 110322.54it/s]\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].progress_apply(lambda x:split_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9a3008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195868 218561 87929\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca1399",
   "metadata": {},
   "source": [
    "### Remove words with single letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab22cf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277156"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find number of words with only one letter\n",
    "tweets = np.array(data['tweet'])\n",
    "count = 0\n",
    "word_list = []\n",
    "for tweet in tweets:\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if len(word) <= 1:\n",
    "            word_list.append(word)\n",
    "            count += 1\n",
    "    \n",
    "    \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4704eaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 368413/368413 [00:01<00:00, 192666.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stopped grocery store curbside pickup they put your orderer your trunk contact she she saw tons boomers going store protection all\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also workers watch hands curbside delivery\n"
     ]
    }
   ],
   "source": [
    "def remove_single_words(text):\n",
    "    single_words = ['o', 'd', 'r', 'k', 'e', 'y', 'n', 'b', 'c', 'h', 'a', 's', 'g', 'u', 'p', 'w', 'z', 'm', 'x', 'f',\n",
    "                'j', 'v', 'l', 't', 'q']\n",
    "    words = text.split()\n",
    "    lst = []\n",
    "    for word in words:\n",
    "        if word not in single_words:\n",
    "            lst.append(word)\n",
    "    return ' '.join(lst)\n",
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x : remove_single_words(x))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5355ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195868 218561 87929\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ac67d",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "455824ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f45569bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 368413/368413 [03:10<00:00, 1935.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stop grocery store curbside pickup they put your orderer your trunk contact she she saw ton boomer go store protection all\n",
      "they also say it probably safer get food restaurant curb side versus grocery shopping right now less exposure they close also worker watch hand curbside delivery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "tqdm.pandas()\n",
    "data[\"tweet\"] = data[\"tweet\"].progress_apply(lambda text: lemmatize_words(text))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6160c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195869 218569 87929\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3454aaec",
   "metadata": {},
   "source": [
    "### Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7e7f8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8eb81ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding words with wrong spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "18998d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(data['tweet']).split()\n",
    "misspelled_words = spell.unknown(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cf5e6d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55552"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspelled_words = list(set(misspelled_words))\n",
    "len(misspelled_words)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bde29f9",
   "metadata": {},
   "source": [
    "spell.correction('hyperlink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d9572a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 55552/55552 [1:04:12<00:00, 14.42it/s]\n"
     ]
    }
   ],
   "source": [
    "corrected_words = {}\n",
    "for i in tqdm(misspelled_words):\n",
    "    corrected = spell.correction(i)\n",
    "    if corrected != None and i != corrected:\n",
    "        corrected_words[i] = corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dfd079f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35574"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1edbb556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 368413/368413 [00:01<00:00, 361480.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def correct_spellings(text, corrected):\n",
    "    words = text.split(' ')\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in corrected:\n",
    "            new_words.append(corrected[word])\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "            \n",
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x:correct_spellings(x, corrected_words))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "864c61d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195869 220145 89060\n"
     ]
    }
   ],
   "source": [
    "print(count_link(data), count_i(data), count_my(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a0554",
   "metadata": {},
   "source": [
    "### Remove non-english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f2fceed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "80efa11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 6610274/6610274 [00:20<00:00, 322165.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34028"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find non-english words\n",
    "non_english = []\n",
    "eng_words = set(nltk.corpus.words.words())\n",
    "words = ' '.join(data['tweet']).split()\n",
    "for i in tqdm(words):\n",
    "    if i.lower() not in eng_words and i.lower() not in non_english:\n",
    "        non_english.append(i)\n",
    "len(non_english)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0f178e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "eng_words = set(nltk.corpus.words.words())\n",
    "keep = ['timelsots','timeslot','convenience','efficient','dissapoint','convenient','inconvenience','substitute',\n",
    "       'recycling', 'recycle', 'pandemic','omnichannel', 'covid', 'curbside','hyperlink','biodegradable', 'hyperlink']\n",
    "\n",
    "def remove_nonenglish_words(sent,eng_words,keep):\n",
    "    words = [w for w in nltk.wordpunct_tokenize(sent) if w.lower() in eng_words or w.lower() in keep]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6b4c4ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 368413/368413 [00:02<00:00, 155129.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife stop grocery store curbside pickup they put your ordered your trunk contact she she saw ton boomer go store protection all\n",
      "they also say it probably get food restaurant curb side versus grocery shopping right now less exposure they close also worker watch hand curbside delivery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x:remove_nonenglish_words(x,eng_words,keep))\n",
    "print(data['tweet'][5003])\n",
    "print(data['tweet'][5006])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be396ee",
   "metadata": {},
   "source": [
    "### Find words related to Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e6b5a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_curb(text):\n",
    "    text = text.replace('curb side', 'curbside')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f8a392d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 368413/368413 [00:00<00:00, 1488164.99it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x: replace_curb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1eb86389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_covid(text):\n",
    "    text = text.replace('corona', 'covid')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1a68648b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 368413/368413 [00:00<00:00, 1496361.95it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "data['tweet'] = data['tweet'].progress_apply(lambda x: replace_covid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3e7c78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(data['tweet']).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4fa15b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368413"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cda1e7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Consumer         8377\n",
       "Advertisement    8092\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f7ccbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Cleaned_data_for_classification.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc160e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
